# Gestione della memoria nei SO
#### Memoria principale (RAM)
La RAM (Random Access Memory) √® una memoria volatile utilizzata per archiviare temporaneamente dati e istruzioni necessarie per l'esecuzione dei programmi.
√à una componente fondamentale ma, nonostante stia crescendo in capacit√† e velocit√†, i programmi e le applicazioni moderne tendono a crescere ancora pi√π rapidamente.

#### Gerarchia della memoria
Abbiamo due tipologie di memoria
- **Memorie veloci e costose** (es. RAM, chace), usate per dati che devono essere accessibili rapidamente
- **Memoria lente e grandi** (es. SSD), offrono spazio maggiore per l'archiviazione.

Il sistema operativo funge da intermediario, fornendo un modello astratto della memoria che permette ai processi di utilizzarla senza preoccuparsi della complessit√† della gerarchia sottostante.

#### Gestore della memoria
Il SO ha questo componente chiamato gestore della memoria che si occupa di
- Gestire la memoria scegliendo dove allocare i dati (quelli usati pi√π frequentemente nella RAM, quelli usati meno frequentemente nell'SSD)
- Tenere traccia di quale regione della memoria √® libera e quale occupata, assegnandola correttamente ai processi; quando un processo termina il gestore si occupa di liberare la regione di memoria precedentemente occupata


# Nessuna astrazione della memoria
Si tratta di un modello semplice, in cui viene fatto uso diretto della memoria fisica.
Venivano eseguite istruzioni del tipo
```arm-asm
MOV Register1, 1000
```

Il problema stava nel fatto che ogni programma poteva interferire con un altro, causando crash.

Questo modello era definito "monoprogrammato"

### Monoprogrammazione
La **monoprogrammazione** √® un modello in cui non esiste un'astrazione avanzata della memoria: un unico programma utente viene eseguito alla volta, insieme al sistema operativo.

Abbiamo tre approcci principali
1. **SO in RAM**
	In cui il sistema operativo veniva inserito nella RAM insieme al programma utente (tipico nei mainframe e nei minicomputer)
	![[Pasted image 20241120163645.png|center]]

2. **SO in ROM**
	 In cui il sistema operativo √® memorizzato nella ROM e il programma utente nella RAM (sistemi embedded, con SO piccolo, stabile e non modificabile)
	 ![[Pasted image 20241120163855.png|center]]

3. **SO + Drivers in ROM - User Program in RAM**
	SO e driver principali risiedono nella ROM, i programmi utente nella RAM (primi PC)
	![[Pasted image 20241120164411.png|center]]

#### Multiprogrammazione
La multiprogrammazione consente l'esecuzione di pi√π programmi contemporaneamente, ma senza tecniche avanzate di gestione della memoria.

Questo √® possibile attraverso al meccanismo dello **swapping**, una tecnica che consiste nel salvare TEMPORANEAMENTE il contenuto della memoria di un programma (`A`) in un file su memoria non volatile per liberare spazio ad un altro programma (`B`) e quando necessario il processo `A` viene ricaricato in memoria e riprende l'esecuzione.

La multiprogrammazione utilizza un metodo semplice chiamato **Naive Approach**, che carica pi√π programmi in memoria fisica consecutivamente, senza astrazione dell'indirizzo.
	<font color="#ff0000">a)</font> Un programma di 16KB che inizia con l'istruzione `JMP 24`
	<font color="#ff0000">b)</font> Un altro programma di 16KB che inizia con l'istruzione `JMP 28`
	<font color="#ff0000">c)</font> Entrambi i programmi caricati consecutivamente
		PROBLEMA
		- quando il secondo programma viene eseguito, `JMP 28` indirizza erroneamente all'istruzione del primo programma, causando errori. 
		![[Pasted image 20241120170738.png|center]]


---

# Astrazione della memoria
L'accesso diretto alla memoria fisica da parte dei programma pu√≤ causare molti problemi.

La soluzione a questo problema √® l'**astrazione di memoria**, che introduce un livello intermedio tra i programmi e la memoria fisica, con il quale
- ogni programma vede una memoria "virtuale" isolata
- il SO gestisce la traduzione degli indirizzi virtuali in indirizzi fisici

#### Spazio degli indirizzi
Ogni programma dispone di un proprio spazio degli indirizzi, che √® un insieme unico di indirizzi virtuali utilizzati per accedere alla memoria. Questo spazio √® indipendente da altri processi, creando un'illusione di memoria privata per ciascun programma.


#### Implementazione con registri base e limite
Per riuscire a gestire al meglio l'utilizzo da parte dei programmi dello spazio di indirizzo fisico a loro dedicato il SO introduce due registri
1. **REGISTRO BASE**
	- Contiene l'indirizzo fisico del programma in memoria

2. **REGISTRO LIMITE**
	- Contiene la lunghezza del programma e garantisce che il programma non acceda a zone di memoria oltre i limiti assegnati

Quando un programma tenta di accedere a un indirizzo, viene aggiunto automaticamente il valore del **Registro Base** all'indirizzo logico generato (dal programma). 
L'hardware poi verifica che l'indirizzo risultante non superi il valore del **Registro Limite**.

###### Esempio
Ogni programma lavora con indirizzi **logici** che il sistema operativo traduce in **indirizzi fisici**.

|**Programma**|**Registro Base**|**Registro Limite**|**Indirizzi fisici assegnati**|
|---|---|---|---|
|Programma A|2000|1000|Da **2000 a 2999**|
|Programma B|5000|1500|Da **5000 a 6499**|

Se un programma genera un indirizzo logico **L**, il sistema lo converte in indirizzo fisico:

$$\text{Indirizzo Fisico} = \text{Registro Base} + \text{Indirizzo Logico}$$
üí° **Esempio**:
- Se **Programma A** vuole accedere all‚Äôindirizzo **L = 50**:
    $$\text{Indirizzo Fisico} = 2000 + 50 = 2050$$
    ‚úÖ **Accesso consentito** perch√© 2050 √® all'interno di 2000-2999.
    
- Se prova ad accedere a **L = 1200**:
    $$\text{Indirizzo Fisico} = 2000 + 1200 = 3200$$
    ‚ùå **Accesso negato**, perch√© supera il **Registro Limite (1000)**.


![[Pasted image 20241120184059.png]]

#### Vantaggi e Svantaggi 
Come pro abbiamo il fatto che ogni processo ha un proprio spazio degli indirizzi separato e protetto, cos√¨ si evita che ogni programma acceda alla memoria di altri processi o al sistema operativo.

D'altro canto abbiamo un overhead di calcolo, perch√© ad ogni accesso alla memoria la CPU deve
- eseguire un'operazione di somma (indirizzo logico + base)
- effettuare un confronto con il limite
e questo porta ad una leggera latenza in termini di prestazioni.
E ovviamente questa tecnica funziona per sistemi semplici, ma non per ambienti complessi.


#### Cosa fare in caso di sovraccarico di memoria
Dato che un computer moderno pu√≤ eseguire centinaia di processi simultaneamente (e molti possono essere pesanti a livello di memoria), per gestire questo sovraccarico si possono utilizzare due strategie:
1. **Swapping dei processi**, in cui vengono spostati interi processi da RAM a memoria non volatile (generalmente i processi non attivi vengono messi nella memoria non volatile e poi ripresi)
2. **Memoria virtuale**, che permette l'esecuzione dei programmi **anche se non sono completamente presenti nella RAM**. 


#### Swapping
L'approccio delle partizioni dinamiche con lo swapping migliora la gestione della memoria rispetto alle partizioni fisse, ma introduce anche dei problemi.
Guardiamo la foto
![[Pasted image 20241120191606.png|center]]
Ogni processo ottiene la quantit√† di memoria necessaria, senza limitazioni.
Quando per√≤ allochiamo `D`, spostando `A`, la memoria viene frammentata nello spazio tra `D` e `B`, creando quindi dei problemi.

Una soluzione potrebbe essere quella di compattare la memoria, ossia spostare tutti i processi attivi in modo da creare un unico grande blocco di spazio libero.


#### Crescita dei processi
Un aspetto fondamentale da tenere in considerazione √® l'eventuale crescita dinamica di segmenti come lo stack e i dati, che richiedono spazio extra per evitare conflitti. Per ovviare, il sistema operativo pu√≤ riservare memoria aggiuntiva durante lo swapping intorno ai processi oppure, in caso di memoria insufficiente pu√≤
- "uccidere il processo"
- trasferire il processo
- fare lo swapping


---

# Gestione della memoria libera
L'obiettivo principale √® monitorare l'uso della memoria per allocare e liberare spazio in modo efficiente.
Due sono gli approcci principali:
1. Bitmap
	Una struttura che usa una mappa di bit per indicare quali blocchi sono occupati o liberi (trovare spazi liberi √® pi√π lento)

2. Lista collegata
	Una struttura che tiene traccia della memoria non allocata utilizzando liste collegate, in modo da rintracciare spazi liberi e occupati (trovare spazi liberi √® pi√π veloce)

Questo tracciamento non si limita alla memoria, ma si applica anche ad altre risorse come il file system, garantendo una gestione ordinata ed efficiente delle risorse disponibili.
##### Come passare da una bitmap a una lista collegata
![[Pasted image 20241120194109.png]]
  
>[!tip] Modi per scrivere memorizzarle e scriverle
MEMORIZZAZIONE:
>- densa: memorizzo tutto cos√¨ com‚Äô√®
>- sparsa: tolgo gli 0 inutili e le ripetizioni
>
RAPPRESENTAZIONE:
>- densa: rappresento tutto cos√¨ com‚Äô√®
>- sparsa: tolgo gli 0 inutili e le ripetizioni

|**Struttura**|**Memorizzazione**|**Rappresentazione**|
|---|---|---|
|**Bitmap**|Densa o Sparsa (dipende da cosa salvo)|Densa o Sparsa (dipende da cosa mostro)|
|**Lista Collegata**|Sempre Sparsa (salva solo i valori importanti)|Sempre Sparsa (mostra solo i dati utili)|

#### Liste collegate doppie
√à uno dei metodi pi√π usati, in cui ogni blocco di memoria √® collegato al precedente e al successivo con dei puntatori.
Questo √® molto utile perch√© si pu√≤ controllare lo stato dei blocchi adiacenti e, se sono liberi pi√π blocchi adiacenti, questi vengono uniti in un unico grande blocco.
###### ESEMPIO
![[Pasted image 20241120194913.png|center]]
 `A`, `X`, `B` sono tre processi che occupano tre blocchi di memoria
	 <font color="#ff0000">a)</font> TUTTI i blocchi sono occupati, quando termina `X`, `A` e `B` NON vengono uniti
	 <font color="#ff0000">b)</font> Il blocco di `B` √® libero, quando termina `X`, i blocchi liberi di `X` e `B` vengono uniti
	 <font color="#ff0000">c)</font> Il blocco di `A` √® libero, quando termina `X`, i blocchi liberi di `A` e `X` vengono uniti
	 <font color="#ff0000">d)</font> I blocchi di `A` e `B` sono liberi, quando termina `X`, TUTTI i blocchi liberi vengono uniti


#### Algoritmi di allocazione di memoria
Gli **algoritmi di allocazione della memoria** vengono utilizzati quando un **nuovo processo viene caricato in memoria**.
**üîπ Cosa fanno?**  
Decidono **dove** allocare la memoria per il processo nel sistema.  
Si occupano di assegnare uno spazio RAM ai processi attivi.

***ALGORITMI***
- **FIRST FIT**
	Seleziona il primo spazio disponibile (il primo in assoluto)

- **NEXT FIT**
	Selezione il SUCCESSIVO spazio disponibile

- **BEST FIT**
	Seleziona lo spazio pi√π adeguato a quel determinato processo

- **WORST FIT**
	Seleziona lo spazio peggiore per quel determinato processo

- **QUICK FIT**
	Crea delle liste separate in base ai blocchi di memoria di dimensioni comuni (es. una lista per blocchi da 4KB, una per blocchi da 8KB, ecc.).
	Questo crea un problema di **coalescenza**, nel senso che tutti i blocchi sono visti nelle liste, ma se ci fossero due blocchi liberi, uno da 4KB e uno da 8KB, l'algoritmo non li rileverebbe e non potrebbe unirli in un unico blocco da 12KB

- **BUDDY ALLOCATION** (Linux)
	Migliora le performance di coalescenza del Quick Fit
	- La memoria inizia come un singolo pezzo contiguo
	- Ad ogni richiesta, la memoria viene divisa secondo una potenza di 2
	- Blocchi di memoria continui vengono uniti quando rilasciati (coalescenza)
	
	DIFFERENZA TRA BUDDY ALGORITHM E BUDDY ALLOCATOR 
	- BUDDY ALGORITHM = algoritmo
	- BUDDY ALLOCATOR = processo che si occupa di verificare l'efficienza del lavoro svolto dall'algoritmo
	
	PROBLEMA: Il Buddy Algorithm pu√≤ causare frammentazione (spazio inutilizzato).
		Se arriva, ad esempio, una richiesta di 65 pagine, verr√† allocato un blocco da 128 pagine di cui 63 saranno inutilizzate.
	**ESEMPIO**
	![[Pasted image 20241120201225.png|]]
	![[Pasted image 20241120201252.png]]


#### Slab Allocator
Lo **Slab Allocator** risolve il problema descritto dal Buddy, sfruttando il Buddy stesso
- chiede al Buddy blocchi di memoria (generalmente molto grandi)
- li suddivide in tanti piccoli "**slab**" (blocchi pi√π piccoli)
- a loro volta suddivide gli slab in **chunk** di dimensione uniforme, ognuno dei quali √® progettato per contenere un oggetto specifico
- quando un oggetto non serve pi√π, viene deallocato e il chunk diventa libero

Quindi se un processo chiede 65 pagine
- viene allocato un blocco da 128 pagine, di cui 65 verranno utilizzate
- le restanti 63, verranno prese dallo **slab allocator** e suddivisi per darli ad altri processi

Ogni slab pu√≤ trovarsi in tre stati
- **pieno**, in cui tutti i chunk sono occupati
- **parzialmente pieno**, alcuni chunk occupati, altri liberi
- **vuoto**, tutti i chunk sono liberi
![[Pasted image 20241121172334.png|center]]

Quando un oggetto viene deallocato, viene mantenuto nella chace in modo tale da poter inserire un altro oggetto con la stesse dimensioni qualora arrivasse; in questo modo non si verifica l'overhead di inizializzazione.

###### ESEMPIO DI SLAB
![[Pasted image 20241121174253.png]]
Due elementi molto importanti
- ***SLAB DESCRIPTOR***
	Contiene informazioni importanti sullo slab
	- slot totali
	- numero di slot liberi
	- numero di slot occupati
	Punta al **bufctl array**

- **BUFCTL ARRAY**
	Un array in cui sono contenute le posizioni degli slot liberi
	- NELLA FOTO IL 5 √à LIBERO, quindi il bufctl sar√† `[5]`
	- Se si libera, ad esempio, il 3, avremo `[3, 5]`