Generalmente abbiamo:
- INPUT: una sequenza di n numeri
- OUTPUT: una permutazione (arrangiamento) in cui ogni elemento è minore del successivo 


# Complessità quadratica
### Selection Sort
Si tratta di un algoritmo molto semplice, facile da programmare MA estremamente inefficiente.

FUNZIONAMENTO: 
- fisso una posizione `p`
- da lì in poi scorro tutto l'array cercando il minimo
- una volta trovato lo mettiamo nella posizione fissata inizialmente.
Una volta fatto questo 
- fisso la nuova posizione `p = p+1`
- rifaccio tutto quello detto in precedenza

![[selection-600.gif|center]]

PSEUDO-CODICE
```scss (MESSO A CASO)
SelectionSort(A)
1.  for k = 0 to n-2 do         // i vari round 
2.      m = k+1                 // la posizione che fisso (quella blu)
 
3.      for j = k+2 to n do     // parto dalla posizione successiva a quella blu
4.          if (A[j] < A[m]) then m = j   // i vari confronti (quelli verdi)

5.      scambia A[m] con A[k+1]    // lo scambio (swap)
```

>[!warning] PICCOLA PRECISAZIONE
L'algoritmo fa cacare perché la posizione 0 non viene controllata, BOH.

>[!question]- Perché nella riga uno abbiamo "n-2"?
Perché innanzitutto le posizioni da controllare sono "n-1" però poi durante il     penultimo confronto abbiamo che l'ultimo elemento è già ordinato, quindi non       serve controllarlo.

>[!tip]- Parentesi sulle INVARIANTI
>Le invarianti sono delle proprietà che devono rimanere immutate nella fase di esecuzione di un algoritmo e che vengono utilizzate per capire se un algoritmo è corretto.
>Ragionare per invarianti è uno strumento utile perché permette di isolare proprietà dell’algoritmo, spiegarne il funzionamento, capire a fondo l’idea su cui si basa.
>
>Nel nostro algoritmo abbiamo le seguenti invarianti.
>
>Al generico passo k abbiamo che:
>- (i) i primi k+1 elementi sono ordinati
>- (ii) i primi k+1 elementi sono i più piccoli dell'array

#### Complessità temporale
- CASO UPPER BOUND: 
	- sia il ciclo for esterno che il ciclo for interno verranno eseguiti al più (fai finta) **`n`** volte,
	- ogni linea di codice per il nostro modello RAM costa $O(1)$
	![[Pasted image 20241028151821.png]]

- CASO LOWER BOUND:
	Prendiamo la parte nel codice che esegue più operazioni, I CONFRONTI.
	- Il ciclo va da `k = 0` a `n-2`
	- Io eseguo `n-k-1` confronti, perché il primo elemento non viene mai contato (quindi n-1) e ho un k che aumenta
	Quindi
	![[Pasted image 20241028154659.png]]

Allora $T(n) = \Theta(n^2)$ 


### Insertion sort
Simile al Selection sort.

Partendo dalla posizione 1 fino a n-1, confronta l'elemento nella posizione corrente con i precedenti finché non trovo un caso in cui questo elemento risulta maggiore del precedente controllato.
![[insertion-600.gif|center]]

PSEUDO-CODICE
```scss (MESSO A CASO)

```


### Bubble sort
- gli elementi più grandi vengono spinti verso destra confrontando gli elementi adiacenti tra loro
***Come funziona?*** 
- Si scorre l'array `n-1 volte` per un numero `i` di volte, dovuto al fatto che dobbiamo ogni volta scorrere il nostro array; 
- ad ogni posizione nell'array confrontiamo il numero corrispondente con il suo successivo e se non abbiamo che CORRISPONDENTE $\le$ SUCCESSIVO li switchamo
- andiamo avanti così finché l'array non è ordinato.
La differenza tra questo e l'insertion sort sta nel fatto che gli elementi vengono confrontati a coppie, mentre nell'insertion sort io confronto un elemento con tutti ricorsivamente.
![[Pasted image 20241028205056.png|350]]![[bubble-640.gif|350]]

PSEUDO-CODICE
```scss (MESSO A CASO)

```



# Complessità meno che quadratica
### MergeSort
Utilizza la tecnica del ***divide et impera***:
- ***DIVIDI** il problema a metà
- risolvi i due sottoproblemi ricorsivamente
- **IMPERA (fondi)**** le due sottosequenze ordinate
![[merge-sort-400.gif|center]]

PSEUDO-CODICE
```scss (MESSO A CASO)
MergeSort(A, i, f)
1.  if(i < f)  then
2.     m = PARTE_INFERIORE[(i + f) / 2]
3.     MergeSort(A, i, m)
4.     MergeSort(A, m+1, f)
5.     Merge(A, i, m, f)

6.  if(i = f) then return A[i] 
```
In pratica:
1. se `inizio < fine` entro nell'`IF`

2. trovo la metà

3. calcolo ricorsivamente il `MergeSort` della prima metà 
	- fintanto che `i < f` (quindi fino a quando non ho UN SINGOLO ELEMENTO DA PASSARE AL `MERGESORT`) calcolo ricorsivamente; 
	- quando ho un singolo elemento vuol dire `i = f`, quindi ritorno quel valore
	- a questo punto ho i risultati della riga 3 e 4 del penultimo MergeSort chiamato
	- ripercorro tutto tornando sopra e avrò il risultato finale della riga 3 chiamata la prima volta (in pratica ho la prima metà ordinata)

4. calcolo ricorsivamente il MergeSort della seconda metà
	- rifaccio la stessa cosa che ho scritto nel punto 3

5. QUANDO HO IL RISULTATO DEL PUNTO 3 E 4 posso eseguire il merge.

ALBERO DELLE CHIAMATE RICORSIVE
![[Pasted image 20241028221651.png|center]]

#### MERGE
Il merge può essere eseguito quando ho due liste A e B ordinate, quindi posso fonderle rapidamente:
- estraggo ripetutamente il minimo di A e B e lo copio nell'array di output, finché A oppure B diventa vuoto
- copio gli elementi dell'array non vuoto alla fine dell'array di output.
![[ezgif.com-animated-gif-maker.gif]]
PSEUDOCODICE
```scss
Merge (A, i₁, f₁, f₂)

1.  Sia X un array ausiliario di lunghezza f₂ - i₁ + 1
2.  i = 1; k₁ = i₁
3.  k₂ = f₁ + 1
4.  while (k₁ ≤ f₁ e k₂ ≤ f₂) do
5.      if (A[k₁] ≤ A[k₂])
6.          then X[i] = A[k₁]
7.          incrementa i e k₁
8.      else X[i] = A[k₂]
9.          incrementa i e k₂
10.  if (k₁ ≤ f₁) then copia A[k₁; f₁] alla fine di X
11.  else copia A[k₂; f₂] alla fine di X
12.  copia X in A[i₁; f₂]
```
SPIEGAZIONE
La funzione Merge prende come argomenti
- l'array($A$)
- la posizione iniziale($i_1$)
- la posizione al centro($f_1$)        
- la posizione finale($f_2$)

Funzionamento
- crea un array lungo quanto la posizione $finale-inizio+1$
- $i$ è uguale a $1$ perché alcune volte lo pseudo codice si fa con array che partono da $1$
- $k_1$ prende il valore iniziale; $k_2$ prende il valore dell'inizio della seconda parte dell'array
- inseriamo gli elementi dei vari array scambiando in base alla necessità quale prendere in considerazione dei due per l'inserimento
    - ad ogni assegnazione nell'array incremento $i$ e $k_1$ se uso l'array1 oppure $k_2$ se uso array2
- se alla fine del for $k_1$ deve ancora finire di inserirsi del tutto metterò per completezza gli ultimi elementi alla fine dell'array, oppure copio gli ultimi elementi di $k_2$
- mettiamo x nelle posizioni dell'array che avevamo

###### COSTO MERGE
Il merge fonde due sequenze ordinate di lunghezza $n_1$ e $n_2$ in un tempo $$\Theta(n_{1}+n_{2)}= \Theta(n)$$questo perché
- ogni confronto "consuma" un elemento di una delle due sequenze, lunghe $n_1$ e $n_2$
- ogni posizione di X è riempita in tempo costante
- anche la linea 12 costa $\Theta(n_{1}+ n_2)$ 

#### COSTO MERGESORT
Abbiamo$$T(n) = 2T\left(\frac n 2\right)+O(n)$$ dove
- $2T\left(\frac n 2\right)$ è dato dalle due chiamate ricorsive `MergeSort` (riga 3 e 4) che diminuiscono proporzionalmente la dimensione degli array di $\frac n 2$ 
- $O(n)$ è dato dal Merge (riga 5); utilizziamo $O$ perché nelle ultime chiamate non lavoreremo propriamente su `n` elementi
- righe 1 e 2 hanno costo $O(1)$

###### TEOREMA MASTER
![[photo_2024-10-29_17-55-29.jpg]]La notazione asintotica rimane la stessa di $f(n)$. 

>[!question]- Quanta memoria (ausiliaria) usiamo?
>La complessità spaziale del MergeSort è $\Theta(n)$, perché:
>- il Merge usa memoria ausiliaria pari alla dimensione di porzione da fondere (ATTENZIONE, non crea subito un array di `n` elementi MA ad ogni chiamata sui vede quanti elementi ci sono nella riga 3 e nella riga 4 e crea il suo array di conseguenza)
>- non sono mai attive due procedure di Merge contemporaneamente (prima eseguo riga 3 e poi riga 4)
>- ogni chiamata di MergeSort usa memoria costante (esclusa la memoria usata dal Merge)
>- il numero di chiamate di MergeSort attive contemporaneamente sono $O(log(n))$
>
>Il MergeSort non ordina ***in loco*** ma creo un nuovo array, lo ordino e lo stampo.


### QuickSort
Anche lui usa la tecnica del divide et impera
- **DIVIDE**: scegli un elemento x della sequenza (perno) e partiziona la sequenza in elemento $\le x$ ed elementi $> x$ 
- risolvi i due sottoproblemi ricorsivamente
- **IMPERA**: restituisci la concatenazione delle due sottosequenze ordinate
Rispetto al MergeSort abbiamo un DIVIDE COMPLESSO e un IMPERA SEMPLICE

Il funzionamento in linea di massima è il seguente:
1. scegli un perno
2. scorri l'array "in parallelo" da sinistra verso destra e da destra verso sinistra
	- da sinistra verso destra, quando trova un elemento maggiore del perno si ferma
	- da destra verso sinistra, quando trova un elemento minore del perno si ferma
3. scambio i due elementi su cui mi sono fermato e riprendo la scansione dal punto 2
4. mi fermo quando i due indici si sono incrociati

##### Funzionamento algoritmo Partition  
![[Pasted image 20241029211356.png|center|600]]

##### PSEUDOCODICE Partition 
![[Pasted image 20241029211428.png]]
- ogni volta viene creato un perno $x$ dato dalla posizione $A[i]$
- la parte inferiore è data da i
- la parte superiore dalla fine +1

- facciamo un ciclo while che continua finché non si attiva il break
	- abbiamo quei due cicli che dicevamo prima che scorrono da dx a sx e viceversa che si fermano se trovano una contraddizione della richiesta di una tipica lista ordinata
	- ha senso il fatto che da sx verso dx cerchi il maggiore perché così se abbiamo sempre numeri minori significa che siamo su un punto già ordinato e che non dobbiamo togliere
	- stessa cosa vale da dx a sx
- una volta terminati i due cicli procediamo a fare uno scambio se inf continua a essere più piccolo del sup (significa che i due non si stanno incrociando)
- se si incrociano usciamo e mettiamo il perno al centro perchè sup sarà arrivato dove doveva arrivare(se ha fatto poca strada metteremo il perno scelto alla fine dell'array)
- restituiamo la posizione del perno

#### PSEUDOCODICE Quick Sort 
```scss
QuickSort(A, i, f)
1. if (i < f) then
2.     m = Partition(A, i, f)
3.     QuickSort(A, i, m - 1)
4.     QuickSort(A, m + 1, f)
```
L'algoritmo di per sé fa questo
1. se i = f vuol dire che ho solo un elemento da controllare
2. Partition, dato un perno "m", mette tutti gli elementi più piccoli di m a sinistra e a destra i più grandi e infine inserisce m "al centro";
	io ora so che a sinistra del perno ho tutti gli elementi più piccoli (non per forza ordinati) e a destra stessa cosa ma con i più grandi
3. eseguo il QuickSort sulla parte a sinistra
4. eseguo il QuickSort sulla parte a destra

###### RAPPRESENTAZIONE GRAFICA
![[Pasted image 20241030165314.png]]

>[!warning]- ATTENZIONE
>Gli scambi avvengono così
>- 5 è il perno
>
>**ENTRO NELLA RIGA 5**
>SINISTRA
>- il secondo 5 va bene
>	- incremento `inf`
>- 6 NON VA BENE 
>	- si blocca e di conseguenza `inf` non viene incrementato
>	
>**ESCO DALLA RIGA 5**
>
>**ENTRO NELLA RIGA 6**
>DESTRA
>- 4 NON VA BENE
>	- si blocca
>	
>**ESCO DALLA RIGA 6**
>
>**ENTRO NELLA RIGA 7**, `inf < sup`
>SCAMBIA 6 E 4 (quindi `A[inf]` e `A[sup]`)
>
>incremento `inf` e decremento `sup`
>
>**ENTRO NELLA RIGA 5**
>SINISTRA
>- 7 NON VA BENE
>	- si blocca
>	
>**ESCO DALLA RIGA 5**
>
>**ENTRO NELLA RIGA 6**
>DESTRA
>- 3 NON VA BENE
>	- si blocca
>	
>**ESCO DALLA RIGA 6**
>
>**ENTRO NELLA RIGA 7**, `inf < sup`
>SCAMBIA 7 E 3
>
>incremento `inf` e decremento `sup`
>
>**ENTRO NELLA RIGA 5**
>SINISTRA
>- 1 va bene 
>	- incremento `inf` 
>- 2 va bene
>	- incremento `inf` 
>- 7 NON VA BENE
>	- si blocca
>
>**ESCO DALLA RIGA 5**
>
>**ENTRO NELLA RIGA 6**
>DESTRA
>- 2 NON VA BENE
>	- si blocca 
>	
>**ESCO DALLA RIGA 6**
>
>La condizione di riga 7 non è più verificata perché `inf > sup` e quindi NON POSSO SCAMBIARE.
>`A[sup]` corrisponde a 2
>
>SCAMBIO IL `PERNO` CON `A[sup]`.

#### Analisi caso peggiore
Sappiamo che ogni invocazione di Partition posiziona almeno un elemento in modo corretto (il perno).
Dopo n invocazioni di Partition, ognuna di costo O(n) ho un vettore ordinato.
Costo complessivo $$O(n^2)$$(DATO DALLA FORMULA $NUMERO \space CHIAMATE \times COSTO \space DI \space OGNI \space CHIAMATA$)

Il caso peggiore si verifica quando il perno scelto ad ogni passo è o il minimo o il massimo degli elementi dell'array.
La complessità in questo caso è$$T(n) = T(n-1) + T(0) + O(n)$$ $$= T(n-1) + O(1)+O(n)$$  $$= T(n-1) + O(n)$$QUINDI$$T(n) = O(n^2)$$
Spiegazione di alcuni concetti
- IPOTIZZIAMO IO STIA PRENDENDO IL MINIMO AD OGNI CHIAMATA
	- ogni volta quindi posizionerò il perno nell'attuale prima posizione
	- così facendo la riga 3 del codice non può essere eseguita
	- da qui $T(0)$

	- Quando arrivo ad avere $T(n-1) + O(n)$ io uso la formula $NUMERO \space CHIAMATE \times COSTO \space DI \space OGNI \space CHIAMATA$ 
	- Quante chiamate ho all'interno di $T(n-1)$? `n`
	- Quanto costa ogni chiamata? $O(n)$
	- Quindi $n \times n = n^2$ 


#### Caso migliore
È facile intuire che il miglioramento di costo avviene quando le partizioni sono bilanciate ovvero il perno viene posto al centro dell'array, mentre all'aumentare dello sbilanciamento il tutto si complica. 


#### Caso medio
Nel caso medio si osserva che anche se il 90% degli elementi sta a sinistra del perno e il restante 10% a destra il QuickSort ha comunque un costo di $$nlog(n)$$![[Pasted image 20241030181055.png]]
Tutti i vari casi medi dello stesso problema avranno la medesima altezza dell'albero.

#### VERSIONE RANDOMIZZATA
![[Pasted image 20241030183634.png]]