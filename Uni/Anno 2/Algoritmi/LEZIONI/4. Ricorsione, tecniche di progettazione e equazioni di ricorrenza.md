### Ricerca di un elemento in un array/lista NON ordinato
L'algoritmo torna la posizione di x in L se x è presente, -1 altrimenti
```c
algoritmo RicercaSequenziale(array L, elem x) --> intero
	n = lunghezza L
	i = 1
	
	for i = 1 to n do
		if (L[i] = x) then return i  // trovato
	return -1  // non trovato

	/* CALCOLO T(n)
		T_worst(n) = n  // sia che x stia in ultima posizione o non ci sia proprio
		T_avg(n) = (n + 1) / 2  // x può trovarsi in qualsiasi posizione di L
	
	QUINDI
		T_worst(n) = THETA(n)
		T_avg(n) = THETA(n)
```


### VARIANTE: ricerca di un elemento in un array ORDINATO
Algoritmo di ricerca binaria.
Gli indici $i$ e $j$ inizialmente corrispondono alla posizione iniziale e finale della lista.
L'algoritmo ritorna la posizione x in L, se x è presente, -1 altrimenti
```c
algoritmo RicercaBinariaRic(array L, elem x, int i, int j) --> intero
	if (i > j) then return -1
	m = [ (i + j) / 2]  // SAREBBE LA PARTE INFERIORE
	
	if(L[m] = x) then return m
	if(L[m] > x) then return RicercaBinariaRic(L, x, i, m-1)
				 else return RicercaBinariaRic(L, x, m+1, j)

	COSTO T(n)
		T(n) = T(n/2) + O(1)  -->  T(n) = O(log(n))
```


![[Pasted image 20241016113651.png]]

## Come analizzare un algoritmo
La complessità computazionale di un algoritmo ricorsivo può essere espressa in modo naturale come una ***equazione di ricorrenza***.


METODI PER SVOLGERE LE EQUAZIONI DI RICORRENZA.
### Metodo dell'iterazione
IDEA: "srotolare" la ricorsione, ottenendo una sommatoria dipendente solo dalla dimensione `n` del problema iniziale.
![[Pasted image 20241020165422.png|center|400]]

>[!question]- Perché $i = log_2(n)$?
>Dal passo $i-esimo$ voglio arrivare a trovare $T(1)$, quindi l'unico modo per trasformare $\frac{n}{2^i}$ in $1$ è con $i = log_2(n)$



![[Pasted image 20241020165440.png|350]]![[Pasted image 20241020165517.png|350]]![[Pasted image 20241020165606.png|center]]


### Tecnica albero della ricorsione
IDEA:
- disegnare l'albero delle chiamate ricorsive indicando la dimensione di ogni nodo
- stimare il tempo speso da ogni nodo dell'albero
- stimare il tempo complessivo "sommando" il tempo speso da ogni nodo

>[!tip] SUGGERIMENTI
>1) se il tempo speso da ogni nodo è costante, T(n) è proporzionale al numero di nodi
>2) a volte conviene analizzare l'albero per livelli:
>	- analizzare il tempo speso su ogni livello (fornendo upper bound)
>	- stimare il numero di livelli

#### ESEMPI
![[Pasted image 20241020174327.png|center|500]]
- Il ***costo*** è `1` perché $$T(n) = T(n-1)+1$$ e aprendo T(n-1)$$T(n-1) = T(n-2)+1$$ QUINDI $$T(n) = T(n-2)+2$$e se vedi ogni volta incremento il tutto di `1`.

Il costo TOTALE è dato da$$NUMERO \space DI \space NODI \space \times \space COSTO \space DI \space OGNI \space NODO$$Quindi $$1 \times n = n$$ che è ESATTAMENTE l'input, quindi$$T(n) = \Theta(n)$$

![[Pasted image 20241021115110.png]]
Qui ora abbiamo $T(n) = T(n-1) + n$, quindi ogni nodo avrà come costo AL PIÙ n, quindi usiamo come notazione $O$.

Per arrivare a capire se è un $\Theta$ dobbiamo creare un Lower Bound tramite un $\Omega$.

Possiamo prendere la metà dei nodi, quindi $\frac n 2$ nodi che ognuno costerà ***AL MENO*** ($\ge$) $\frac n 2$ (questo perché originariamente io ho $T(n) = T(n) + n$ e quindi tutto dipende da `n`, se io prendo metà dei nodi, il costo sarà comunque AL MENO $\frac n 2$).
![[Pasted image 20241021115641.png]]

##### UN ALBERO EFFETTIVO
La classica struttura ad albero la avrò in casi tipo $T(n) = 2T(n) + O(1)$, infatti  ad ogni ricorsione mi si sdoppia il tutto, quindi creo due rami.
![[Pasted image 20241021121444.png]]

>[!question]- Perché l'albero è alto `n-1`?
>h è uguale al cammino più lungo, ossia il numero di collegamenti tra `n` e `1`.
>Nell'esempio, seppur non completo, puoi contare 5 righe = 4 collegameni.

>[!question]- Perché $\sum_{i=0}^{h} 2^i = 2^{h+1} - 1$?
>È una SERIE GEOMETRICA
>![[Pasted image 20241021122129.png]]
>
>Nel nostro caso
>![[Pasted image 20241021121939.png]]


ALTRO ESEMPIO
![[Pasted image 20241021162557.png]]
Abbiamo:
- LOWER BOUND = $log_3(n)$
- UPPER BOUND = $log\frac{3}{2}(n)$  
Se vedi il ramo tutto a destra scende di più rispetto a quello a sinistra, quindi i LIVELLI TOTALI sono dettati da $log_\frac{3}{2}(n)$.
Dato che è l'upper bound possiamo affermare che il costo sarà $O(n \space log(n))$ 

>[!warning] ATTENZIONE
>Tu crederai che $n \space log(n)$ sia in base 2, beh anche io ma Valerio dice che la base del $log$ è una costante e nella notazioni noi le togliamo. QUINDI BOH.

##### Per calcolare il Theta utilizziamo il lower bound:
![[Pasted image 20241021163337.png]]


### Metodo sostituzione
Passaggi:
- abbiamo un'equazione di ricorrenza
- intuiamo asintoticamente una forma della nostra funzione (ES. $O(n^2)$); in pratica l'andamento
- utilizza l'induzione per verificare l'ipotesi
- risolvi sostituendo delle costanti al posto delle $c$ 

#### ESEMPIO
![[Pasted image 20241022194000.png]]


>[!question]- Ma io come posso ipotizzare l'andamento?
>Prendiamo questo come esempio
>![[Pasted image 20241022195512.png]]![[Pasted image 20241022195633.png]]
>Se vedi il professore utilizza per lo stesso esercizio due ipotesi diverse.
>Questo perché? Proviamo a mettere insieme le nostre conoscenze dello "srotolamento" e albero.
>Noi dall'albero sappiamo che il costo totale è dato da $$Numero \space di \space nodi \space \times \space Costo \space di \space ogni \space nodo$$ 
>
>Come calcolare il costo di ogni nodo? 
>Il costo è quel valore che si trova sommato fuori da $T(QUALCOSA)$. In questo caso abbiamo $$T(n) = 4T(\frac n 2) + n$$ quindi il costo è _**n**_.
>
>Ora cerchiamo di intuire "ad occhio" il numero dei nodi.
>Sappiamo che se $T(n) = T\left(\frac n 2\right)+ QUALCOSA$ allora l'andamento sarà $log(n)$.
>
>Quindi:
>- Numero di nodi = $log(n)$
>- Costo di ogni nodo = $n$
>
>Uniamo entrambe le cose e il costo totale sarà$$n log(n)$$
>
>Quindi INTUITIVAMENTE possiamo ipotizzare che il costo sarà MAGGIORE di $n$, da qui derivano le due ipotesi $O(n^3)$ e $O(n^2)$.
>Diciamo che prima ti tieni "alto", mettendo come ipotesi un qualcosa di meno efficiente; se lo verifichi inizi a scendere.

#### ESEMPIO
![[Pasted image 20241023102523.png]]

Perfetto, quindi è un $O(n^3)$, scendiamo un po' e vediamo se è un $O(n^2)$.
![[Pasted image 20241023102447.png|center|500]]
Mettendo una sola $c$ non siamo in grado di dimostrarlo perché arriviamo alla fine con un qualcosa di impossibile. Abbiamo bisogno di più informazioni.

![[Pasted image 20241023104739.png]]


### Tecnica del divide et impera
Il 90% delle volte, questa tecnica, si può applicare se abbiamo una formula di questo tipo:
![[Pasted image 20241023113448.png|]]

Ad esempio se prendiamo l'algoritmo Fibonacci 6, il suo costo è$$T(n) = T\left(\frac n 2\right)+1$$dove:
- a = 1
- b = 2
- f(n) = O(1)

Oppure l'algoritmo 4 della pesatura$$T(n)= T\left(\frac n 3\right)+1$$dove:
- a = 1
- b = 3
- f(n) = O(1)

## Teorema master, enunciato
$$n^{log_{b}(a)} \space \space vs \space \space f(n)$$quale va più velocemente a infinito?
- stesso ordine asintotico  $$T(n) = \Theta(f(n) log(n))$$
- se una delle due è "polinomialmente" più veloce $$T(n) \space ha \space l'ordine \space asintotico \space della \space più \space veloce$$
In pratica le soluzioni saranno:
1. $T(n) = \Theta(n^{log_{b}(a)})$   
	- se   $f(n) = O(n^{log_{b}(a) - \epsilon})$ per $\epsilon > 0$
		in pratica se $f(n) \le n^{log_{b}(a)}$ 

2. $T(n) = \Theta(n^{log_{b}a}log(n))$  
	- se  $f(n) = \Theta(n^{log_{b}(a)})$
		in pratica se $f(n) = n^{log_{b}(a)}$ 

3. $T(n) = \Theta(f(n))$   
	- se   $f(n) = \Omega(n^{log_{b}(a) + \epsilon})$ per $\epsilon > 0$ 
		(se le operazioni extra sono più costose della ricorsione)
	- e    $af\left(\frac n b\right) \le cf(n)$ per $c < 1$ e $n$ sufficientemente grande
		
		in pratica se $f(n) \ge n^{log_{b}(a)}$ 

### ESEMPI
1. $T(n) = n + 2T(\frac n 2)$
	- a = 2
	- b =2
		- $n^{log_{b}(a)} = n^{log_{2}(2)} = n$
		
	- $f(n) = n$
	
	CASO 2 DEL TEOREMA MASTER $$f(n) = \Theta(n)$$allora:$$T(n) = \Theta(n \space log (n))$$

2.  $T(n) = c + 3T(\frac n 9)$
	- a = 3
	- b = 9
		- $n^{log_{b}(a)} = n^{log_{9}(3)} = \sqrt n$ 
		
	- $f(n) = c$
	
	CASO 1 DEL TEOREMA MASTER $$f(n) = O(\sqrt n)$$allora:$$T(n) = \Theta(\sqrt n)$$

3. $T(n) = n + 3T(\frac n 9)$
	- a = 3
	- b = 9
		- $n^{log_{b}(a)} = n^{log_{9}(3)} = \sqrt n$
	
	- $f(n) = n$
	
	CASO 3 DEL TEOREMA MASTER $$f(n) = \Omega(\sqrt n)$$allora:$$T(n) = \Theta(n)$$

4.  $T(n) = n \space log(n) + 2T(\frac n 2)$
	- a = 2
	- b = 2
		- $n^{log_{b}(a)} = n^{log_{2}(2)} = n$
	
	- $f(n) = n \space log(n)$
	
	NESSUN CASO DEL TEOREMA MASTER $$f(n) = \omega(n^{log_{2}(2)})$$ma $$f(n) \ne \Omega(n^{log_{2}(2) \space + \space \epsilon})$$
	ALLORA NON POSSO APPLICARE IL TEOREMA MASTER

### Cambiamento di variabile
![[Pasted image 20241023130916.png]]
- La prima sostituzione, $n = 2^x$ la uso per ricondurmi ad una forma nota (in questo caso $O(log (x))$ alla fine).
- La seconda sostituzione la utilizzo per concludere il passaggio iniziato con la prima sostituzione (ossia arrivare a scrivere $O(log (x))$)