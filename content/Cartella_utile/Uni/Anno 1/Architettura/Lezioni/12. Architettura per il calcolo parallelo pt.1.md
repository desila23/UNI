# INTRODUZIONE 
L'obiettivo principale dell'industria dei computer è sempre stata quella di voler aumentare le performance, in passato si faceva aumentando il clock ma poi divenne impossibile per un limite fisico ovvero che un segnale non può essere più veloce della velocità della luce e per problemi di gestione delle temperature. un computer con un clock a $1 THz$ deve essere più piccolo di 100 $\mu m$ per dare abbastanza spazio al singolo ciclo di clock è fattibile ma ci sono comunque limiti strutturali di dissipazione del calore una soluzione a questo problema é l'architettura parallela ovvero più CPU che collaborano per il medesimo obiettivo.

# Classificazione di Flynn 
ci permette di classificare i tipi di architetture in base a flusso di istruzioni e flusso di dati 
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508142128.png]]


# Classificazione dei calcolatori 
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508142117.png]]

# ARCHITETTURE PER IL CALCOLO PARALLELO 
Il parallelismo aiuta appunto a migliorare le performance con tecnologie come il pipelining e le architetture superscalari si può arrivare ad un miglioramento dal 5 al 10. Con più CPU si arriva a un miglioramento addirittura dal 50 al 100% esistono tre approcci riguardo queste architetture: - Data parallel computers(SIMD) - Multiprocessors (MIMD) - Multicomputers (MIMD) 

# Le varie architetture 
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508142047.png]]
# Parallelismo nel chip 
Può avvenire in 3 modi: 
### Parallelismo a livello di istruzioni 
mettere più istruzioni per ogni ciclo di clock con due tipi di processori: 
- Superscalari con pipeline e unità funzionali parallele 
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508142038.png|center]]

- Processori VLIW (Very Long Instruction Word) con istruzioni lunghe che dividono il problema nelle varie componenti 
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508142020.png|center]]

## Multithreading nel chip 
Risolve un problema di stallo della pipeline, ovvero quando una CPU tenta di accedere ad una informazione in memoria e non nella cache e perciò per accedervi ha bisogno di tempo per caricarla prima di riprendere l'esecuzione. 
Il multithreading si divide in 3 approcci: 
- a grana fine 
- a grana grossa 
- di tipo simultaneo

>[!tip]- Prerequisito, che cos'è un Thread?
>È uno spezzettamento di un'istruzione in due o più parti, dette processi.
# Il problema dello stallo della pipeline
Supponiamo di avere una CPU che emette una istruzione per ciclo di clock con tre processi A, B, C, che durante il primo ciclo eseguono i thread A1, B1, C1.
Durante il secondo ciclo A2 fa un riferimento non presente nella cache del primo livello e deve attendere 2 cicli per recuperarlo dalla cache di secondo livello.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508144711.png]]

## Multithreading a grana fine
ABBIAMO UNA SOLA PIPELINE.
Tutte le singole istruzioni dei thread sono eseguire ogni ciclo di clock, a turno.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508151050.png]]
- Nei due cicli di stallo la CPU è occupata a svolgere le istruzioni degli altri due thread.
- Ogni Thread ha il proprio insieme di registri che sono definiti a priori in fase di progettazione del chip.
- Nella pipeline non ci sarà mai più di una istruzione per thread e quindi il numero massimo di thread è pari al numeri di stadi della pipeline.

## Multithreading a grana grossa
ABBIAMO UNA SOLA PIPELINE.
Un thread va avanti finché 
- non raggiunge uno stallo
- perde un ciclo
- commuta su un altro thread
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508151646.png]]
Esiste una variante che permette di "guardare avanti" le istruzioni anticipando lo stallo e approssimando il multithreading a grana fine.

## Multithreading in CPU superscalari
ABBIAMO DUE PIPELINE.
Le CPU possono avere più unità in parallelo ed emettere più istruzioni per ciclo.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508160442.png]]

### Multithreading parallelo
Ciascun thread emette due istruzioni per ciclo fintanto che può, altrimenti, non appena raggiunge uno stallo, viene emessa immediatamente un’istruzione del thread che segue affinché la CPU resti pienamente impegnata.

QUESTO ACCADE CON MULTITHREADING A GRANA FINE E GROSSA
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508163222.png]]

QUESTO ACCADE CON MULTITHREADING SIMULTANEO
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240508163247.png]]
Se noti gli stalli della prima tabella sono riempiti.

# Multiprocessori in un solo chip
Dato che fisicamente non è possibile incrementare la sequenza di clock per ottenere CPU più veloci, le industrie produttrici di chip hanno incominciato ad inserire più CPU (chiamate core) all'interno dello stesso chip (o meglio die).

I multiprocessori possono essere realizzati
- con core che svolgono le medesime attività (multiprocessori omogenei)
- con core aventi specifiche funzionalità (multiprocessori eterogenei)

## Multiprocessori omogenei
I multiprocessori condividono la cache di primo e secondo livello e la memoria principale.

Si dividono in altre due sotto-categorie:
1. Quelle con una sola CPU e più pipeline che possono moltiplicare il throughtput in base al numero di pipeline. 
2. Quelle che hanno più CPU (core) ciascuna con la propria pipeline.

Il processo di ***snooping*** (eseguito dall'hardware) garantisce che se una parola è presente in più cach e una CPU modifica il suo valore in memoria, essa è automaticamente rimossa in tutte le cache in modo da garantire consistenza.

# Multiprocessori Eterogenei
Un tipo di multicore in cui ogni core ha un compito specifico: poiché queste architetture realizzano un vero e proprio calcolatore completo in un singolo chip sono spesso dette ***system on a chip***.

I chip multicore sono come dei piccoli multiprocessori e per questa ragione vengono chiamati CMP.
Sfruttare questi chip prevede l'uso di algoritmi che hanno un uso in parallelo e sono molto complessi da realizzare per i chip multicore. Per farli comunicare tra loro viene usato un bus, ma per i sistemi più grandi ne serve più di uno o ad anello: l'anello è un sistema che fa comunicare i core tutti insieme collegandoli due a due formando un anello per poter trasmettere dei dati da core a core si usa un token che viene preso dalla singola cpu, quando lo prende può trasferire le informazioni e una volta riposto il token qualche altra cpu può prenderlo (tipo gioco delle sedie) il bus è frutto dell'interconnessione delle cpu quindi una cpu può passare le informazioni tra le cpu.

# Chip-level Multiprocessor
Multiprocessori al livello di chip, quindi molto piccoli, e la progettazione software è identica ai normali multiprocessori a bus ma al posto di avere una chace per ogni CPU hanno solo una in comune e per questo può saturarsi.

Un'altra differenza sta nel fatto che questi chip sono meno tolleranti ai malfunzionamenti (es. in CPUlator se faccio un ciclo while che si ripete all'infinito si blocca, mentre un normale multiprocessore proverebbe a risolverlo).

# Coprocessori
Processori che collaborano e che svolgono specifiche funzioni, permettono il miglioramento delle performance del processore principale poiché prendono il compito di svolgere quelle determinate funzioni.

Diversi tipi:
- grafici
- di rete
- crittografici

# MIMD (Multiple Instruction Multiple Data)
Dalla classificazione di Flynn, sono dei processori che lavorano con multiple istruzioni e dati multipli.

Tre tipi di MIMD:
1. Multiprocessori a memoria condivisa e il tempo di accesso ad un parola in memoria è di $2-10 ns$. Data la memoria condivisa non serve installare un SO per ogni CPU. Inoltre può esserci un processore che si occupa dello scambio I/O e in questo caso si parla di **multiprocessore asimmetrico***. 
	Esistono due differenti tipologie di multiprocessori:
	- UMA, le parole vengono lette in memoria tutte alla stessa velocità (Uniform Memory Access)
	- NUMA, le parole vengono lette in memoria tutte a velocità diverse (Not Uniform Memory Access)

### Esistono cinque tipi di UMA
#### Basati su BUS
Le CPU comunicano tramite un singolo bus e si collegano alla share memory; per evitare intoppi è sconsigliato l'utilizzo di troppe CPU.

#### Con singolo BUS e chace nella CPU
Ogni CPU ha una piccola chace personale oltre che alla share memory; questo può ridurre i tempi di attesa. Ogni blocco di chace è READ ONLY oppure READ/WRITE.

#### Con singolo BUS e CPU dotate di RAM
Oltre ad avere una chace hanno anche una RAM dedicata e la share memory serve solo per salvare la variabili globali.  A questo punto il compilatore deve spezzettare per bene il progetto per far sì che le CPU le gestiscano bene per scindere le variabili globali e locali.

#### Con crossbar switch
Dato che utilizzare un solo bus è un limite, per avere più parallelismi, si utilizza un circuito chiamato commutatore, ossia una griglia che instrada il segnale azionando e spegnendo vari interruttori, ovvero non succede mai che venga negata a una CPU la connessione di cui ha bisogno. 
In qualsiasi momento ogni CPU può collegarsi ad una memoria "libera", senza creare una sovrapposizione, altrimenti si creerebbero i problemi del bus singolo.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240607170327.png]]
Un altro problema sta nel fatto che avendo il rapporto tra CPU e memorie identico si va ad occupare molto spazio (con un bus singolo, pur non essendo ottimizzato, si aveva un solo bus che collegava tutto, qui invece hai $n^2$ fili, dove n = numero di CPU/memorie).

#### Con reti a commutazione multilivello
Basata su commutatore 2x2 (due ingressi e due uscite); i messaggi che provengono da ciascuno dei due input possono essere indirizzati verso ognuna delle due uscite. 
Ogni messaggio contiene:    
- Modulo, che specifica la memoria da usare;
- Indirizzo, specifica un indirizzo all’interno di quel modulo;
- Opcode, che stabilisce il tipo di operazione, per esempio READ o WRITE;
- Valore, che potrebbe contenere un operando.
![](https://lh7-us.googleusercontent.com/VqCQsPwChU6VeGUb7FU3uQ546w57ZOP4lx0FqIUuocMoBEUxem0cAaPg3me0elCXJjoJZG0u4t1YiHx8j2b3nLfPSNqfXqNcb4Qs4sySufbm58Ro6X-hd3TBv5q70POidPMoHwH9yU9q38-hS5Po4A)
   
Una possibilità economica e semplice è la rete omega ![](https://lh7-us.googleusercontent.com/BpsOsItK9-U-4qWbcltg7aprraae1rWw3mawlJ33tnU5xtSCasuTfuOfznVgBvFq_4J_TLQY7c1Jk3gmX2z6tKWQhhZg3JEHxyqorADzuWaQBELcSwmzaZ_3v2FisyDp6fAWAONM5X3GfzKsyT08cw)
In questo caso ci sono 8 CPU connesse a 8 memorie tramite 12 commutatori. In generale, per collegare n CPU a n memorie ci vogliono $log_2n$ livelli con $\frac n 2$ commutatori ciascuno, per un totale di $(\frac n 2)log_2n$ commutatori: è un numero molto inferiore a $n^2$, specie per valori grandi di n.

>[!tip]- FUNZIONAMENTO
>Per capire il funzionamento di una rete omega, supponiamo che la CPU 011 voglia leggere una parola dal modulo di memoria 110. La CPU spedisce un messaggio read contenente 110 nel campo Modulo al commutatore 1D. Il commutatore preleva il primo bit (quello più a sinistra) da 110 e lo usa per l’instradamento. Lo 0 instrada verso l’uscita in alto, 1 verso la linea in basso. Poiché il bit vale 1 il messaggio viene instradato a 2D attraverso l’uscita in basso. Tutti i commutatori di secondo livello, compreso 2D, usano per l’instradamento il secondo bit. In questo caso è ancora un 1 e così il messaggio viene spedito a 3D attraverso l’uscita in basso. Qui viene testato il terzo bit che vale 0. Di conseguenza il messaggio fuoriesce dalla linea in alto e raggiunge la memoria 110, come richiesto.
>
>QUESTO È UN ALTRO ESEMPIO
>![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240607115751.png]]

Il problema qui si riscontra quando una CPU prova ad usare un commutatore già utilizzato da un'altra CPU, ad esempio se 001 e 101 si collegano entrambe a 1B e questo non può avvenire.

# Multiprocessori NUMA
Per superare le 100 CPU bisogna rinunciare a qualcosa, spesso all’idea che tutti i moduli di memoria richiedano lo stesso tempo d’accesso. Questa rinuncia conduce ai multiprocessori NUMA (NonUniform Memory Access) che, come i cugini UMA, gestiscono un solo spazio degli indirizzi per tutte le CPU, ma diversamente da loro, garantiscono un accesso più veloce ai moduli vicini rispetto a quelli lontani. 

Le macchine NUMA hanno tre caratteristiche chiavi:
- c’è un unico spazio di indirizzamento visibile a tutte le CPU;
- l’accesso alla memoria remota è attraverso istruzioni di LOAD e STORE;
- l’accesso alla memoria remota è più lento dell’accesso rispetto alla memoria locale.

Ci sono due tipi di macchine NUMA:
- No Cache NonUniform Memory Access (NC-NUMA): quando il tempo di accesso alla memoria distante non è nascosto (perché non c’è caching);
- Cache Coherent NonUniform Memory Access (CC-NUMA): quando sono presenti cache coerenti.

### NC-NUMA
Contiene varie CPU, ciascuna dotata di una piccola memoria cui accedeva tramite un bus locale. Inoltre, le CPU erano collegate attraverso un bus di sistema.![](https://lh7-us.googleusercontent.com/C5qS0y0dY3_2d3XMqQ82Ebkj82DOIG3IQ2wdigchXzfIKNZ9nsGuDerwhMzopjETkc8t_ob3-Aiu3GGsdbV-A_REdm0Ak9_GKYtO7-oFQGaYp5jKKXNyr1xvQxXbau_inNY3HyZ1EXeYtM6ph96j2w)
L'MMU è il controllore che permette al bus locale di accedere al bus di sistema.
## CC-NUMA
Attualmente il modo più diffuso per costruire grandi CC-NUMA è il sistema multiprocessore basato su directory (directory-based multiprocessor). L’idea è quella di mantenere un database per ricordare la collocazione e lo stato di ciascuna linea di cache.    
Fanno uso di snooping. 
L’unione di tutte le memorie dei nodi costituisce l’intero spazio di indirizzamento.

# MULTICOMPUTER
Il secondo progetto di architettura parallela prevede per ogni CPU una memoria privata, cioè accessibile solo da essa e non dalle altre tramite semplici istruzioni LOAD e STORE, ma cui nessun altra CPU può accedere. 
Programmare un multicomputer è molto più difficile che programmare un multiprocessore. 
D’altra parte i multicomputer grandi sono molto più semplici ed economici da costruire rispetto a multiprocessori con lo stesso numero di CPU. ![](https://likingaxis.github.io/UNI/UNI/UTILITY/Pasted-image-20240511171623.png)

## SISTEMI DISTRIBUITI
Sono computer che lavorano su aree geografiche estese come internet e i messaggi impiegano dai 10 ai 100 ms sono tipo i client e i server
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240607123444.png|center]]



# Tipi di sistemi operativi multiprocessore
### Ogni CPU ha un proprio sistema operativo e memoria privata
Le CPU operano come computer indipendenti. 
Una ovvia ottimizzazione è rendere possibile la condivisione del sistema operativo tra CPU e fare copie private delle sole strutture dati del sistema operativo.
![](https://lh7-us.googleusercontent.com/ehe8fpxbAJDpVzMKQvj9u5XpfNtUf08zUS6UwysMKp729vHEee3FEktFSDDqDUxZPOurNW6cE0T9LkI-KCsWJBekxCYXUoS6gx4X9hePafTp55L47ayAlVpKcZbr8QF455ItE8Vk_0XijgeHENlFcg)
Questo schema è migliore rispetto a n computer separati, poiché permette alle CPU di condividere un insieme di dispositivi di I/O. Però, presenta diverse problematiche:
- quando un processo effettua una chiamata di sistema, essa è intercettata e gestita dalla propria CPU, utilizzando le strutture dati presenti nelle tabelle del proprio sistema operativo;
- non c’è condivisione di processi, ogni CPU gestisce indipendentemente i propri processi;
- non c’è condivisione di pagine;
- se un sistema operativo mantiene un buffer cache dei blocchi del disco usati recentemente, indipendentemente dagli altri, questo può condurre a letture inconsistenti (causate da letture sporche nei vari buffer).

### Multiprocessori Master-Slave
In questo schema il sistema operativo e le sue tabelle sono presenti sulla CPU 1, e non sulle altre; quindi tutte le chiamate di sistema sono reindirizzate alla CPU 1 perché siano ivi elaborate.
![](https://lh7-us.googleusercontent.com/ehe8fpxbAJDpVzMKQvj9u5XpfNtUf08zUS6UwysMKp729vHEee3FEktFSDDqDUxZPOurNW6cE0T9LkI-KCsWJBekxCYXUoS6gx4X9hePafTp55L47ayAlVpKcZbr8QF455ItE8Vk_0XijgeHENlFcg)
Questo modello risolve la maggior parte dei problemi del modello precedente:
- c’è una singola struttura dati che tiene traccia dei processi pronti;
- la CPU master può bilanciare il carico sulle varie CPU;
- le pagine sono allocate dinamicamente tra i processori e c’è una sola buffer cache che evita le inconsistenze.

Il problema è che, con molte CPU, il master diventa un collo di bottiglia, perché deve gestire tutte le chiamate di sistema, quindi è un sistema utilizzabile avendo poche CPU

### Multiprocessori simmetrici
L’SMP (MultiProcessore Simmetrico) elimina l’asimmetria del modello master-slave: in pratica, ogni CPU può diventare master.
**![](https://lh7-us.googleusercontent.com/-HrKmSHoyNxySS-eTlVR3FXUIGBjwIzmRbyC7RXk0VJwc7dbTUmvYofPi9PoaBzktJbeL1Hau24scrFp8CCnF6nDvYiwRn0VPWvWjRalA9YWMhWcOeFXl2wj_OawWVp8JgfBNO0jyiEQpfTpcs5yoQ)**
Una copia del sistema operativo è in memoria e ciascuna CPU può eseguirla. 
La CPU invia una richiesta al kernel (programma situato al centro del sistema operativo che ha generalmente un controllo completo dell'intero sistema) e si aspetta una risposta, elaborandola successivamente.

Questo schema bilancia processi e memoria dinamicamente (visto che c’è solo un insieme di tabelle del sistema operativo) ed elimina il collo di bottiglia. 
D’altra parte introduce le seguenti problematiche:
- le CPU devono essere sincronizzate a causa della condivisione del sistema operativo;
- bisogna evitare i deadlock (potrebbe essere impossibile risolverli);
- la gestione è critica perché dipende dall’esperienza del programmatore nel contesto del sistema.
#### Sincronizzazione dei multiprocessori
Le CPU in un multiprocessore hanno bisogno di sincronizzarsi: le regioni critiche del kernel e le tabelle devono essere protette da mutex.

>[!tip]- MUTEX
>Procedimento di sincronizzazione fra processi o thread concorrenti con cui si impedisce che più task paralleli accedano contemporaneamente ai dati in memoria o ad altre risorse soggette a corsa critica

Innanzitutto occorrono dei pre-requisiti per la sincronizzazione. 
Nei sistemi operativi che girano su architetture monoprocessore, è relativamente semplice gestire l'accesso esclusivo a risorse condivise, come le tabelle critiche del kernel. Il kernel può semplicemente disabilitare le interruzioni per evitare che altri processi possano interrompere e accedere alla stessa risorsa mentre il processo corrente la sta modificando.

Nei sistemi multiprocessore, la situazione è più complessa perché la disabilitazione delle interruzioni su una singola CPU non impedisce alle altre CPU di operare. Questo significa che, anche se una CPU ha disabilitato le interruzioni per proteggere l'accesso a una risorsa, altre CPU nel sistema possono ancora tentare di accedere alla stessa risorsa, portando a potenziali conflitti o condizioni di gara.

Per gestire questa situazione, i sistemi multiprocessore si affidano a meccanismi di sincronizzazione più robusti come i mutex (mutual exclusion locks).

Quando una CPU ha bisogno di accedere a una regione critica:
1. **Richiesta di Lock**: La CPU tenta di acquisire il mutex associato a quella risorsa. Se il mutex è già detenuto da un'altra CPU, la CPU richiedente dovrà attendere (o "dormire") fino a quando il mutex non sarà rilasciato.
2. **Accesso alla Risorsa**: Una volta acquisito il mutex, la CPU può accedere in sicurezza alla risorsa condivisa, sapendo che nessun'altra CPU può accedervi finché il mutex è detenuto.
3. **Rilascio del Lock**: Dopo aver completato le operazioni sulla risorsa condivisa, la CPU rilascia il mutex, permettendo ad altre CPU in attesa di tentare di acquisirlo per accedere alla risorsa.

>[!tip] REGIONE CRITICA
>Porzione di codice in cui sono presenti tutte le informazioni necessarie ai thread e solo le informazioni che non possono essere condivise tra i thread.