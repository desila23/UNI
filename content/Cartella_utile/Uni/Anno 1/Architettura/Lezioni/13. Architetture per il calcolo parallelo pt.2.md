 # SCHEDULING
È la pianificazione e gestione delle risorse di elaborazione come:
- attività
- processi
- informazioni
Nel caso della CPU lo scheduler mette in ordine le risorse in base alla priorità e decide quanto tempo e quanta porzione di calcolo assegnare a qualsiasi programma.

Su un monoprocessore lo scheduling è ***mono-dimensionale***: avendo una singola CPU, la sola domanda cui si deve rispondere è: “qual è il prossimo processo da eseguire?”.
In un multiprocessore, lo scheduling è ***bidimensionale***: lo schedulatore deve decidere quale processo eseguire, e su quale CPU eseguirlo.

### Lo scheduling si divide in tre tipi:
#### Time-sharing
Funziona con processi indipendenti, quindi le risorse di uno non devono essere utili per un altro.
L'algoritmo utilizza un vettore che contiene tutti i processi (tutti nella stato "ready") con diverse priorità di esecuzione.
![center|600](https://lh7-us.googleusercontent.com/bT22XyUZ8M0bUffVMf1tWm_0dpKVW0OXaXjqU3xwzLiyzs3zlnlfKgyzJZOT_Jh8-vmNJz9ZSixd1kSY3dTZrRQ06UurarxoiHvosteQt73rjptah1yO8ysIq5waaZTvrSv2paEm9lkXmcvyUR_YEQ)
Il fatto di avere un’unica struttura dati utilizzata da tutte le CPU garantisce che tutte abbiano la stessa quantità di lavoro, come se ci trovassimo in un sistema a singola CPU.

Abbiamo diversi problemi:
- con troppe CPU tutte vogliono accedere al vettore per capire i lavori da svolgere
	
- Gestire i campi di processo, ad esempio una CPU deve passare dall'esecuzione di un processo all'esecuzione di un altro processo (es. I/O); in questo case viene innestato lo ***spin lock***, un meccanismo di protezione che aiuta la sincronizzazione tra le risorse della CPU e evita che $n$ CPU interagiscano con la stessa risorsa.
  Lo spin lock sic divide in:
	- spin, momento in cui la risorsa è in attesa di un interazione
	- lock, blocco della risorsa a tutte le CPU (tranne quella assegnata) perché già sta avendo un'interazione.

### METODI DI RISOLUZIONE
##### Smart scheduling
Lo scheduler assegna un ***quantum di tempo*** ad ogni CPU (un tot di tempo per eseguire un thread); se la CPU impiega più tempo, lo il thread richiede un extra-quantum (smart scheduling).

##### Affinity scheduling
Alcuni multiprocessore tentano di far eseguire lo stesso processo alla medesima CPU, dato che questa possiede una memoria cache (affinity scheduling).

Per creare questa affinità si utilizza un algoritmo di ***schedulazione a due livelli***:
- quando è creato un processo viene assegnato alla CPU più scarica (top-level scheduling);
- lo scheduling reale è fatto separatamente da ogni CPU (bottom-level scheduling) utilizzando l’affinità della cache e le priorità dei processi.

Lo scheduling a due livelli presenta tre vantaggi:
- distribuisce il carico in modo equo fra le CPU disponibili;
- l’utilizzo delle affinità della cache migliora le performance;
- si minimizzano le dispute sulle liste di processi poiché ogni CPU tenta di svolgere nuovamente processi che hanno già svolto, dato che conoscono le risorse necessarie.


#### Condivisione di spazio
Quando i thread non sono indipendenti ma complementari viene usato.

Il processo viene suddiviso in piccoli thread e lo scheduler assegna tot thread a tot CPU; essendo processi ***complementari***, lo scheduler fa avviare questi thread se e solo se tutte le CPU necessarie sono disponibili (es. se il processo ha bisogno di 10 thread lo scheduler cerca  CPU e finché non le trova tutte e 10 non avvia il processo).
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240513124002.png|center|]]
Un vantaggio evidente è la riduzione degli overhead (quando si esegue una determinata operazione che richiede troppi costi e risorse).
Uno svantaggio è il tempo sprecato quando si blocca una CPU.


#### Gang scheduling
Nasce dall'unione dello scheduling di tempo e di spazio, si compone in tre passaggi:
1. i gruppi di thread correlati sono schedulati come un’unità (o gang);
2. tutti i membri della gang sono in esecuzione simultaneamente, in differenti CPU in timesharing;
3. tutti i thread della gang iniziano e terminano la loro porzione di tempo contemporaneamente.

Questa schedulazione funziona perché tutte le CPU sono sincrone: ciò significa che il tempo è suddiviso in quanti discreti. Quando un quantum termina alle CPU viene di nuovo eseguita un'operazione di scheduling.
Se un thread si blocca, la sua CPU rimane inattiva fino allo fine del quanto.
![](https://lh7-us.googleusercontent.com/kruWiCOVJDIxDJM5sKu5bgKIuV1H7ilKaV1PLdAMIBpCj7qSvplPKekFJ_9R1vgMod5VcJ7jBrWVouNHKYN2rOKU-m9QiRZ1BSScEY4va5MoHTc2JZ1utfUe5N8V2rFhjP3ULod9KboEBb_4TLGN8Q)

L’idea della schedulazione gang consiste nell’avere tutti i thread di un processo in esecuzione insieme, cosicché se uno di essi manda una richiesta ad un altro, quest’ultimo riceverà il messaggio e sarà in grado di rispondere quasi immediatamente. 

Nella Figura 8.15, poiché tutti i thread di A sono in esecuzione insieme durante un quanto, in tale intervallo di tempo possono mandare e ricevere un gran numero di messaggi, eliminando così il problema della Figura 8.14.
**![|center||600](https://lh7-us.googleusercontent.com/Bt7lXXGZ-vEeVpAWzpByqdblH8CCK1Zqefx3v_yy0jTE3wW0DJ5nKSrluTkzgILI_-sYr_qMLqfJkfzpY7kDFoVjtwKE30pt8JEaVfd_tnBcZ3njGeZYkr_cnJorvrSXO9BadQw1UiQ05T5LyhUMCg)**

# MULTICOMPUTER A SCAMBIO DI MESSAGGI
A differenza del multiprocessore, questo tipo di architettura prevede che ogni CPU abbia una memoria privata a cui può accedere tramite semplici istruzioni di LOAD/STORE, solo la CPU stessa e non tutte le altre.
Dato che ogni CPU ha una memoria privata non è possibile utilizzare una memoria comune per lo scambio di informazioni e c'è quindi bisogno di un altro meccanismo di comunicazione che avviene lungo la rete di interconnessione.

La comunicazione tra i processi di un multicomputer avviene tramite normalissime operazioni di richiesta/risposta (send/recieve).

Questi sistemi sono anche chiamati Cluster (insieme) di Computers o Cluster of Workstations (COW).


# TOPOLOGIA
La topologia di una rete di interconnessione descrive il modo in cui sono disposti i collegamenti e i commutatori.
Le topologie sono composte da due componenti:
- nodi, i computer veri e propri con CPU, E UTILIZZANO LA SCHEDA DI RETE (UN COPROCESSORE) PER SCAMBIARSI DATI (tramite fili, rete e così via)
- switch (opzionale), gestiscono lo scambio di pacchetti tra i vari nodi 

### Diversi tipi di topologia
#### A stella
Tutti i computer sono collegati ad un singolo switch a cui inviano i pacchetti di informazioni e il destinatario.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240513145356.png|center|]]

### Ad anello
Ogni nodo è connesso ad altri due nodi in ordine per formare un anello circolare (non sono necessari gli switch).
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240513150040.png||center|200]]
### Ad albero
I computer stanno solo in basso, quelli che vedi sulle prime tre file sono SWITCH.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240513150104.png|center|270]]
### Grid o Mesh
È un progetto bidimensionale, utilizzato in molti sistemi complessi. È altamente regolare quindi altamente scalabile. Il percorso più lungo tra due nodi (diametro) aumenta come la radice quadrata del numero dei nodi. 
SEMBRA IL CROSSBAR SWITCH WOWWWWWWW
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240513150126.png||center|250]]
### Doppio toro
Variante della griglia con i nodi estremi che si congiungono, è più tollerante ai guasti ma con un diametro più piccolo.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240607163437.png|center]]
### Cubo
Ogni nodo ha sei vicini, due lungo ciascun asse. I nodi sui bordi hanno collegamenti che li collegano al bordo opposto, proprio come in un toro. 
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240607163452.png|center]]

### Ipercubo superscalare
Molti computer paralleli usano questa topologia perché il suo diametro (distanza massima tra due nodi) cresce linearmente con la sua dimensione: diametro = $log_2$ numero dei nodi.
In figura viene mostrato un ipercubo a quattro dimensioni costruito a partire da due cubi mediante il collegamento dei nodi corrispondenti. 
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240513151028.png|center| 400]]


# CLUSTER
Centinaia o migliaia di PC o di workstation (stazioni di lavoro) collegate per mezzo di schede di rete reperibili sul mercato. 

Le tipologie dominanti sono due: 
- ***centralizzati***: cluster di workstation o di PC montati su grossi scaffali in un unico ambiente. Si tratta in genere di macchine omogenee
- ***decentralizzati***: formati da workstation e PC diffusi all’interno di un edificio o di un intero campus. Si tratta in genere di macchine eterogenee e dotate di un ricco equipaggiamento di periferiche.


# SCHEMI DI SWITCHING
Nei multicomputer sono usati due tipi di schemi di switching.
### Store-and-forward packet switching (connection-less)
Ogni messaggio è suddiviso in pacchetti che sono poi inseriti nella rete. Il pacchetto raggiunge il nodo destinatario .
Questo sistema è flessibile ed efficiente ma ha il problema dell’incremento dei tempi di latenza lungo la rete di interconnessione.

### Circuit switching (connection-oriented)
Il primo switch stabilisce un collegamento fisico, attraverso tutti gli switch coinvolti, fino allo switch del nodo destinazione. Una volta che la connessione è creata i bit sono spediti alla massima velocità possibile. 
Questo sistema richiede una fase di inizializzazione che prende tempo, ma una volta terminata il processo è velocissimo.

Una variante del circuit switching è il ***wormhole routing***, spezza il pacchetto in sottopacchetti e permette a quest’ultimi di iniziare il tragitto prima che sia stato inizializzato il collegamento.


# INTERFACCIA DI RETE
Ogni computer (nodo) del multicomputer ha almeno una scheda di rete e una memoria RAM (per memorizzare i pacchetti che entrano ed escono dal nodo)
**![center|500](https://lh7-us.googleusercontent.com/RDs3xV9ON6h81oQe6_uq2ntZWXVy8X8EE6KScRC6dwCfxJVTyuFvJl9qYcAriHEuLAqNC66yGLUJ6lMlHAAB2Y1obnW9G3X6w-7czdZ9hEHegZwRuoLM2VaYLYVvD6ttl4Fyktxy11pA0HOsaLKu5A)**

La scheda d’interfaccia può avere uno o più canali DMA oppure una CPU completa (processori di rete). I canali DMA possono copiare i pacchetti tra la scheda d’interfaccia e la RAM principale ad alta velocità, richiedendo i trasferimenti dei blocchi sul bus di sistema.


# SOFTWARE COMUNICAZIONE DI BASSO LIVELLO
L'eccessiva copia di pacchetti è il nemico principale delle prestazioni dei multicomputer (i pacchetti vengono copiati in per carie ragioni tra cui evitare perdita di dati, backup ecc...). Per vitare che ciò accada alcuni multicomputer accedono direttamente alla scheda di rete per ricevere o inviare dati. Se così non fosse tutta l'operazione di invio/ricezione dati sarebbe gestito dal sistema operativo che dovrebbe fare chiamate multiple di sistema per gestire il tutto, questo porta a creare molteplici copie dei pacchetti durante il passaggio dati tra utente e kernel.

Problemi: 
- difficile da programmare
- rischi di sicurezza poiché si accede direttamente alla scheda di rete
- potrebbero esserci tanti processi sullo stesso nodo che vogliono spedire pacchetti.

# SOFTWARE DI COMUNICAZIONE A LIVELLO UTENTE
I processi sulle CPU di un multicomputer comunicano attraverso lo scambio di messaggi (send e receive).

Come funzionano?
 - ***SEND***: Questa funzione invia un messaggio ad un processo identificato come destinazione (destination). Quando si chiama la funzione SEND, si specifica il destinatario del messaggio e un puntatore al messaggio stesso (message_pointer). Il chiamante, ovvero il processo che esegue la chiamata SEND, viene bloccato fino a quando il messaggio non viene effettivamente spedito. Questo significa che il processo chiamante rimane in sospeso fino a quando la CPU non ha completato l'invio del messaggio alla destinazione.

- ***RECEIVE***: Questa funzione è chiamata da un processo che desidera ricevere un messaggio da un'altra CPU. Quando si chiama la funzione RECEIVE, si specifica l'indirizzo su cui il processo ricevente è in ascolto (address) e un puntatore al buffer in cui memorizzare il messaggio ricevuto (message_pointer). Il processo chiamante viene bloccato fino a quando non arriva un messaggio sulla porta specificata dall'indirizzo. Una volta ricevuto il messaggio, viene copiato nel buffer e il processo chiamante viene sbloccato, consentendogli di procedere con l'elaborazione.

In un multicomputer statico il numero di CPU è noto a priori, quindi il campo address è formato dall’identificativo della CPU e dall’identificativo del processo o della porta sulla CPU selezionata.

### Primitive bloccanti e non bloccanti
Le operazioni di base (primitive) send e receive possono essere bloccanti (sincrone) o non bloccanti (asincrone).

Se una send è bloccante, il mittente si blocca quando in messaggio viene spedito e fin quando non termina questa operazione di sand non si può fare altro. In modo analogo la receive non può ricevere niente fin quando non riceve la prima receive.

Se non sono bloccanti spediscono o ricevono e nel mentre possono fare altro BASTA CHE NON SIA UN'ALTRA OPERAZIONE DI SEND/RECEIVE; lo svantaggio è che il mittente non sa quando è terminata la trasmissione e inoltre non può usare la rete del buffer (la rete dove avvengono le comunicazioni) per inviare altri pacchetti se ne sta già inviando uno, quindi proveranno ad inviare nuovi pacchetti finché l'operazione non andrà a buon fine.
**![](https://lh7-us.googleusercontent.com/ZUjqvmHdivjpziGJntFkHfBR1IuhpwlDcTUW0VdMr6gDf_patruYwON0fmG1g0fXpHz7XL4N8ObWe1FMabL1850EePP6NyQer4EzIjAvIrcFpXLgjwtYpUMN0Phvdx68grDMkkbSr0FRenKoWTeNTw)**

Esistono 3 soluzioni:
- il kernel copia il messaggio in un buffer interno e rimane lì finché non può essere inviato
- il mittente riceve un interrupt in modo che possa riutilizzare il buffer
- il mittente manda un primo messaggio e successivamente, quando ancora in primo non è arrivato, ne invia un secondo; per non ricevere errore il buffer si sdoppia, mette il primo in read/only e può inviare anche il secondo.

Il processo mittente può eseguire quindi:
- una spedizione bloccante e mantenere bloccata la CPU;
- una spedizione non bloccante con copia (la CPU spreca tempo solo per eseguire una copia);
- una spedizione non bloccante con interrupt (programmazione difficile);
- una spedizione non bloccante con copia su scrittura (la CPU spreca tempo anche per la copia di fine processo oltre le scritture richieste).
In un sistema multi-thread la prima è la scelta migliore: mentre il thread che esegue la send è bloccato, gli altri continuano a lavorare.

L’arrivo di un messaggio può essere gestito:
- tramite interrupt, ma sono difficili da programmare e molto lenti;
- richiamando una procedura poll che restituisce se ci sono messaggi in attesa di essere letti;
- attraverso la creazione automatica di un thread (chiamato thread pop-up) che finito il suo compito muore spontaneamente;
- attraverso un interrupt che attiva nella ISR il codice di gestione (questo schema è una versione ibrida dei precedenti che sfrutta l’idea di un thread pop-up senza creare alcun thread, migliorando così le performance, e si chiama messaggi attivi).
Una receive bloccante implica un'attesa continua.


# Remote procedure Call (RPC)
Il modello a scambio di messaggi è molto conveniente per un SO multicomputer ma soffre di un grave difetto: tutte le comunicazioni utilizzano I/O.
È possibile bypassare ciò consentendo ai programma di chiamare le procedure che si trovano su altre CPU.

Quando un processo sulla macchina 1 chiama una procedura sulla macchina 2, il processo su 1 è sospeso, e l’esecuzione della procedura chiamante avviene su 2. 
L’informazione può essere trasportata dal chiamante al chiamato attraverso i parametri, e può tornare indietro attraverso i risultati della procedura.

Questa tecnica è detta RPC (Remote Procedure Call, chiamata di procedura remota) ed è alla base del software multicomputer.

La procedura chiamante è nota come client, quella chiamata come server.
Il programma client utilizza una procedura chiamata client stub per chiamare una procedura remota; viceversa il chiamato ha il server stub.

I PASSI DEL RPC
1. Chiamata da parte del client tramite client stub, che è una chiamata di procedura locale, con i parametri che sono messi sullo stack
2. i parametri vengono impacchettati in un messaggio (il processo di impacchettamento è detto ***marshaling***)
3. il kernel spedisce il messaggio dalla macchina client a quella server;
4. il kernel passa il pacchetto in arrivo al server stub;
5. il server stub chiama la procedura del server. 
La risposta effettua lo stesso cammino in direzione opposta.
**![|center](https://lh7-us.googleusercontent.com/CO0FB1orxSGiiYjJLcjAMkbqU1-iG44RkCPvLXvBeaci7_3mpwKBP3lpjk_cdwCEDqPJVGxgaV0hQHWgPyAj5dNyMNqyEO9WirfJfMBfCcydJGkdaNJGQejVkiHyh61WP2wPhzVg7jvm5l3S2j7Fsg)**

### Problemi implementativi
- ***parametri puntatore***: il passaggio dei puntatori è impossibile, perché il client ed il server hanno un diverso spazio degli indirizzi;
	
- ***array come parametri***: quando viene scambiato un array all'interno dello stack (nei parametri) la cui dimensione non definita e ha un simbolo di fine conosciuto solo dal client                      (es. in C si usa `\0`)
	
- ***tipi di dato***: non è sempre possibile dedurre i tipi dei parametri, nemmeno con una specifica normale, o dal codice stesso (funzioni polimorfe). Ad esempio la `printf`.
	
- ***variabili globali***: la procedura chiamante e quella chiamata non possono comunicare per mezzo di variabili globali in quanto non esiste un contesto comune.


# MEMORIA CONDIVISA DISTRIBUITA
>[!tip] DEFINIZIONE: PAGINA
>Suddivisione della memoria in parti di dimensioni minori e fisse. In questo modo l'algoritmo di paging le alloca al programma da eseguire usando pagine come blocco minimo di lavoro.

La DSM è un'alternativa più funzionale (in alcuni ambiti) alla RPC. In questo modo ciascuna pagina si trova in una delle memorie, ogni macchina ha la propria memoria virtuale e le proprie tabelle delle pagine. 

Quando una CPU effettua una LOAD o una STORE su una pagina che non ha, avviene una ***trap*** al sistema operativo, che quindi, localizza la pagina, e chiede alla CPU che la possiede correntemente di invalidare la pagina, e spedirla sulla rete di interconnessione. 
Quando arriva, la pagina viene mappata e viene fatta ripartire l’istruzione che aveva provocato il fault.

La differenza tra una vera memoria condivisa e DSM è illustrata nella Figura 8.22. Nella (a) si vede un vero multiprocessore con memoria fisica condivisa implementata tramite hardware; nella (b) si vede DSM implementata dal sistema operativo; nella (c) troviamo DSM implementata da livelli superiori del software.**![center|500](https://lh7-us.googleusercontent.com/hj9xBcFvCMXZbvELMoTPsYMv309mTUgOmFmJOEM5uaspuBA-S2GuDpvTyRKY6_BGq3Yg8ZrQWrC2iHrNvtfvYOA5T2xBLUSk_NFvvhxYXC-VP-nFq8U7m7mDOxHatcu2OSCzESJzmwlpDxN5wCjJOA)**


# BILANCIAMENTO DEL CARICO
Abbiamo degli algoritmi che permettono la gestione dei vari processi dei nodi (computer) visto che è molto difficile controllarli. 
Diverse sono le cose da notare per gestire questi algoritmi: 
- Il fabbisogno di CPU, 
- l’utilizzo di memoria 
- la quantità di comunicazione con ogni altro processo.  

I possibili obiettivi sono: 
- la minimizzazione dei cicli di CPU sprecati per la carenza di lavoro locale, 
- la minimizzazione della larghezza di banda di comunicazione totale
- le condizioni eque per gli utenti e i processi.

# VIRTUALIZZAZIONE
È una tecnica che permette di creare più computer virtuale all'interno di un computer reale, chiamati macchine virtuali. 
Ciascuna macchina virtuale è un computer “virtuale” in cui è possibile eseguire un proprio sistema operativo e dei propri servizi e applicazioni.

La virtualizzazione introduce importanti vantaggi:
- forte isolamento tra le macchine (isolamento dei malfunzionamenti); questo è comodo perché, ad esempio, quando voglio scaricare qualcosa da internet che possibilmente contiene un virus, con la virtualizzazione non rischio nulla.
- minore spazio occupato;
- minore consumo;
- minore calore da dissipare;
- maggiore manutenibilità;
- possibilità di creare dei checkpoint (punti di ripristino);
- possibilità di far girare applicazioni legacy su ambienti obsoleti;
- possibilità di effettuare test delle applicazioni su differenti sistemi operativi senza disporre dell’hardware fisico necessario.

Ogni CPU con una modalità kernel (modalità privilegiata o supervisor) e una modalità utente ha un insieme di istruzioni:
- istruzioni sensibili, eseguibili da entrambe ma con comportamenti diversi
- istruzioni privilegiate, che, quando vengono eseguite in modalità utente, causano un trap.
Una macchina è virtualizzabile solo se le istruzioni sensibili sono un sottoinsieme di quelle privilegiate.


# HYPERVISOR
Un hypervisor, noto anche come Virtual Machine Monitor (VMM), è un software che crea e gestisce macchine virtuali (VM) su un host fisico. In altre parole, un hypervisor permette a un singolo computer fisico di eseguire più sistemi operativi contemporaneamente, ognuno su una VM separata.

Ne esistono di due tipi:
## Hypervisor di tipo 1
Noto anche come bare-metal hypervisor, funziona direttamente sull'hardware fisico del computer senza bisogno di un sistema operativo sottostante. Gestisce direttamente le risorse hardware e controlla le macchine virtuali (VM) in modo efficiente e sicuro.
Funziona in modalità Kernel.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240514155209.png|center|300]]

La macchina virtuale non può eseguire istruzioni sensibili perché è in modalità utente anche se pensa di essere in modalità kernel; se il SO ospite esegue un'istruzione sensibile avviene un crash oppure una trap che l'hypervisor può provare a risolvere.
**![|center|600](https://lh7-us.googleusercontent.com/vC3hcQl_YFS6IH9_6rhINLRETH9tTRBYsDvRyr3tO5__czPweBX0pZDGqII4CtOhiJkEKTpN6WawAB_N3z4HzcekgJH8vl6M2i-bdwe0YQim1PcIbdgIpae5Ggj1dpOqoQsQXBWRFsFJwDBCZD2wrQ)**
## Hypervisor di tipo 2
Noto anche come hosted hypervisor, è un programma utente che interpreta le istruzioni della VM e le traduce sul SO della macchina reale.
Un esempio è VMware.
![[content/Uni/Anno 1/Architettura/IMMAGINI/Pasted image 20240514155442.png|center|300]]

Per eseguire un programma, l'hypervisor di tipo 2 sfrutta una tecnica di traduzione binaria, ovvero traduce le istruzioni del codice sorgente in istruzioni equivalenti che possono essere eseguite correttamente sul processore di destinazione- È importante sapere che queste istruzioni vengono tradotte in ordine di priorità. Questo blocco tradotto è messo in una cache virtuale di VMware e vengono eseguiti alla velocità della macchina fisica

## Confronto
Tutte le istruzioni sensibili in un Hypervisor di tipo due vengono emulate e quindi non vanno ad intaccare direttamente la macchina fisica.
Negli HV di tipo 1 vengono generati troppi trap con il sistema "trap-and-emulate" e quindi molte volte si comportano come gli HV di tipo 2 con la traduzione binaria.

# PARAVIRTUALIZZAZIONE
La paravirtualizzazione è una tecnica di virtualizzazione che modifica il sistema operativo guest per migliorare le prestazioni. Invece di inviare direttamente le istruzioni sensibili all'hardware, il sistema operativo guest utilizza delle chiamate speciali chiamate "hypercall" per comunicare con l'hypervisor. Quindi l’hypervisor definisce una interfaccia, cioè delle API (Application Program Interface), che i sistemi operativi guest possono attivare. 

Le hypercall permettono al sistema operativo guest di chiedere all'hypervisor di eseguire operazioni sensibili. Questo riduce l'overhead delle tecniche tradizionali di virtualizzazione e migliora l'efficienza.


>[!tip] CHIAMATA DI SISTEMA
Una chiamata di sistema, in informatica, indica il meccanismo usato da un processo a livello utente o livello applicativo, per richiedere un servizio a livello kernel del sistema operativo del computer in uso



