Ripartiamo da questi concetti
![[content/Zphoto/Pasted image 20250411195327.png]]

### Esempio 1: PDA che accetta per pila vuota
Costruiamo un PDA $$„Äà \{a,b\}, \ \{Z_{0}, A, B\}, \ Z_{0} , \ \{q_{0} , q_{1} \} , \ \varnothing, \ q_{0} , \ Œ¥ „Äâ$$che riconosce **PER PILA VUOTA** (lo riconosci dal simbolo $\varnothing$) il linguaggio $L_{PPAL}$ delle parole palindrome pari sull'alfabeto $\{a,b\}$ 
![[content/Zphoto/Pasted image 20250412104255.png]]
###### ***COMPORTAMENTO DEL PDA*** üîÅ
Il comportamento del PDA √® il seguente:
- Finch√© legge i simboli della **prima met√† della parola**, il PDA li **accumula sulla pila**, uno alla volta, **sovrascrivendo il simbolo iniziale** $Z_{0}$‚Äã.
    
- Una volta raggiunta la **met√† della parola**, il PDA **inizia a leggere la seconda met√†**.
    
- In questa fase, per ogni simbolo letto dall'input, controlla se **corrisponde al simbolo in cima alla pila**:
    - Se corrisponde, lo **rimuove dalla pila**
    - Altrimenti, la computazione **fallisce**
    
- Se al termine dell‚Äôinput la **pila risulta completamente vuota**, allora la parola √® **palindroma e di lunghezza pari**, e quindi **accettata**.
###### ***CONCETTO FONDAMENTALE***
Nota come in alcune funzioni il PDA possa eseguire anche due passi, questo lo rende **NON DETERMINISTICO**.
<font color="#d83931">E perch√© √® cos√¨ importante?</font> il PDA di per s√© √® stupido e non pu√≤ sapere concretamente quando √® arrivato a met√† parola -> cos√¨ ad un certo punto dice "io qua non so cosa cazzo fare, nel dubbio io le provo entrambe tanto mi basta che ANCHE SOLO UN RAMO arrivi a $\varnothing$ e posso accettare".

>[!tip]- Con parole della prof
>![[content/Zphoto/Pasted image 20250412105657.png]] 

![[content/Zphoto/Pasted image 20250412105245.png]]


### Esempio 2: PDA che accetta per stato finale
Ora, costruiamo un PDA $$„Äà \{a,b\}, \ \{Z_{0}, A, B\}, \ Z_{0} , \ \{q_{0} , q_{1}, q_{2} \} , \ \{q_{2}\}, \ q_{0} , \ Œ¥ „Äâ$$che riconosce **PER STATO FINALE** ($q_{2}$) il linguaggio $L_{PPAL}$ delle parole palindrome pari sull'alfabeto $\{a,b\}$ 

Qui la costruzione √® identica a prima, con alcune cosa modificate
![[content/Zphoto/Pasted image 20250412105811.png]]
Quindi
- finch√© si trova PRIMA della met√† aggiunge parole alla pila (NON CANCELLANDO $Z_{0}$)
- quando arriva alla met√†, se pu√≤, cancella
- se cancella tutto e arriva a leggere, di nuovo, $Z_{0}$, entra in $q_{2}$ **e accetta**

>[!tip]- Parole della prof
>![[content/Zphoto/Pasted image 20250412110003.png]]

Anche qui √® fondamentale il **non determinismo**.

![[content/Zphoto/Pasted image 20250412110037.png]]

### Macchina di Turing VS Automa a pila
Un PDA sembra in tutto e per tutto una MdT con caratteristiche speciali quali
- due nastri
- N1 di solo lettura
- N2 gestito come una pila

BENISSIMO, allora mostriamo come simulare un PDA mediante una Macchina di Turing (e perch√© mai non dovremmo farlo, **mannaggia a Cr**$\square$**sto**)

Incollo le foto che tanto, secondo me, √® inutile
![[content/Zphoto/Pasted image 20250412110553.png]]
![[content/Zphoto/Pasted image 20250412110601.png]]


>[!question] ESERCIZIO: dimostrare l'equivalenza di A e T

##### Punto della situazione
Fino ad ora abbiamo descritto solo PDA non deterministici, in cui la funzione di transizione $\delta$ associa a uno stato interno, a un simbolo sul nastro (o nessun simbolo sul nastro) e a un simbolo sulla pila *un insieme di azioni fra le quali scegliere quella da eseguire*.

Precisamente, trovandosi nello stato interno $q_{1}$, leggendo `a` su $n1$ e `Z` sulla pila, l‚Äôazione da eseguire deve essere scelta nell‚Äôinsieme$$ ùúπ(q_{1} , a, Z) ‚à™ ùúπ(q_{1}, ùú∫, Z)$$ossia:
- un'azione che "consuma" `a` (se si sceglie l'azione $\delta(q_{1}, a, Z)$)
- un'azione che "non consuma" `a` (se si sceglie l'azione $\delta(q_{1}, \varepsilon, Z)$)


## PDA deterministico
Affinch√© un automa a pila sia deterministico √® necessario che per ogni stato interno, per ogni simbolo sul nastro e per ogni simbolo sulla pila l‚Äôinsieme di azioni fra le quali scegliere **si riduca a un‚Äôunica azione**.

>[!lemma] DEFINIZIONE
>Un PDA $$‚Ñ≥=„Äà Œ£, \ Œì , \ Z_{0} , \ Q , \ Q_{F} , \ q_{0} , \ Œ¥ „Äâ$$ √® deterministico se ogni $q ‚àà Q$, per ogni $a ‚àà Œ£$, e per ogni $Z ‚àà Œì$ accade che $$|\delta(q_{1}, a, Z)| \ + \ |\delta(q_{1}, \varepsilon, Z)| \ = \ 1 $$

###### Una piccola differenza
Sappiamo che dal punto di vista della calcolabilit√†, una Macchina di Turing NON DETERMINISTICA √® equivalente a una Macchina di Turing DETERMINISTICA
	ossia, tutto ci√≤ che possiamo fare con la NON DETERMINISTICA lo possiamo possiamo fare anche con la DETERMINISTICA

>[!question] Vale lo stesso per i PDA?
>No, perch√© esistono dei **linguaggi context-free che non sono accettati da automi a pila non deterministici**


>[!lemma] TEOREMA G.12
> L‚Äôinsieme dei linguaggi accettati da automi a pila deterministici √® un sottoinsieme proprio ($\subset$) dei linguaggi context-free
> 
> In altri termini, *gli automi a pila deterministici sono ‚Äò‚Äôstrettamente meno potenti‚Äô‚Äô di quelli non deterministici*.


##### Conclusione Macchina di Turing VS PDA
###### üîç La domanda che la prof si pone √®:
> *Se possiamo trasformare (lez. iniziali) un PDA non deterministico (NPDA) in una macchina di Turing non deterministica (NTM), e poi una NTM in una deterministica (DTM)‚Ä¶*  
> *allora perch√© non possiamo "rientrare" e costruire un **DPDA** equivalente?*
###### üö® Risposta:
> **Perch√© non sappiamo trasformare una DTM in un PDA deterministico!**

Cio√®:
- Possiamo partire da NPDA ‚Üí NTM ‚Üí DTM, e accettare i linguaggi.
- Ma **non possiamo tornare indietro da DTM ‚Üí DPDA**, perch√©:
    - I PDA hanno **meno memoria** (solo una pila).
    - E **non possono simulare ogni DTM**.

###### ‚úÖ Quindi:
- Ogni linguaggio context-free √® **accettato da una macchina di Turing** (deterministica).
- Ma non tutti i linguaggi context-free sono accettabili da un **DPDA**.


---


## Alberi sintattici üå≥
#### üü£ Cos'√® un albero sintattico üå≥?
√à un **albero** (come in informatica, con nodi e rami) che **rappresenta visivamente il processo di derivazione** di una parola secondo le regole della grammatica.

#### üîç Struttura dell‚Äôalbero sintattico:
- **Radice = S**  
    Parte sempre dal simbolo iniziale della grammatica (cio√® il punto di partenza della derivazione).
    
- **Nodi interni = simboli non terminali** (cio√® ‚àà V<sub>N</sub>)  
    Questi sono i simboli che vengono _espansi_ secondo le regole di produzione.
    
- **Foglie = simboli terminali** (cio√® ‚àà V<sub>T</sub>)  
    Sono i caratteri finali della parola generata (quelli che _vedi nell'output_).
    
- **Espansione dei nodi - rami**:  
    Se in una regola hai per esempio `A ‚Üí x‚ÇÅ x‚ÇÇ ... x‚Çô`,  
    allora nel tuo albero:
    - `A` sar√† un **nodo interno**
    - `x‚ÇÅ, x‚ÇÇ, ..., x‚Çô` saranno i suoi **figli**, nell‚Äôordine indicato dalla produzione.

###### üìò A cosa serve?
Gli alberi sintattici forniscono una descrizione sintetica della struttura sintattica di una parola 
- ossia, delle produzioni che hanno permesso di generarla.

###### ESEMPIO
![[content/Zphoto/Pasted image 20250412122007.png]]
L'osservazione √® molto importante perch√© dice **uno stesso albero sintattico corrisponde a pi√π di una derivazione**.

Significa che **una stessa parola pu√≤ essere generata in modi diversi**, cio√® seguendo **derivazioni diverse**, **ma che alla fine portano allo stesso albero sintattico**.
In parole semplici:
- Anche se si scelgono **ordini diversi** con cui applicare le regole di produzione,
- l‚Äô**albero delle regole usate e dei simboli generati** pu√≤ essere **lo stesso**,
- e quindi **la struttura della parola (cio√® la sua forma)** **non cambia**.

>[!example] Il che non √® assolutamente un problema
>Perch√© l‚Äôobiettivo dell‚Äôalbero sintattico √® **mostrare la struttura** della parola, **non il percorso preciso** con cui √® stata costruita.


Per√≤ un problema c'√®

>[!problem] Ad una stessa parola possono corrispondere pi√π alberi sintattici
>Ossia, io posso generare una parola con alberi sintattici diversi
>![[content/Zphoto/Pasted image 20250412122834.png]]

Questo problema √® definito ***ambiguit√† di una grammatica***

>[!question] E perch√© √® un problema?
>Perch√© **l‚Äôalbero sintattico** pu√≤ essere usato **non solo per generare la parola**, ma anche per **capirne il significato**, cio√® per **interpretarla semanticamente**.

Prendiamo l'esempio nella foto, e ipotizziamo che l'albero sia impostato per interpretare la parola generata **PARTENDO DAL BASSO**
1. <font color="#d83931">ALBERO IN ROSSO</font>: `3 + (3 * 3) = 12`
2. <font color="#245bdb">ALBERO IN BLU</font>: `(3 + 3) * 3 = 18`


Sarebbe veramente bello poter capire in anticipo se una grammatica sia ambigua o no MA OVVIAMENTE NON SI PU√í FARE.

>[!lemma] Teorema G.13
>sia $L_{A}$ l‚Äôinsieme delle grammatiche di tipo 2 ambigue -> **il linguaggio** $L_{A}$ **√® non decidibile**.
>
>Ci√≤ significa che non esiste un algoritmo che, data una grammatica G di tipo 2 decide se G √® ambigua



>[!tip]- Chiacchiere finali della prof 
>1. üìå **I linguaggi di programmazione ‚Äúgrosso modo‚Äù sono di tipo 2 (context-free)**:
>    - Ovvero: la **struttura sintattica di base** di un linguaggio di programmazione pu√≤ essere espressa con una grammatica context-free.
>        
>    - Tuttavia, **alcune regole**, come per esempio **l‚Äôunicit√† della dichiarazione di una variabile**, richiedono controlli pi√π potenti ‚Üí cio√® **di tipo 1** (context-sensitive).
>    
>1. üí° **Si possono separare** gli aspetti:
>    - Di tipo 2: la struttura del programma.
>    - Di tipo 1: le regole contestuali, come i nomi unici, i tipi delle variabili ecc.
>    
>1. üîç L‚Äôanalisi sintattica (`parsing`):
>    - Viene **divisa in due fasi**: una per controllare la struttura generale (tipo 2), l‚Äôaltra per gli aspetti contestuali (tipo 1).
>    
>1. ‚öôÔ∏è Il `parsing` √® **la fase di verifica della correttezza** del programma:
>    - Serve **prima** della traduzione in codice eseguibile.
>    - In questa fase si costruisce **l‚Äôalbero sintattico** della frase/programmazione.
>    - Da questo albero poi **si genera il codice oggetto** (eseguibile).
>	
>1. üìö Le grammatiche formali sono nate per **descrivere le frasi nei linguaggi naturali**, ma...
>    - ...sono risultate **inadatte** a quello scopo.    
>    - Tuttavia, si sono rivelate perfette per **analizzare i linguaggi di programmazione**.
>    
>1. üí° **A cosa servono** nei linguaggi di programmazione?
>    - Per **studiare la sintassi**.
>    - Per **descrivere formalmente** se un programma √® corretto dal punto di vista sintattico.
>    
>1. üß† Ogni linguaggio di programmazione pu√≤ essere associato a una grammatica $G_{P}$‚Äã:
>    - Un **programma √® sintatticamente corretto** se √® una parola del linguaggio generato da $G_{P}$‚Äã.