Riprendiamo il discorso dell'ultima lezione sulla correlazione polinomiale.
Prendi questo esempio
![[content/Zphoto/Pasted image 20250424164625.png]]
Vedi che posso andare avanti all'infinito.

Ma nella teoria della Complessità Computazionale le cose non sono proprio così.

---

## Alla ricerca della macchina più veloce
#### Teorema per `dtime`
>[!lemma] Teorema 6.7 - Accelerazione lineare
>![[content/Zphoto/Pasted image 20250424165016.png]]

Questo teorema ci dice che, dato un qualunque algoritmo, ne esiste sempre uno più veloce (di un fattore costante!).

>[!question] Perché però abbiamo i due addendi $O(|x|^{2})$ e $O(|x|)$?
>Perché per essere più efficienti, gli algoritmi devono
>- codificare in forma espressa il proprio input (vedi teorema successivo)
>	- se la codifica è scritta su un nastro apposito ($T_{2}$) allora bastano $O(|x|)$ passi
>	- se la macchia dispone di un solo nastro ($T_{1}$) allora occorrono $O(|x|^{2})$ passi

#### Teorema per `dspace`
Dimostriamo un teorema analogo a quello di prima ma per `dspace`
>[!lemma] Teorema 6.6 - Compressione lineare
>![[content/Zphoto/Pasted image 20250424170459.png]]

Anche qui, viene detto che, dato un qualunque algoritmo, ne esiste un altro che usa una frazione costante della memoria del primo

>[!question] Perché l'addendo $O(|x|)$?
>Intanto, l'input di $T_{1}$ è lo stesso di $T$.
>Pertanto $T_{1}$ deve codificare in forma compressa il proprio input e poi lavorare sull'alfabeto compresso.
>
>Osserva come l'alfabeto compresso sia $\Sigma^{k}$ (ossia, un carattere dell’alfabeto compresso è una parola di k caratteri di Σ) e che l'alfabeto di $T_{1}$ è $$\Sigma^{k} \cup \Sigma$$perché l'alfabeto originale non scompare.


---

## Classi di complessità (deterministiche)
Siamo pronti a raggruppare i linguaggi in base all'efficienza delle macchine che li decidono.

>[!question] Cosa vuol dire che una macchina che decide un linguaggio ha una certa efficienza?
>Significa che la macchina che decide un linguaggio $L \subseteq \Sigma^{*}$ si comporti "bene" su <u>ogni</u> parola $x \in \Sigma^{*}$

Però ovviamente non possiamo trovare la macchina "migliore", perché tanto sappiamo che se ne prendo una ne esisteranno altre più potenti (teoremi di prima).

Per risolvere questa questione utilizziamo la notazione $O$ e diciamo che 
	==Un linguaggio `L` appartiene all'insieme caratterizzato dalla "efficienza temporale" individuata dalla funzione totale e calcolabile `f`, se esiste una macchina `T` che decide (o accetta) `L` e che, per ogni `x` sull'alfabeto di `L`, termina in `O(f(|x|))` istruzioni.==

E discorso analogo per "efficienza spaziale".

#### Effettive classi di complessità deterministiche
##### Efficienza temporale - DTIME
Le classi che misurano "efficienza temporale" nel caso deterministico si chiamano <font color="#ff0000">DTIME</font>: data una <font color="#245bdb">funzione totale e calcolabile f</font> ![[content/Zphoto/Pasted image 20250424182246.png]]
ATTENZIONE:  ^460bf9
- <font color="#245bdb">dtime</font> (minuscolo) è la **misura di complessità**, ossia, una <u>funzione</u>
- <font color="#ff0000">DTIME</font> (MAIUSCOLO) è una **classe di complessità**, ossia, un <u>insieme</u>

##### Efficienza spaziale - DSPACE
Le classi che misurano "efficienza spaziale" nel caso deterministico si chiamano <font color="#ff0000">DSPACE</font>: data una <font color="#245bdb">funzione totale e calcolabile f</font> ![[content/Zphoto/Pasted image 20250424182520.png]]


---

## Classi di complessità (non deterministiche)
Facciamo le stesse considerazioni delle deterministiche.
##### Efficienza temporale - NTIME
Le classi che misurano "efficienza temporale" nel caso non deterministico si chiamano <font color="#ff0000">NTIME</font>: data una <font color="#245bdb">funzione totale e calcolabile f</font> ![[content/Zphoto/Pasted image 20250424182637.png]]
Qui si parla di ACCETTAZIONE perché sappiamo che se un linguaggio è accettato entro un certo numero di istruzioni, sappiamo che è decidibile; <u>MA NON SAPPIAMO QUANTO CI METTE A RIGETTARE</u>.
E, per di più, a noi interessa solo accettare le parole del linguaggio.

##### Efficienza spaziale - NSPACE
Le classi che misurano "efficienza spaziale" nel caso non deterministico si chiamano <font color="#ff0000">NSPACE</font>: data una <font color="#245bdb">funzione totale e calcolabile f</font> ![[content/Zphoto/Pasted image 20250424183033.png]]


---

## Classi di complemento
![[content/Zphoto/Pasted image 20250424183058.png]]


---

### Un paio di questioni
1) Qui stiamo considerando linguaggi sull'alfabeto ${0,1}$ per comodità ma possiamo considerare anche altri alfabeti (e lo faremo)
2) La funzione f che definisce una classe di complessità (ad esempio `DTIME[`<font color="#245bdb">fn</font>`]`) diamo il nome di <font color="#245bdb">funzione limite</font>
	- che ovviamente deve essere totale e calcolabile sennò, per definizione, avremo una funzione che ci dice quante istruzioni esegue una computazione MA di cui non sappiamo il reale valore (inutile se ci pensi!)


---

## Relazioni fra classi di complessità
>[!lemma] Teorema 6.8
>![[content/Zphoto/Pasted image 20250424183845.png]]

La dimostrazione è facile.
Una macchina deterministica è **una particolare macchina non deterministica con il grado di non determinismo pari a `1`** e, inoltre
- una parola decisa in un certo numero di passi è anche accettata in quel certo numero di passi
- una parola decisa utilizzando un certo numero di celle è anche accettata in quel certo numero di celle

---

>[!lemma] Teorema 6.9
>![[content/Zphoto/Pasted image 20250424185029.png]]

Questa dimostrazione segue direttamente dal [[content/Cartella_utile/Uni/Anno 2/Fondamenti/21. Misure di complessità#^a43723|Teorema 6.1]]
Sia $L \subseteq \{0,1\}^{*}$ tale che $L \in DTIME[f(n)]$.
Sappiamo, per il Teorema 6.1, che $$\text{spazio} \le \text{tempo}$$ per ogni macchina di Turing:
	se una macchina fa al massimo `t` passi, non può usare più di `t` caselle

Perciò $$\text{dspace(T,x)} \le \text{dtime(T,x)}$$e dato che $$\text{dtime} \in O(f(|x|))$$ (per [[content/Cartella_utile/Uni/Anno 2/Fondamenti/22. Classi di complessità#^460bf9|definizione di DTIME]]) 
Allora possiamo scrivere $$\text{dspace(T,x)} \le \text{dtime(T,x)}  \in O(f(|x|))$$e quindi anche $$\text{dspace(T,x)} \in O(f(|x|))$$
##### ✅ **Conclusione**

> Se $L \in \text{DTIME}[f(n)]$, allora $L \in \text{DSPACE}[f(n)]$

Cioè: $$\text{DTIME}[f(n)] \subseteq \text{DSPACE}[f(n)]$$
✅ **Vale anche nel caso non deterministico**: $$\text{NTIME}[f(n)] \subseteq \text{NSPACE}[f(n)]$$

---


>[!lemma] Teorema 6.10
>![[content/Zphoto/Pasted image 20250424192810.png]]

Per la dimostrazione non ho capito un cazzo, la copio e incollo.
##### 📘 Teorema 6.10 (detto in parole semplici)

> Se usi **al massimo `f(n)` celle di memoria** per decidere un problema, allora esiste un modo per risolverlo che richiede **tempo al massimo `2^O(f(n))`** (cioè tempo esponenziale rispetto allo spazio).

###### Formalmente:
```
DSPACE[f(n)] ⊆ DTIME[2^O(f(n))]
NSPACE[f(n)] ⊆ NTIME[2^O(f(n))]
```

##### 🧠 Ok, ma perché?
Immagina che la macchina di Turing T usi al massimo `f(n)` celle del nastro per un input lungo `n`.

> 🔍 Ogni "configurazione" della macchina è:  
> stato + contenuto del nastro + posizione della testina

Quante possibili **configurazioni diverse** può assumere T?

##### Le combinazioni sono tante, ma **finite**! E in particolare:
- Gli **stati** sono un numero fisso, diciamo `|Q|`
- Le celle usate sono al massimo `f(n)`
- Ogni cella può contenere uno dei simboli dell’alfabeto `|Σ|` (più il blank)
- La testina può stare in una delle `f(n)` celle

Allora il numero totale di configurazioni è al massimo:

```
|Q| × |Σ + 1|^f(n) × f(n)
```

📌 **Questo è un numero esponenziale rispetto a `f(n)`**  
⇒ diciamo che è **al massimo `2^O(f(n))` configurazioni**

##### 🔁 E come ci serve questo?
Se conosci **tutte le configurazioni possibili**, puoi:
- simulare la macchina T "dall’esterno"
- esplorare tutte le possibili sequenze di configurazioni (tipo albero di esecuzione)
- controllare se si arriva a uno stato di accettazione

Questo è un algoritmo che **decide il linguaggio**, anche se **è più lento**, perché deve controllare tutte le configurazioni.

Ma è comunque **un algoritmo deterministico** che termina!

##### ✅ Conclusione:

> Se un problema si può decidere usando al massimo `f(n)` celle di memoria,
> allora **esiste un algoritmo deterministico** che risolve lo stesso problema in **tempo `2^O(f(n))`**

E lo stesso vale anche nel caso **non deterministico**.

---

>[!lemma] Teorema 6.11 – Relazioni tra classi e loro complementi
![[content/Zphoto/Pasted image 20250425115346.png]]
#### 🧠 Cosa vuol dire?
- Se un linguaggio `L` si può **decidere in tempo f(n)**, allora anche il **complementare di L**, cioè `Lᶜ`, si può decidere **nello stesso tempo**.
- Lo stesso vale per lo spazio.

#### 🔨 Dimostrazione (passo-passo)
###### 🔹 Supponiamo che `L ∈ DTIME[f(n)]`  
Allora esiste una macchina di Turing deterministica `T` che **decide** `L` in tempo `O(f(|x|))`.
- Se `x ∈ L`, allora `T(x)` termina in uno stato accettante `q_A`
- Se `x ∉ L`, allora `T(x)` termina in uno stato rifiutante `q_R`

###### 🔹 Costruiamo una nuova macchina `T’`
Facciamo che `T’` è **identica** a `T`, ma **scambia gli stati finali**:
- accetta dove `T` rifiutava
- rifiuta dove `T` accettava

Quindi:
- `T’` accetta **se e solo se** `x ∉ L`, cioè `x ∈ Lᶜ`
- Il tempo di esecuzione di `T’` è uguale a quello di `T`

###### 🔹 Cosa abbiamo ottenuto?
- `T’` decide `Lᶜ` in tempo `O(f(|x|))`
- Quindi `Lᶜ ∈ DTIME[f(n)]`
- Questo significa che `L ∈ coDTIME[f(n)]`

E quindi:
```
DTIME[f(n)] ⊆ coDTIME[f(n)]
```

Ma anche il viceversa vale (scambiando `L` con `Lᶜ`) ⇒ **sono uguali!**

##### ✅ Conclusione

```
DTIME[f(n)] = coDTIME[f(n)]
DSPACE[f(n)] = coDSPACE[f(n)]
```

>[!tip] 🧠 Intuizione
> Nelle **macchine deterministiche**, è facile costruire il complemento:
> basta **invertire** gli stati finali.
> 
⚠️ **Questo non è vero** per le **macchine non deterministiche**, perché **non possiamo "negare" un'esistenza** di una computazione accettante.


---


## ⚠️ Classi “poco precise”

#### 📌 Problema
Quando diciamo che un linguaggio `L` è in `DTIME[f(n)]`, stiamo usando la **notazione O(f(n))**.  
Questo significa che **non stiamo identificando con precisione** quanto tempo serve, ma solo **un limite superiore asintotico**.

#### 🔁 Conseguenza
Se `L ∈ DTIME[f(n)]`, allora `L` appartiene anche a **molte altre classi** `DTIME[g(n)]`  
per ogni `g(n)` che cresce più velocemente di `f(n)`.

#### 🔍 Definizione di O(g(n))
Diciamo che:
```
f(n) ∈ O(g(n)) ⇔ ∃ n₀ ∈ ℕ, ∃ c ∈ ℕ tali che ∀ n ≥ n₀: f(n) ≤ c ⋅ g(n)
```

Cioè: da un certo punto in poi, `f(n)` è al massimo un multiplo costante di `g(n)`.

#### 📘 Teorema 6.12
![[content/Zphoto/Pasted image 20250425120834.png]]
#### ✅ In pratica
Se `f(n)` cresce **meno o uguale** a `g(n)` (da un certo punto in poi),  
allora ogni linguaggio decidibile in tempo/spazio `f(n)` si trova anche nella classe `g(n)`.

>[!tip] 🧠 Intuizione finale
> Le classi di complessità **non individuano univocamente un linguaggio**,  
> perché esistono **infinite funzioni più grandi** che includono lo stesso linguaggio.
> ##### 🧩 Esempio
Se dici che un linguaggio è in `DTIME[n²]`, allora è anche in:
>- `DTIME[n³]`
>- `DTIME[n⁵]`
>- `DTIME[2ⁿ]`
Perché tutte queste funzioni crescono più rapidamente di `n²`.

>[!bug] PERÒ ATTENZIONE
>L'intuizione è giusta a metà, perché in realtà se collochiamo un linguaggio `L`, ad esempio, in `DTIME[f(n)]`, questo non implica che `L` non possa appartenere anche a qualche classe `DTIME[r(n)]` tali che, definitivamente `r(n) ≤ f(n)`.
>
>###### Detto in parole spiccie
>Se io progetto un algoritmo per decidere `L`, magari esiste qualcuno che ha progettato un algoritmo più efficiente del mio!!!
>Ma questo compito (spero) non ci interessa.


---

## Gap Theorem
#### 🧠 Contesto generale
Nel mondo della complessità computazionale, si cerca di classificare i problemi (cioè i linguaggi) in base a quante risorse (tempo, spazio, ecc.) servono per risolverli.

Idealmente, vorremmo che:
Se una funzione `f(n)` cresce molto più velocemente di un’altra `g(n)`, allora:

```
DTIME[g(n)] ⊊ DTIME[f(n)]
```

cioè: ci sono problemi che si possono risolvere con `f(n)` ma non con `g(n)`, quindi `f(n)` definisce una classe più potente.

#### 👀 Il problema 
Sarebbe bello se ci fosse sempre una gerarchia stretta tra le classi di complessità: più tempo → più potere.
Ma non è sempre così! Entra in scena il:

>[!lemma] Teorema 6.13 - Gap Theorem 
> ![[content/Zphoto/Pasted image 20250425125730.png]]

#### 😮 Ma com'è possibile?
Normalmente, uno si aspetta che se lasci alla macchina molto più tempo, possa risolvere più problemi.

Ad esempio, `2^f(n) ≫ f(n)`, quindi dovrebbe valere:

```
DTIME[f(n)] ⊊ DTIME[2^f(n)]
```

Invece il Gap Theorem ti dice il contrario:

> Esiste una funzione `f(n)` calcolabile tale che dando tempo esponenziale in `f(n)` non si guadagna nulla rispetto al tempo `f(n)`.

E allora, questi comportamenti strani si verificano quando le funzioni limite sono anch’esse “strane”.

##### 🎯 Intuizione
Significa che c’è un “buco” nella gerarchia di classi temporali:
- Nonostante il salto enorme tra `f(n)` e `2^f(n)`, nessun nuovo linguaggio si aggiunge.
- La classe `DTIME[2^f(n)]` è uguale a `DTIME[f(n)]`.

Questo fa crollare l’intuizione che "più tempo = più potere".


---

## 🔧 Funzioni time- e space-constructible
##### 🧱 Cosa sono le funzioni _constructible_?
Le funzioni **time-constructible** e **space-constructible** sono funzioni che **possono essere “costruite” da una macchina di Turing** **entro le stesse risorse** (tempo o spazio) che rappresentano.


#### ⏱️ TIME-CONSTRUCTIBLE — Definizione e spiegazione
##### 📌 Definizione (semplificata):
Una funzione `f(n)` è **time-constructible** se **esiste una macchina di Turing** che, dato `n` scritto in **notazione unaria**, riesce a **calcolare `f(n)`** (sempre in unario) **entro tempo O(f(n))**.

##### ✋ Ma che vuol dire?
Immagina che `f(n) = n^2`. Dire che è time-constructible significa:
- Esiste una macchina che, ricevuto in input `111` (cioè 1^3), calcola `f(3) = 9`,
- E scrive `111111111` (1^9) **sul nastro di output**,
- Usando **al massimo `c * f(n)` passi** per una qualche costante `c`.


#### 💾 SPACE-CONSTRUCTIBLE — Definizione e spiegazione
##### 📌 Definizione (semplificata):
Una funzione `f(n)` è **space-constructible** se **una macchina di Turing** può calcolare `f(n)` (in unario), **usando al massimo `O(f(n))` celle di nastro**.


#### 🎓 A cosa servono queste funzioni?
Servono per:
- **formulare correttamente** i teoremi della gerarchia temporale e spaziale,
- evitare anomalie tipo quelle viste nel **Gap Theorem**,
- assicurarsi che quando diciamo “tempo f(n)” o “spazio f(n)”, sia **davvero realizzabile** rispettare quel limite.


#### 🔵 1. Notazione unaria come input

> “L’input `n` deve essere in **notazione unaria**”.

➡️ Questo significa che la **lunghezza dell’input coincide con il suo valore**:
```
|1^n| = n
```

👉 Così possiamo misurare tempo e spazio in funzione della lunghezza reale dell’input.

#### 💜 2. Output in notazione unaria
La macchina che “testimonia” che `f` è time-constructible:
- Prende in input $1^{n}$
- E scrive su nastro **il valore `f(n)` in unario**

#### ✏️ Esempio:
Se $f(n) = n^2 + 3$, allora per 
- `n = 3` -> `f(3) = 12`.

- La macchina prende in input $1^3 = 111$
- E scrive $1^{12} = 111111111111$ sul nastro

#### 🔴 3. Cosa significa “time-constructible” in parole semplici?

> Una funzione time-constructible **è molto più di una funzione calcolabile**.

##### ✅ Vuol dire:
- Può essere **calcolata in tempo proporzionale al suo stesso valore**
- Cioè: scrivere `f(n)` simboli `1` richiede **al massimo `O(f(n))` passi**

💡 *Ogni ‘1’ richiede un numero costante di istruzioni (in media)*

➡️ È una funzione che **si può “costruire” nel tempo che rappresenta**


>[!example] Le stesse considerazioni valgono anche per lo spazio


![[content/Zphoto/Pasted image 20250425132907.png]]


---

## 🚫 Addio Gap Theorem!

### ❗ Punto chiave iniziale:

> La funzione $f(n)$ definita nel Gap Theorem **non è time-constructible**.

Questo spiega **perché il Gap Theorem non viola** i teoremi di gerarchia: quei teoremi **valgono solo se** $f(n)$ è **costruibile**.


>[!lemma] Teorema 6.14 - Teorema di gerarchia spaziale
>![[content/Zphoto/Pasted image 20250425133201.png]]


>[!lemma] Teorema 6.15 - Teorema di gerarchia temporale
>![[content/Zphoto/Pasted image 20250425133239.png]]


#### 📈 Ma cosa significano davvero i teoremi di gerarchia?
Quando diciamo che: $$\lim_{n \ \to \ \infty} \  \frac{g(n)}  {f(n)}  = 0$$
stiamo dicendo che `f(n)` cresce **molto più velocemente** di `g(n)` (in modo asintotico).

➡️ Man mano che `n` cresce, la distanza tra `f(n)` e `g(n)` aumenta sempre di più.



Allo stesso modo, per la **gerarchia temporale**, la condizione è ancora più severa: $$\lim_{n \ \to \ \infty} \  \frac{g(n) \cdot log(g(n))}  {f(n)}  = 0$$
➡️ Significa che `f(n)` deve essere **ancora più grande** rispetto a `g(n)` per garantire una separazione tra le classi di complessità.


### ✅ Conclusione
Grazie ai teoremi di gerarchia possiamo dire che:
- Se `f(n)` è **constructible**
- E `f(n) ≫ g(n)` (secondo le rispettive condizioni sui limiti)

Allora le classi **DTIME** e **DSPACE** sono **strettamente crescenti**:

```
DTIME[g(n)] ⊊ DTIME[f(n)]
DSPACE[g(n)] ⊊ DSPACE[f(n)]
```

>[!bug] ⚠️ Il **Gap Theorem** è un'eccezione, perché la funzione `f(n)` **non era constructible**, quindi **non si potevano applicare** i teoremi di gerarchia.
