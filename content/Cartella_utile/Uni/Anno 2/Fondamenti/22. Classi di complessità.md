Riprendiamo il discorso dell'ultima lezione sulla correlazione polinomiale.
Prendi questo esempio
![[content/Zphoto/Pasted image 20250424164625.png]]
Vedi che posso andare avanti all'infinito.

Ma nella teoria della Complessit√† Computazionale le cose non sono proprio cos√¨.

---

## Alla ricerca della macchina pi√π veloce
#### Teorema per `dtime`
>[!lemma] Teorema 6.7 - Accelerazione lineare
>![[content/Zphoto/Pasted image 20250424165016.png]]

Questo teorema ci dice che, dato un qualunque algoritmo, ne esiste sempre uno pi√π veloce (di un fattore costante!).

>[!question] Perch√© per√≤ abbiamo i due addendi $O(|x|^{2})$ e $O(|x|)$?
>Perch√© per essere pi√π efficienti, gli algoritmi devono
>- codificare in forma espressa il proprio input (vedi teorema successivo)
>	- se la codifica √® scritta su un nastro apposito ($T_{2}$) allora bastano $O(|x|)$ passi
>	- se la macchia dispone di un solo nastro ($T_{1}$) allora occorrono $O(|x|^{2})$ passi

#### Teorema per `dspace`
Dimostriamo un teorema analogo a quello di prima ma per `dspace`
>[!lemma] Teorema 6.6 - Compressione lineare
>![[content/Zphoto/Pasted image 20250424170459.png]]

Anche qui, viene detto che, dato un qualunque algoritmo, ne esiste un altro che usa una frazione costante della memoria del primo

>[!question] Perch√© l'addendo $O(|x|)$?
>Intanto, l'input di $T_{1}$ √® lo stesso di $T$.
>Pertanto $T_{1}$ deve codificare in forma compressa il proprio input e poi lavorare sull'alfabeto compresso.
>
>Osserva come l'alfabeto compresso sia $\Sigma^{k}$ (ossia, un carattere dell‚Äôalfabeto compresso √® una parola di k caratteri di Œ£) e che l'alfabeto di $T_{1}$ √® $$\Sigma^{k} \cup \Sigma$$perch√© l'alfabeto originale non scompare.


---

## Classi di complessit√† (deterministiche)
Siamo pronti a raggruppare i linguaggi in base all'efficienza delle macchine che li decidono.

>[!question] Cosa vuol dire che una macchina che decide un linguaggio ha una certa efficienza?
>Significa che la macchina che decide un linguaggio $L \subseteq \Sigma^{*}$ si comporti "bene" su <u>ogni</u> parola $x \in \Sigma^{*}$

Per√≤ ovviamente non possiamo trovare la macchina "migliore", perch√© tanto sappiamo che se ne prendo una ne esisteranno altre pi√π potenti (teoremi di prima).

Per risolvere questa questione utilizziamo la notazione $O$ e diciamo che 
	==Un linguaggio `L` appartiene all'insieme caratterizzato dalla "efficienza temporale" individuata dalla funzione totale e calcolabile `f`, se esiste una macchina `T` che decide (o accetta) `L` e che, per ogni `x` sull'alfabeto di `L`, termina in `O(f(|x|))` istruzioni.==

E discorso analogo per "efficienza spaziale".

#### Effettive classi di complessit√† deterministiche
##### Efficienza temporale - DTIME
Le classi che misurano "efficienza temporale" nel caso deterministico si chiamano <font color="#ff0000">DTIME</font>: data una <font color="#245bdb">funzione totale e calcolabile f</font> ![[content/Zphoto/Pasted image 20250424182246.png]]
ATTENZIONE:  ^460bf9
- <font color="#245bdb">dtime</font> (minuscolo) √® la **misura di complessit√†**, ossia, una <u>funzione</u>
- <font color="#ff0000">DTIME</font> (MAIUSCOLO) √® una **classe di complessit√†**, ossia, un <u>insieme</u>

##### Efficienza spaziale - DSPACE
Le classi che misurano "efficienza spaziale" nel caso deterministico si chiamano <font color="#ff0000">DSPACE</font>: data una <font color="#245bdb">funzione totale e calcolabile f</font> ![[content/Zphoto/Pasted image 20250424182520.png]]


---

## Classi di complessit√† (non deterministiche)
Facciamo le stesse considerazioni delle deterministiche.
##### Efficienza temporale - NTIME
Le classi che misurano "efficienza temporale" nel caso non deterministico si chiamano <font color="#ff0000">NTIME</font>: data una <font color="#245bdb">funzione totale e calcolabile f</font> ![[content/Zphoto/Pasted image 20250424182637.png]]
Qui si parla di ACCETTAZIONE perch√© sappiamo che se un linguaggio √® accettato entro un certo numero di istruzioni, sappiamo che √® decidibile; <u>MA NON SAPPIAMO QUANTO CI METTE A RIGETTARE</u>.
E, per di pi√π, a noi interessa solo accettare le parole del linguaggio.

##### Efficienza spaziale - NSPACE
Le classi che misurano "efficienza spaziale" nel caso non deterministico si chiamano <font color="#ff0000">NSPACE</font>: data una <font color="#245bdb">funzione totale e calcolabile f</font> ![[content/Zphoto/Pasted image 20250424183033.png]]


---

## Classi di complemento
![[content/Zphoto/Pasted image 20250424183058.png]]


---

### Un paio di questioni
1) Qui stiamo considerando linguaggi sull'alfabeto ${0,1}$ per comodit√† ma possiamo considerare anche altri alfabeti (e lo faremo)
2) La funzione f che definisce una classe di complessit√† (ad esempio `DTIME[`<font color="#245bdb">fn</font>`]`) diamo il nome di <font color="#245bdb">funzione limite</font>
	- che ovviamente deve essere totale e calcolabile senn√≤, per definizione, avremo una funzione che ci dice quante istruzioni esegue una computazione MA di cui non sappiamo il reale valore (inutile se ci pensi!)


---

## Relazioni fra classi di complessit√†
>[!lemma] Teorema 6.8
>![[content/Zphoto/Pasted image 20250424183845.png]]

La dimostrazione √® facile.
Una macchina deterministica √® **una particolare macchina non deterministica con il grado di non determinismo pari a `1`** e, inoltre
- una parola decisa in un certo numero di passi √® anche accettata in quel certo numero di passi
- una parola decisa utilizzando un certo numero di celle √® anche accettata in quel certo numero di celle

---

>[!lemma] Teorema 6.9
>![[content/Zphoto/Pasted image 20250424185029.png]]

Questa dimostrazione segue direttamente dal [[content/Cartella_utile/Uni/Anno 2/Fondamenti/21. Misure di complessit√†#^a43723|Teorema 6.1]]
Sia $L \subseteq \{0,1\}^{*}$ tale che $L \in DTIME[f(n)]$.
Sappiamo, per il Teorema 6.1, che $$\text{spazio} \le \text{tempo}$$ per ogni macchina di Turing:
	se una macchina fa al massimo `t` passi, non pu√≤ usare pi√π di `t` caselle

Perci√≤ $$\text{dspace(T,x)} \le \text{dtime(T,x)}$$e dato che $$\text{dtime} \in O(f(|x|))$$ (per [[content/Cartella_utile/Uni/Anno 2/Fondamenti/22. Classi di complessit√†#^460bf9|definizione di DTIME]]) 
Allora possiamo scrivere $$\text{dspace(T,x)} \le \text{dtime(T,x)}  \in O(f(|x|))$$e quindi anche $$\text{dspace(T,x)} \in O(f(|x|))$$
##### ‚úÖ **Conclusione**

> Se $L \in \text{DTIME}[f(n)]$, allora $L \in \text{DSPACE}[f(n)]$

Cio√®: $$\text{DTIME}[f(n)] \subseteq \text{DSPACE}[f(n)]$$
‚úÖ **Vale anche nel caso non deterministico**: $$\text{NTIME}[f(n)] \subseteq \text{NSPACE}[f(n)]$$

---


>[!lemma] Teorema 6.10
>![[content/Zphoto/Pasted image 20250424192810.png]]

Per la dimostrazione non ho capito un cazzo, la copio e incollo.
##### üìò Teorema 6.10 (detto in parole semplici)

> Se usi **al massimo `f(n)` celle di memoria** per decidere un problema, allora esiste un modo per risolverlo che richiede **tempo al massimo `2^O(f(n))`** (cio√® tempo esponenziale rispetto allo spazio).

###### Formalmente:
```
DSPACE[f(n)] ‚äÜ DTIME[2^O(f(n))]
NSPACE[f(n)] ‚äÜ NTIME[2^O(f(n))]
```

##### üß† Ok, ma perch√©?
Immagina che la macchina di Turing T usi al massimo `f(n)` celle del nastro per un input lungo `n`.

> üîç Ogni "configurazione" della macchina √®:  
> stato + contenuto del nastro + posizione della testina

Quante possibili **configurazioni diverse** pu√≤ assumere T?

##### Le combinazioni sono tante, ma **finite**! E in particolare:
- Gli **stati** sono un numero fisso, diciamo `|Q|`
- Le celle usate sono al massimo `f(n)`
- Ogni cella pu√≤ contenere uno dei simboli dell‚Äôalfabeto `|Œ£|` (pi√π il blank)
- La testina pu√≤ stare in una delle `f(n)` celle

Allora il numero totale di configurazioni √® al massimo:

```
|Q| √ó |Œ£ + 1|^f(n) √ó f(n)
```

üìå **Questo √® un numero esponenziale rispetto a `f(n)`**  
‚áí diciamo che √® **al massimo `2^O(f(n))` configurazioni**

##### üîÅ E come ci serve questo?
Se conosci **tutte le configurazioni possibili**, puoi:
- simulare la macchina T "dall‚Äôesterno"
- esplorare tutte le possibili sequenze di configurazioni (tipo albero di esecuzione)
- controllare se si arriva a uno stato di accettazione

Questo √® un algoritmo che **decide il linguaggio**, anche se **√® pi√π lento**, perch√© deve controllare tutte le configurazioni.

Ma √® comunque **un algoritmo deterministico** che termina!

##### ‚úÖ Conclusione:

> Se un problema si pu√≤ decidere usando al massimo `f(n)` celle di memoria,
> allora **esiste un algoritmo deterministico** che risolve lo stesso problema in **tempo `2^O(f(n))`**

E lo stesso vale anche nel caso **non deterministico**.

---

>[!lemma] Teorema 6.11 ‚Äì Relazioni tra classi e loro complementi
![[content/Zphoto/Pasted image 20250425115346.png]]
#### üß† Cosa vuol dire?
- Se un linguaggio `L` si pu√≤ **decidere in tempo f(n)**, allora anche il **complementare di L**, cio√® `L·∂ú`, si pu√≤ decidere **nello stesso tempo**.
- Lo stesso vale per lo spazio.

#### üî® Dimostrazione (passo-passo)
###### üîπ Supponiamo che `L ‚àà DTIME[f(n)]`  
Allora esiste una macchina di Turing deterministica `T` che **decide** `L` in tempo `O(f(|x|))`.
- Se `x ‚àà L`, allora `T(x)` termina in uno stato accettante `q_A`
- Se `x ‚àâ L`, allora `T(x)` termina in uno stato rifiutante `q_R`

###### üîπ Costruiamo una nuova macchina `T‚Äô`
Facciamo che `T‚Äô` √® **identica** a `T`, ma **scambia gli stati finali**:
- accetta dove `T` rifiutava
- rifiuta dove `T` accettava

Quindi:
- `T‚Äô` accetta **se e solo se** `x ‚àâ L`, cio√® `x ‚àà L·∂ú`
- Il tempo di esecuzione di `T‚Äô` √® uguale a quello di `T`

###### üîπ Cosa abbiamo ottenuto?
- `T‚Äô` decide `L·∂ú` in tempo `O(f(|x|))`
- Quindi `L·∂ú ‚àà DTIME[f(n)]`
- Questo significa che `L ‚àà coDTIME[f(n)]`

E quindi:
```
DTIME[f(n)] ‚äÜ coDTIME[f(n)]
```

Ma anche il viceversa vale (scambiando `L` con `L·∂ú`) ‚áí **sono uguali!**

##### ‚úÖ Conclusione

```
DTIME[f(n)] = coDTIME[f(n)]
DSPACE[f(n)] = coDSPACE[f(n)]
```

>[!tip] üß† Intuizione
> Nelle **macchine deterministiche**, √® facile costruire il complemento:
> basta **invertire** gli stati finali.
> 
‚ö†Ô∏è **Questo non √® vero** per le **macchine non deterministiche**, perch√© **non possiamo "negare" un'esistenza** di una computazione accettante.


---


## ‚ö†Ô∏è Classi ‚Äúpoco precise‚Äù

#### üìå Problema
Quando diciamo che un linguaggio `L` √® in `DTIME[f(n)]`, stiamo usando la **notazione O(f(n))**.  
Questo significa che **non stiamo identificando con precisione** quanto tempo serve, ma solo **un limite superiore asintotico**.

#### üîÅ Conseguenza
Se `L ‚àà DTIME[f(n)]`, allora `L` appartiene anche a **molte altre classi** `DTIME[g(n)]`  
per ogni `g(n)` che cresce pi√π velocemente di `f(n)`.

#### üîç Definizione di O(g(n))
Diciamo che:
```
f(n) ‚àà O(g(n)) ‚áî ‚àÉ n‚ÇÄ ‚àà ‚Ñï, ‚àÉ c ‚àà ‚Ñï tali che ‚àÄ n ‚â• n‚ÇÄ: f(n) ‚â§ c ‚ãÖ g(n)
```

Cio√®: da un certo punto in poi, `f(n)` √® al massimo un multiplo costante di `g(n)`.

#### üìò Teorema 6.12
![[content/Zphoto/Pasted image 20250425120834.png]]
#### ‚úÖ In pratica
Se `f(n)` cresce **meno o uguale** a `g(n)` (da un certo punto in poi),  
allora ogni linguaggio decidibile in tempo/spazio `f(n)` si trova anche nella classe `g(n)`.

>[!tip] üß† Intuizione finale
> Le classi di complessit√† **non individuano univocamente un linguaggio**,  
> perch√© esistono **infinite funzioni pi√π grandi** che includono lo stesso linguaggio.
> ##### üß© Esempio
Se dici che un linguaggio √® in `DTIME[n¬≤]`, allora √® anche in:
>- `DTIME[n¬≥]`
>- `DTIME[n‚Åµ]`
>- `DTIME[2‚Åø]`
Perch√© tutte queste funzioni crescono pi√π rapidamente di `n¬≤`.

>[!bug] PER√í ATTENZIONE
>L'intuizione √® giusta a met√†, perch√© in realt√† se collochiamo un linguaggio `L`, ad esempio, in `DTIME[f(n)]`, questo non implica che `L` non possa appartenere anche a qualche classe `DTIME[r(n)]` tali che, definitivamente `r(n) ‚â§ f(n)`.
>
>###### Detto in parole spiccie
>Se io progetto un algoritmo per decidere `L`, magari esiste qualcuno che ha progettato un algoritmo pi√π efficiente del mio!!!
>Ma questo compito (spero) non ci interessa.


---

## Gap Theorem
#### üß† Contesto generale
Nel mondo della complessit√† computazionale, si cerca di classificare i problemi (cio√® i linguaggi) in base a quante risorse (tempo, spazio, ecc.) servono per risolverli.

Idealmente, vorremmo che:
Se una funzione `f(n)` cresce molto pi√π velocemente di un‚Äôaltra `g(n)`, allora:

```
DTIME[g(n)] ‚ää DTIME[f(n)]
```

cio√®: ci sono problemi che si possono risolvere con `f(n)` ma non con `g(n)`, quindi `f(n)` definisce una classe pi√π potente.

#### üëÄ Il problema 
Sarebbe bello se ci fosse sempre una gerarchia stretta tra le classi di complessit√†: pi√π tempo ‚Üí pi√π potere.
Ma non √® sempre cos√¨! Entra in scena il:

>[!lemma] Teorema 6.13 - Gap Theorem 
> ![[content/Zphoto/Pasted image 20250425125730.png]]

#### üòÆ Ma com'√® possibile?
Normalmente, uno si aspetta che se lasci alla macchina molto pi√π tempo, possa risolvere pi√π problemi.

Ad esempio, `2^f(n) ‚â´ f(n)`, quindi dovrebbe valere:

```
DTIME[f(n)] ‚ää DTIME[2^f(n)]
```

Invece il Gap Theorem ti dice il contrario:

> Esiste una funzione `f(n)` calcolabile tale che dando tempo esponenziale in `f(n)` non si guadagna nulla rispetto al tempo `f(n)`.

E allora, questi comportamenti strani si verificano quando le funzioni limite sono anch‚Äôesse ‚Äústrane‚Äù.

##### üéØ Intuizione
Significa che c‚Äô√® un ‚Äúbuco‚Äù nella gerarchia di classi temporali:
- Nonostante il salto enorme tra `f(n)` e `2^f(n)`, nessun nuovo linguaggio si aggiunge.
- La classe `DTIME[2^f(n)]` √® uguale a `DTIME[f(n)]`.

Questo fa crollare l‚Äôintuizione che "pi√π tempo = pi√π potere".


---

## üîß Funzioni time- e space-constructible
##### üß± Cosa sono le funzioni _constructible_?
Le funzioni **time-constructible** e **space-constructible** sono funzioni che **possono essere ‚Äúcostruite‚Äù da una macchina di Turing** **entro le stesse risorse** (tempo o spazio) che rappresentano.


#### ‚è±Ô∏è TIME-CONSTRUCTIBLE ‚Äî Definizione e spiegazione
##### üìå Definizione (semplificata):
Una funzione `f(n)` √® **time-constructible** se **esiste una macchina di Turing** che, dato `n` scritto in **notazione unaria**, riesce a **calcolare `f(n)`** (sempre in unario) **entro tempo O(f(n))**.

##### ‚úã Ma che vuol dire?
Immagina che `f(n) = n^2`. Dire che √® time-constructible significa:
- Esiste una macchina che, ricevuto in input `111` (cio√® 1^3), calcola `f(3) = 9`,
- E scrive `111111111` (1^9) **sul nastro di output**,
- Usando **al massimo `c * f(n)` passi** per una qualche costante `c`.


#### üíæ SPACE-CONSTRUCTIBLE ‚Äî Definizione e spiegazione
##### üìå Definizione (semplificata):
Una funzione `f(n)` √® **space-constructible** se **una macchina di Turing** pu√≤ calcolare `f(n)` (in unario), **usando al massimo `O(f(n))` celle di nastro**.


#### üéì A cosa servono queste funzioni?
Servono per:
- **formulare correttamente** i teoremi della gerarchia temporale e spaziale,
- evitare anomalie tipo quelle viste nel **Gap Theorem**,
- assicurarsi che quando diciamo ‚Äútempo f(n)‚Äù o ‚Äúspazio f(n)‚Äù, sia **davvero realizzabile** rispettare quel limite.


#### üîµ 1. Notazione unaria come input

> ‚ÄúL‚Äôinput `n` deve essere in **notazione unaria**‚Äù.

‚û°Ô∏è Questo significa che la **lunghezza dell‚Äôinput coincide con il suo valore**:
```
|1^n| = n
```

üëâ Cos√¨ possiamo misurare tempo e spazio in funzione della lunghezza reale dell‚Äôinput.

#### üíú 2. Output in notazione unaria
La macchina che ‚Äútestimonia‚Äù che `f` √® time-constructible:
- Prende in input $1^{n}$
- E scrive su nastro **il valore `f(n)` in unario**

#### ‚úèÔ∏è Esempio:
Se $f(n) = n^2 + 3$, allora per 
- `n = 3` -> `f(3) = 12`.

- La macchina prende in input $1^3 = 111$
- E scrive $1^{12} = 111111111111$ sul nastro

#### üî¥ 3. Cosa significa ‚Äútime-constructible‚Äù in parole semplici?

> Una funzione time-constructible **√® molto pi√π di una funzione calcolabile**.

##### ‚úÖ Vuol dire:
- Pu√≤ essere **calcolata in tempo proporzionale al suo stesso valore**
- Cio√®: scrivere `f(n)` simboli `1` richiede **al massimo `O(f(n))` passi**

üí° *Ogni ‚Äò1‚Äô richiede un numero costante di istruzioni (in media)*

‚û°Ô∏è √à una funzione che **si pu√≤ ‚Äúcostruire‚Äù nel tempo che rappresenta**


>[!example] Le stesse considerazioni valgono anche per lo spazio


![[content/Zphoto/Pasted image 20250425132907.png]]


---

## üö´ Addio Gap Theorem!

### ‚ùó Punto chiave iniziale:

> La funzione $f(n)$ definita nel Gap Theorem **non √® time-constructible**.

Questo spiega **perch√© il Gap Theorem non viola** i teoremi di gerarchia: quei teoremi **valgono solo se** $f(n)$ √® **costruibile**.


>[!lemma] Teorema 6.14 - Teorema di gerarchia spaziale
>![[content/Zphoto/Pasted image 20250425133201.png]]


>[!lemma] Teorema 6.15 - Teorema di gerarchia temporale
>![[content/Zphoto/Pasted image 20250425133239.png]]


#### üìà Ma cosa significano davvero i teoremi di gerarchia?
Quando diciamo che: $$\lim_{n \ \to \ \infty} \  \frac{g(n)}  {f(n)}  = 0$$
stiamo dicendo che `f(n)` cresce **molto pi√π velocemente** di `g(n)` (in modo asintotico).

‚û°Ô∏è Man mano che `n` cresce, la distanza tra `f(n)` e `g(n)` aumenta sempre di pi√π.



Allo stesso modo, per la **gerarchia temporale**, la condizione √® ancora pi√π severa: $$\lim_{n \ \to \ \infty} \  \frac{g(n) \cdot log(g(n))}  {f(n)}  = 0$$
‚û°Ô∏è Significa che `f(n)` deve essere **ancora pi√π grande** rispetto a `g(n)` per garantire una separazione tra le classi di complessit√†.


### ‚úÖ Conclusione
Grazie ai teoremi di gerarchia possiamo dire che:
- Se `f(n)` √® **constructible**
- E `f(n) ‚â´ g(n)` (secondo le rispettive condizioni sui limiti)

Allora le classi **DTIME** e **DSPACE** sono **strettamente crescenti**:

```
DTIME[g(n)] ‚ää DTIME[f(n)]
DSPACE[g(n)] ‚ää DSPACE[f(n)]
```

>[!bug] ‚ö†Ô∏è Il **Gap Theorem** √® un'eccezione, perch√© la funzione `f(n)` **non era constructible**, quindi **non si potevano applicare** i teoremi di gerarchia.
