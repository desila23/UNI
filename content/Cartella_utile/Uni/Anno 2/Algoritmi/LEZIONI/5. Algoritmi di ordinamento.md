Generalmente abbiamo:
- INPUT: una sequenza di `n` numeri
- OUTPUT: una permutazione (arrangiamento) in cui ogni elemento √® minore del successivo 


# Complessit√† quadratica ($n^{2}$)
### Selection Sort
Si tratta di un algoritmo molto semplice, facile da programmare MA estremamente inefficiente.

FUNZIONAMENTO: 
- fisso una posizione `p`
- da l√¨ in poi scorro tutto l'array cercando il minimo
- una volta trovato lo mettiamo nella posizione fissata inizialmente.
Una volta fatto questo 
- fisso la nuova posizione `p = p+1`
- rifaccio tutto quello detto in precedenza

![[content/Zphoto/PHOTO ALGORITMI/selection-600.gif|center]]

PSEUDO-CODICE
```scss (MESSO A CASO)
SelectionSort(A)
1.  for k = 0 to n-2 do         // i vari round 
2.      m = k+1                 // la posizione che fisso (quella blu)
 
3.      for j = k+2 to n do     // parto dalla posizione successiva a quella blu
4.          if (A[j] < A[m]) then m = j   // i vari confronti (quelli verdi)

5.      scambia A[m] con A[k+1]    // lo scambio (swap)
```

>[!warning] PICCOLA PRECISAZIONE
L'algoritmo fa cacare perch√© la posizione 0 non viene controllata, BOH.

>[!question]- Perch√© nella riga uno abbiamo "n-2"?
Perch√© innanzitutto le posizioni da controllare sono "n-1" per√≤ poi durante il     penultimo confronto abbiamo che l'ultimo elemento √® gi√† ordinato, quindi non       serve controllarlo.

>[!tip]- Parentesi sulle INVARIANTI
>Le invarianti sono delle propriet√† che devono rimanere immutate nella fase di esecuzione di un algoritmo e che vengono utilizzate per capire se un algoritmo √® corretto.
>Ragionare per invarianti √® uno strumento utile perch√© permette di isolare propriet√† dell‚Äôalgoritmo, spiegarne il funzionamento, capire a fondo l‚Äôidea su cui si basa.
>
>Nel nostro algoritmo abbiamo le seguenti invarianti.
>
>Al generico passo k abbiamo che:
>- (i) i primi k+1 elementi sono ordinati
>- (ii) i primi k+1 elementi sono i pi√π piccoli dell'array

#### Complessit√† temporale
- CASO UPPER BOUND: 
	- sia il ciclo for esterno che il ciclo for interno verranno eseguiti al pi√π (fai finta) **`n`** volte,
	- ogni linea di codice per il nostro modello RAM costa $O(1)$
	![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241028151821.png]]

- CASO LOWER BOUND:
	Prendiamo la parte nel codice che esegue pi√π operazioni, I CONFRONTI.
	- Il ciclo va da `k = 0` a `n-2`
	- Io eseguo `n-k-1` confronti, perch√© il primo elemento non viene mai contato (quindi n-1) e ho un k che aumenta
	Quindi
	![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241028154659.png]]

Allora $T(n) = \Theta(n^2)$ 


### Insertion sort
Simile al Selection sort.

Partendo dalla posizione 1 fino a n-1, confronta l'elemento nella posizione corrente con i precedenti finch√© non trovo un caso in cui questo elemento risulta maggiore del precedente controllato.
![[content/Zphoto/PHOTO ALGORITMI/insertion-600.gif|center]]

PSEUDO-CODICE
```scss (MESSO A CASO)

```


### Bubble sort
Gli elementi pi√π grandi vengono spinti verso destra confrontando gli elementi adiacenti tra loro

***Come funziona?*** 
- Si scorre l'array `n-1 volte` per un numero `i` di volte, dovuto al fatto che dobbiamo ogni volta scorrere il nostro array; 
- ad ogni posizione nell'array confrontiamo il numero corrispondente con il suo successivo e se non abbiamo che CORRISPONDENTE $\le$ SUCCESSIVO li switchamo
- andiamo avanti cos√¨ finch√© l'array non √® ordinato.
La differenza tra questo e l'insertion sort sta nel fatto che gli elementi vengono confrontati a coppie, mentre nell'insertion sort io confronto un elemento con tutti ricorsivamente.
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241028205056.png|350]]![[content/Zphoto/PHOTO ALGORITMI/bubble-640.gif|350]]

PSEUDO-CODICE
```scss (MESSO A CASO)

```



# Complessit√† meno che quadratica
### MergeSort
Utilizza la tecnica del ***divide et impera***:
- ***DIVIDI** il problema a met√†
- risolvi i due sottoproblemi ricorsivamente
- **IMPERA (fondi)**** le due sottosequenze ordinate
![[content/Zphoto/PHOTO ALGORITMI/merge-sort-400.gif|center]]

PSEUDO-CODICE
```scss (MESSO A CASO)
MergeSort(A, i, f)
1.  if(i < f)  then
2.     m = PARTE_INFERIORE[(i + f) / 2]
3.     MergeSort(A, i, m)
4.     MergeSort(A, m+1, f)
5.     Merge(A, i, m, f)

6.  if(i = f) then return A[i] 
```
In pratica:
1. se `inizio < fine` entro nell'`IF`

2. trovo la met√†

3. calcolo ricorsivamente il `MergeSort` della prima met√† 
	- fintanto che `i < f` (quindi fino a quando non ho UN SINGOLO ELEMENTO DA PASSARE AL `MERGESORT`) calcolo ricorsivamente; 
	- quando ho un singolo elemento vuol dire `i = f`, quindi ritorno quel valore
	- a questo punto ho i risultati della riga 3 e 4 del penultimo MergeSort chiamato
	- ripercorro tutto tornando sopra e avr√≤ il risultato finale della riga 3 chiamata la prima volta (in pratica ho la prima met√† ordinata)

4. calcolo ricorsivamente il MergeSort della seconda met√†
	- rifaccio la stessa cosa che ho scritto nel punto 3

5. QUANDO HO IL RISULTATO DEL PUNTO 3 E 4 posso eseguire il merge.

ALBERO DELLE CHIAMATE RICORSIVE
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241028221651.png|center]]

#### MERGE
Il merge pu√≤ essere eseguito quando ho due liste A e B ordinate, quindi posso fonderle rapidamente:
- estraggo ripetutamente il minimo di A e B e lo copio nell'array di output, finch√© A oppure B diventa vuoto
- copio gli elementi dell'array non vuoto alla fine dell'array di output.
![[content/Zphoto/PHOTO ALGORITMI/ezgif.com-animated-gif-maker.gif]]
PSEUDOCODICE
![[content/Zphoto/Pasted image 20250505102633.png|center|400]]
SPIEGAZIONE
La funzione Merge prende come argomenti
- l'array($A$) che contiene gli elementi dei due array precedentemente ordinati (ma magari $A$ non √® ordinato)
- la posizione iniziale($i_1$)
- la posizione al centro($f_1$)        
- la posizione finale($f_2$)

Funzionamento
- crea un array lungo quanto la posizione $finale-inizio+1$
- $i$ √® uguale a $1$ perch√© alcune volte lo pseudo codice si fa con array che partono da $1$
- $k_1$ prende il valore iniziale; $k_2$ prende il valore dell'inizio della seconda parte dell'array
- inseriamo gli elementi dei vari array scambiando in base alla necessit√† quale prendere in considerazione dei due per l'inserimento
    - ad ogni assegnazione nell'array incremento $i$ e $k_1$ se uso l'array1 oppure $k_2$ se uso array2
- se alla fine del for $k_1$ deve ancora finire di inserirsi del tutto metter√≤ per completezza gli ultimi elementi alla fine dell'array, oppure copio gli ultimi elementi di $k_2$
- mettiamo x nelle posizioni dell'array che avevamo

###### COSTO MERGE
Il merge fonde due sequenze ordinate di lunghezza $n_1$ e $n_2$ in un tempo $$\Theta(n_{1}+n_{2})= \Theta(n)$$questo perch√©
- ogni confronto "consuma" un elemento di una delle due sequenze, lunghe $n_1$ e $n_2$
- ogni posizione di X √® riempita in tempo costante
- anche la linea 12 costa $\Theta(n_{1}+ n_2)$ 

#### COSTO MERGESORT
Abbiamo$$T(n) = 2T\left(\frac n 2\right)+O(n)$$ dove
- $2T\left(\frac n 2\right)$ √® dato dalle due chiamate ricorsive `MergeSort` (riga 3 e 4) che diminuiscono proporzionalmente la dimensione degli array di $\frac n 2$ 
- $O(n)$ √® dato dal Merge (riga 5); utilizziamo $O$ perch√© nelle ultime chiamate non lavoreremo propriamente su `n` elementi
- righe 1 e 2 hanno costo $O(1)$

###### TEOREMA MASTER
![[content/Zphoto/PHOTO ALGORITMI/photo_2024-10-29_17-55-29.jpg]]La notazione asintotica rimane la stessa di $f(n)$. 

>[!question]- Quanta memoria (ausiliaria) usiamo?
>La complessit√† spaziale del MergeSort √® $\Theta(n)$, perch√©:
>- il Merge usa memoria ausiliaria pari alla dimensione di porzione da fondere (ATTENZIONE, non crea subito un array di `n` elementi MA ad ogni chiamata sui vede quanti elementi ci sono nella riga 3 e nella riga 4 e crea il suo array di conseguenza)
>- non sono mai attive due procedure di Merge contemporaneamente (prima eseguo riga 3 e poi riga 4)
>- ogni chiamata di MergeSort usa memoria costante (esclusa la memoria usata dal Merge)
>- il numero di chiamate di MergeSort attive contemporaneamente sono $O(log(n))$
>
>Il MergeSort non ordina ***in loco*** ma creo un nuovo array, lo ordino e lo stampo.


### QuickSort
Anche lui usa la tecnica del divide et impera
- **DIVIDE**: scegli un elemento x della sequenza (perno) e partiziona la sequenza in elementi $\le x$ ed elementi $> x$ 
- risolvi i due sottoproblemi ricorsivamente
- **IMPERA**: restituisci la concatenazione delle due sottosequenze ordinate
Rispetto al MergeSort abbiamo un DIVIDE COMPLESSO e un IMPERA SEMPLICE

Il funzionamento in linea di massima √® il seguente:
1. scegli un perno (per ora scegliamo sempre il primo, ma poi vedremo che non conviene)
2. scorri l'array "in parallelo" da sinistra verso destra e da destra verso sinistra
	- da sinistra verso destra, quando trova un elemento maggiore del perno si ferma
	- da destra verso sinistra, quando trova un elemento minore del perno si ferma
3. scambio i due elementi su cui mi sono fermato e riprendo la scansione dal punto 2
4. mi fermo quando i due indici si sono incrociati

##### Funzionamento algoritmo Partition  
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241029211356.png|center|600]]

##### PSEUDOCODICE Partition 
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241029211428.png]]
### üß† Come funziona passo per passo
1. **Pivot scelto**: `x = A[i]`
2. **Due puntatori**:
    - `inf` parte da `i` e si muove verso destra
    - `sup` parte da `f+1` e si muove verso sinistra
    
3. **Loop infinito** finch√© i puntatori non si incrociano:
    - **Linea 5**: `inf` avanza finch√© trova un valore > `x`
    - **Linea 6**: `sup` arretra finch√© trova un valore ‚â§ `x`
    - **Linea 7**: se `inf < sup`, scambia `A[inf]` con `A[sup]`
	    - ATTENZIONE: entra in linea 7 se e solo se sia riga 5 che riga 6 non si verificano pi√π E DEVE POI CONTROLLARE SE `inf < sup`
    - **Linea 8**: altrimenti esce dal ciclo
    
4. **Linea 9**: una volta fuori dal ciclo, scambia `A[i]` con `A[sup]`, cio√® mette il pivot nella sua posizione "finale" corretta
    
5. **Linea 10**: ritorna `sup`, cio√® la posizione in cui ora si trova il pivot

#### PSEUDOCODICE Quick Sort 
```scss
QuickSort(A, i, f)
1. if (i < f) then
2.     m = Partition(A, i, f)
3.     QuickSort(A, i, m - 1)
4.     QuickSort(A, m + 1, f)
```
L'algoritmo di per s√© fa questo
1. se i = f vuol dire che ho solo un elemento da controllare
2. Partition, dato un perno "m", mette tutti gli elementi pi√π piccoli di m a sinistra e a destra i pi√π grandi e infine inserisce m "al centro";
	io ora so che a sinistra del perno ho tutti gli elementi pi√π piccoli (non per forza ordinati) e a destra stessa cosa ma con i pi√π grandi
3. eseguo il QuickSort sulla parte a sinistra
4. eseguo il QuickSort sulla parte a destra

###### RAPPRESENTAZIONE GRAFICA
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241030165314.png]]

>[!warning]- ATTENZIONE
>Gli scambi avvengono cos√¨
>- 5 √® il perno
>
>**ENTRO NELLA RIGA 5**
>SINISTRA
>- il secondo 5 va bene
>	- incremento `inf`
>- 6 NON VA BENE 
>	- si blocca e di conseguenza `inf` non viene incrementato
>	
>**ESCO DALLA RIGA 5**
>
>**ENTRO NELLA RIGA 6**
>DESTRA
>- 4 NON VA BENE
>	- si blocca
>	
>**ESCO DALLA RIGA 6**
>
>**ENTRO NELLA RIGA 7**, `inf < sup`
>SCAMBIA 6 E 4 (quindi `A[inf]` e `A[sup]`)
>
>incremento `inf` e decremento `sup`
>
>**ENTRO NELLA RIGA 5**
>SINISTRA
>- 7 NON VA BENE
>	- si blocca
>	
>**ESCO DALLA RIGA 5**
>
>**ENTRO NELLA RIGA 6**
>DESTRA
>- 3 NON VA BENE
>	- si blocca
>	
>**ESCO DALLA RIGA 6**
>
>**ENTRO NELLA RIGA 7**, `inf < sup`
>SCAMBIA 7 E 3
>
>incremento `inf` e decremento `sup`
>
>**ENTRO NELLA RIGA 5**
>SINISTRA
>- 1 va bene 
>	- incremento `inf` 
>- 2 va bene
>	- incremento `inf` 
>- 7 NON VA BENE
>	- si blocca
>
>**ESCO DALLA RIGA 5**
>
>**ENTRO NELLA RIGA 6**
>DESTRA
>- 2 NON VA BENE
>	- si blocca 
>	
>**ESCO DALLA RIGA 6**
>
>La condizione di riga 7 non √® pi√π verificata perch√© `inf > sup` e quindi NON POSSO SCAMBIARE.
>`A[sup]` corrisponde a 2
>
>SCAMBIO IL `PERNO` CON `A[sup]`.

#### Analisi caso peggiore
Sappiamo che ogni invocazione di Partition posiziona almeno un elemento in modo corretto (il perno).
Dopo `n` invocazioni di Partition, ognuna di costo $O(n)$, ho un vettore ordinato.
Costo complessivo $$O(n^2)$$(DATO DALLA FORMULA $NUMERO \space CHIAMATE \times COSTO \space DI \space OGNI \space CHIAMATA$)

Il caso peggiore si verifica quando il perno scelto ad ogni passo √® 
- o il minimo 
- o il massimo degli elementi dell'array.
PER QUESTO MOTIVO NON HA SENSO SCEGLIERE SEMPRE IL PRIMO (o comunque scegliere sempre la stessa posizione).
Tra poco vedrai come una soluzione possa essere quella di sceglierlo a "caso".

La complessit√† in questo caso √®$$T(n) = T(n-1) + T(0) + O(n)$$ $$= T(n-1) + O(1)+O(n)$$  $$= T(n-1) + O(n)$$QUINDI$$T(n) = O(n^2)$$
Spiegazione di alcuni concetti
- IPOTIZZIAMO IO STIA PRENDENDO IL MINIMO AD OGNI CHIAMATA
	- ogni volta quindi posizioner√≤ il perno nell'attuale prima posizione
	- cos√¨ facendo la riga 3 del codice non pu√≤ essere eseguita
	- da qui $T(0)$

	- Quando arrivo ad avere $T(n-1) + O(n)$ io uso la formula $NUMERO \space CHIAMATE \times COSTO \space DI \space OGNI \space CHIAMATA$ 
	- Quante chiamate ho all'interno di $T(n-1)$? `n`
	- Quanto costa ogni chiamata? $O(n)$
	- Quindi $n \times n = n^2$ 


#### Caso migliore
Deduco avvenga quando tutti gli elementi sono gi√† ordinati.


#### Caso medio
Nel caso medio si osserva che anche se il 90% degli elementi sta a sinistra del perno e il restante 10% a destra il QuickSort ha comunque un costo di $$nlog(n)$$![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241030181055.png]]
Tutti i vari casi medi dello stesso problema avranno la medesima altezza dell'albero.

#### VERSIONE RANDOMIZZATA
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241030183634.png]]