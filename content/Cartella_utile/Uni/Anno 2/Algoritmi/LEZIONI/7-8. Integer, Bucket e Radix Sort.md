## Complessit√† di un algoritmo
>[!lemma] DEFINIZIONE: DELIMITAZIONE SUPERIORE (upper bound)
>Un algoritmo **A** ha complessit√† (costo di esecuzione) $O(f(n))$ rispetto ad una certa risorsa di calcolo (tempo o spazio), se la quantit√† `r(n)` di risorsa usata da **A** nel caso <u>peggiore</u> su istanze di dimensione `n` verifica la relazione $r(n)=O(f(n))$.
>
>Quindi, dire che l'algoritmo ha una complessit√† $O(f(n))$ significa che il consumo di risorse **r(n)** non cresce PI√ô velocemente di **f(n)**, anche nel caso peggiore.
>- **f(n)** √® il mio "tetto massimo"


>[!lemma] DEFINIZIONE: DELIMITAZIONE INFERIORE (lower bound)
>Un algoritmo **A** ha complessit√† (costo di esecuzione) $\Omega(f(n))$ rispetto ad una certa risorsa di calcolo, se la quantit√† `r(n)` di risorsa usata da **A** nel caso <u>peggiore</u> su istanze di dimensione `n` verifica la relazione $r(n)= \Omega(f(n))$.
>
>Quindi, dire che l'algoritmo ha una complessit√† $\Omega(f(n))$ significa che il consumo di risorse **r(n)** non cresce MENO velocemente di **f(n)**, anche nel caso peggiore.
>- **f(n)** √® il "punto minimo" da cui partire, e posso solo andare a salire.


---

## Complessit√† di un problema
>[!lemma] DEFINIZIONE: DELIMITAZIONE SUPERIORE (upper bound)
>Un problema **P** ha una complessit√† $O(f(n))$ rispetto ad una risorsa di calcolo <u>SE ESISTE un algoritmo</u> che risolve `P` il cui costo di esecuzione rispetto quella risorsa √® $O(f(n))$
>
>In pratica se io trovo anche un solo algoritmo, che mi risolve quel problema, che abbia un costo di $O(f(n))$ (ossia un risultato top) allora il problema ha complessit√† $O(f(n))$


>[!lemma] DEFINIZIONE: DELIMITAZIONE INFERIORE (lower bound)
>Un problema **P** ha una complessit√† $\Omega(f(n))$ rispetto ad una risorsa di calcolo <u>SE OGNI ALGORITMO</u> che risolve `P` ha costo di esecuzione rispetto quella risorsa √® $\Omega(f(n))$
>
>In pratica se TUTTI gli algoritmi che mi risolvono quel problema hanno un costo di $\Omega(f(n))$ (ossia un risultato non top) allora il problema ha complessit√† $\Omega(f(n))$


---

## Ottimalit√† di un algoritmo
>[!lemma] DEFINIZIONE
>Dato un problema **P** con complessit√† $\Omega(f(n))$ rispetto ad una risorsa di calcolo, un algoritmo che risolve **P** √® (asintoticamente) ***<u>ottimo</u>*** se ha costo di esecuzione $O(f(n))$ rispetto a quella risorsa.
###### ESEMPIO CHE NON SO SE HA SENSO
Se io ho un problema con complessit√† $\Omega(n)$ (quindi ipotizziamo $n^2$), se io trovo un algoritmo che ha costo di esecuzione $O(n)$ (quindi ipotizziamo proprio $n$), questo algoritmo √® considerato ***<u>ottimo</u>.


## Complessit√† temporale del problema dell'ordinamento
 - Lower Bound: $\Omega(n)$
	 - un algoritmo che deve ordinare `n` per forza di cose deve vederli tutti.

- Upper bound: $O(n^2)$
	- Insertion Sort
	- Selection Sort
	- Quick Sort
	- Bubble Sort

- Upper Bound migliore: $O(n \space log(n))$
	- Merge Sort
	- Heap Sort
	Questi due algoritmi, sulla base della definizione di prima, sono definiti ottimi all'interno della classe degli algoritmi basati su confronti.


Abbiamo quindi un gap di $log (n$) tra il **lower bound** e il miglior **upper bound**.

Come accennato prima, **tutti questi algoritmi sono basati su <u>CONFRONTI</u>.

>[!lemma] DEFINIZIONE: ORDINAMENTO PER CONFRONTI
>Dati due elementi $a_i$ e $a_j$, per determinare l'ordinamento relativo effettuiamo una delle seguenti operazioni di confronto:
>- $a_{i} < a_j$
>- $a_{i} \le a_j$
>- $a_{i} = a_j$
>- $a_{i} \ge a_j$
>- $a_{i} > a_j$
>
>Non si possono esaminare i valori degli elementi oppure ottenere informazioni sul loro ordine senza eseguire ALMENO UNA di queste operazioni.



>[!lemma] TEOREMA LOWER BOUND
Ogni algoritmo basato su confronti che ordina `n elementi` deve fare nel caso peggiore ALMENO $n \space log(n)$ confronti, ossia $\Omega(n \space log(n))$ 
>
NOTA: il `#`di confronti che un algoritmo esegue √® un lower bound al `#`di passi elementari che esegue


---

## ALBERO DI DECISIONE
IDEA: descrivere in modo astratto il comportamento di un generico algoritmo di ordinamento per confronto attraverso una struttura ad albero, basandosi sull'ipotetico input dato in pasto all'algoritmo.
In pratica, partendo dall'input, descrive i confronti che l‚Äôalgoritmo esegue.

Un generico algoritmo di ordinamento per confronto lavora nel modo seguente: 
- confronta due elementi $a_i$ e $a_j$ (ad esempio effettua il test $a_i\le a_j$ ); 
- a seconda del risultato, riordina e/o decide il confronto successivo da eseguire.

STRUTTURA: l'albero √® radicato e
	- i **nodi interni** (cerchi grigi) sono i **potenziali confronti**
	- le **foglie** (rettangoli bianchi) sono le **risposte dell'algoritmo**

###### ESEMPIO DI ALBERO CON 3 ELEMENTI IN INPUT
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241030120447.png]]Guardando dalla radice e spostandoci al figlio di sinistra:
- confronta il primo elemento con il secondo `(1:2)`
	- se il primo √® $\le$ del secondo, scende a crea il figlio a sinistra `(2:3)`
- confronta il secondo con il terzo `(2:3)`
	- se il secondo √® $\le$ del terzo dar√† come output `PRIMO, SECONDO, TERZO`
	- se il secondo √® $>$ del terzo crea un altro figlio `(1:3)`
- confronta il primo con il terzo `(1:3)`
	- se il primo √® $\le$ del terzo dar√† come output `PRIMO, TERZO, SECONDO`
	- se il primo √® $>$ del terzo dar√† come output `TERZO, PRIMO, SECONDO`

>[!question]- A naso, qual √® il tempo peggiore in un albero di decisione?
>Corrisponde all'altezza dell'albero; se guardi il grafico io ho 3 elementi e il massimo di confronti che posso fare √® esattamente 3 (ho toccato 3 cerchi se percorro tutto a destra).


##### OSSERVAZIONI
1. L'albero di decisione ***non √®*** associato a un problema
	- questo perch√© non rappresenta il problema da risolvere ma i passi decisionali che un algoritmo potrebbe eseguire per risolvere il problema.

2. L‚Äôalbero di decisione ***non √®*** associato ***solo*** ad un algoritmo
	- non √® associato AL SINGOLO algoritmo ma mostra i passaggi che ALGORITMI DIVERSI potrebbero fare

3. L‚Äôalbero di decisione √® associato ad un ***algoritmo*** e a una ***dimensione dell‚Äôistanza***
	- nel punto 2 abbiamo detto che di per s√© l'albero di decisione non √® associato ad un solo algoritmo; ma se noi costruiamo un albero di decisione per un ***algoritmo specifico*** e per una ***determinata istanza***, allora l'albero sar√† ***ottimizzato*** per quel singolo algoritmo.

4. L‚Äôalbero di decisione descrive le diverse sequenze di confronti che un certo algoritmo pu√≤ eseguire su istanze di una ***data dimensione***
	- l'albero mostra **tutte le possibili sequenze di scelte o confronti** che l‚Äôalgoritmo potrebbe fare per risolvere il problema per un input di una certa dimensione.

5. L'albero di decisione √® una descrizione alternativa dell'algoritmo (customizzato per istanze di una certa dimensione)
	- l'albero di decisione rappresenta un **modo alternativo di visualizzare l'algoritmo**, come un percorso tra nodi (decisioni)


#### Propriet√†
1. Per una particolare istanza, i confronti eseguiti dall‚Äôalgoritmo su quella istanza rappresentano un ***cammino radice ->  foglia***

2. L‚Äôalgoritmo segue un cammino diverso a seconda delle caratteristiche dell‚Äôistanza (input)
	 - Caso peggiore: cammino pi√π lungo

3. Il numero di confronti nel caso peggiore √® pari ***all‚Äôaltezza dell‚Äôalbero di decisione***

4. Un albero di decisione di un algoritmo (corretto) che risolve il problema dell‚Äôordinamento di `n elementi` deve avere necessariamente almeno `n! foglie` (nel caso sopra avevamo 3 elementi in input e di conseguenza 6 foglie, perch√© $3! = 3 \times 2 \times 1 = 6$)
	- se ce ne fossero meno vorrebbe dire che c'√® una permutazione che non compare mai.


>[!lemma] LEMMA
>Un albero binario T con k  foglie, ha un altezza di almeno $log_2(k)$ 

***DIMOSTRAZIONE PER INDUZIONE SU K***
- CASO BASE: `k = 1`
	Quando `k = 1` l'albero ha **solo una foglia**, che √® anche la **radice**, quindi l'altezza dell'albero √® $0$, che √® uguale a $log_2(1)$. QUESTO SODDISFA LA CONDIZIONE DEL LEMMA. 
	![[content/Zphoto/PHOTO ALGORITMI/Screenshot 2024-11-05 134944.png]]

- CASO INDUTTIVO: `k > 1`
	Assumiamo che il lemma sia vero per tutti gli alberi binari con meno di `k foglie` e tentiamo di dimostrare che lo sia per un albero binario con `k foglie`.
	1. ***Nodo v***
		- consideriamo il primo `nodo v`, un padre, che si trova pi√π vicino alla radice; `v` ha due figli (ed esiste perch√© abbiamo detto che `k > 1`). In pratica se sappiamo di avere pi√π di una foglia, esister√† per forza un albero con due figli (nel nostro caso √® proprio `v`)![[content/Zphoto/PHOTO ALGORITMI/Screenshot 2024-11-05 134951.png]]
	
	2. ***Suddivisione dei figli***:
		- Uno dei figli di `v` (nel nostro caso `u`) √® la radice di un sottoalbero con un numero di foglie compreso tra $$\frac k 2 \le NUMERO \space\space DI \space\space FOGLIE < k$$Nel senso, se io so che il mio albero in totale ha 3 foglie, vuol dire che:
			- `v` esiste, lo abbiamo dimostrato prima
			- `u` sar√† il padre di ALMENO 2 foglie e AL PI√ô $k-1$ foglie (perch√© $< k$)
			- `l'altro figlio` sar√† direttamente una foglia
			![[content/Zphoto/PHOTO ALGORITMI/Screenshot 2024-11-05 134957.png|center|150]]
	
	3. ***Altezza dell'albero T***: 
		- L'altezza di T √® ALMENO $$1 + h$$dove:
			- 1 √® l'approssimazione di tutti i nodi dalla `radice` fino a `v` (compreso)
			- h √® l'altezza del `sottoalbero con radice in u`
		
		- Per il `sottoalbero con radice in u` applichiamo l'ipotesi induttiva, perch√© ha `< k` foglie e all'inizio abbiamo assunto che l'ipotesi per chi ha `< k` foglie fosse ***sempre vera***, quindi$$h = log_2\left(\frac k 2\right)$$![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241105134759.png|center]]
		QUINDI sostituendo$$1 + log_2\left(\frac k 2\right)$$$$= 1 + log_{2}(k) - log_{2}(2)$$$$= log_{2}(k)$$
		


#### Il lower bound $\Omega(n\space log(n))$ 
Consideriamo l'albero di decisione di un qualsiasi algoritmo che risolve il problema di ordinamento di n elementi
L'altezza h dell'albero di decisione √® almeno $log_{2}(n!)$ 

Utilizziamo la formula di Stirling: $n! \approx 2(\pi \space n)^{\frac 1 2} \times (\frac n e)^n$    
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241105161953.png|center]]


---

## ALGORITMI NON BASATI SU CONFRONTI
### IntegerSort
FUNZIONAMENTO: 
- parto dal mio array di input
	- vedo quali sono il massimo e il minimo 
- creo un array ausiliario con TUTTI i numeri dal min al max che faranno da contatori; li inizializzo a 0.
	- Scorro l'array originale e incremento ogni volta il contatore corrispondente al numero che sto vedendo.
- Una volta finito di scorrere l'array originale lo riscriver√≤ inserendo numeri, in ordine, basandomi sul contatore.
###### ESEMPIO
![[content/Zphoto/PHOTO ALGORITMI/integersort contatore 2.gif|center]]
- MIN = 1 e MAX = 8, creo un array di 8 elementi
- Scorro l'array originale; 
	- vedo il 5, incremento il suo contatore; 
	- vedo l'1 incremento il suo valore; 
	- e cos√¨ via (ad esempio il 6 lo vedo due volte e il suo contatore sta a 2)

![[content/Zphoto/PHOTO ALGORITMI/integersort ordinamento.gif|center]]
- Riscrivo l'array originale in base al valore del contatore corrispondente al numero

**NON HO FATTO CONFRONTI.**

>[!warning] NOTA BENE
>Questo algoritmo funziona solo con interi e con un array di piccola dimensione

###### PSEUDOCODICE
```scss
Algoritmo IntegerSort (X, k)
1. Sia Y un array di dimensione k   // O(1)

2. for i = 1 to k do        // O(k)  
3.     Y[i] = 0       

4. for i = 1 to n do        // O(n)
5.     Y[X[i]] += 1   

6. j = 1                    // O(1)
7. for i = 1 to k do        // O(k)
8.     while (Y[i] > 0) do  // per i fissato,                                
                            // #volte eseguite √® al pi√π 1 + Y[i] -> O(k + n)
9.         X[j] = i
10.        j += 1
11.        Y[i] -= 1
```
In `1 + Y[i]`, il +1 perch√© quando `Y[i]` = 0 vado a riga 6 ma esco.

###### Spiegazione di $O(k + n)$ 
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241105163558.png]]

#### IntegerSort: analisi
- Tempo $O(1) + O(k) = O(k)$, per inizializzare `Y` a 0 (righe `2-3` del codice)
- Tempo $O(1) + O(n) = O(n)$, per calcolare i valori dei contatori (righe `4-5` del codice).
- Tempo $O(n+k)$, per ricostruire `X` (righe `6-11`)
COSTO TOTALE:$$O(n+k)$$Quindi abbiamo un tempo lineare se $$k = O(n)$$
Questo non contraddice il lower bound di $\Omega(n \space log(n))$ perch√© L'INTEGER SORT NON √à UN ALGORITMO BASATO SU CONFRONTI! 



## Bucket Sort
**UTILIT√Ä**: per ordinare **n record** (elementi in una collezione di dati) con chiavi intere in `[1, k]`.

**FUNZIONAMENTO**: identico all'IntegerSort ma invece di un array di contatori abbiamo un array di liste
- creo una array `Y`, dove in `Y[i]` inserir√≤ tutti gli elementi con chiave uguale a `i`
	- nel senso, se io voglio ordinare una array di prodotti in base al loro prezzo, nella posizione `Y[4]` (che √® di per s√© una lista) metter√≤ tutti i `record` (i prodotti) che costano 4.

- concateno le liste `Y[i]` in ordine, cos√¨ da ottenere una array ordinato correttamente

>[!tip]- ESEMPIO COMPLETO DEL SECONDO PUNTO
>
Supponiamo che tu stia ordinando prodotti con prezzi compresi tra 1 e 5 (quindi `k = 5`):
>
>- Dopo aver distribuito i prodotti, supponiamo di avere:
  >  
>    - `Y[1] = [ProdottoA]`
>    - `Y[2] = [ProdottoB, ProdottoC]`
>    - `Y[3] = []`
>    - `Y[4] = [ProdottoD]`
>    - `Y[5] = [ProdottoE, ProdottoF]`
>   
>Ora, concatenando queste liste in ordine, otterrai una singola lista ordinata in base al prezzo:
  >  `Lista¬†Ordinata=[ProdottoA,ProdottoB,ProdottoC,ProdottoD,ProdottoE,ProdottoF]`
#### ESEMPIO VISIVO
![[content/Zphoto/PHOTO ALGORITMI/bucketsort.gif]]

#### PSEUDOCODICE
```scss
BucketSort (X, k)
1. Sia `Y` un array di dimensione `k`
2. for i = 1 to k do          // creazione dell'array con liste vuote
3.	   Y[i] = lista vuota

4. for i = 1 to n do
5.	   appendi il record X[i] alla lista Y[chiave(X[i])]  
       // appendo (inserisco alla fine) l'elemento nella lista corretta

6. for i = 1 to k do
7.	   copia ordinatamente in `X` gli elementi della lista Y[i]
      // inserisco gli elementi nell'array originale nell'ordine corretto
```

### Stabilit√†
Un algoritmo √® stabile se preserva l'ordine iniziale tra elementi con la stessa chiave.
>[!tip]- Esempio
>Immagina di avere una lista del genere, sempre con prodotti e costi
>`[Pane, 4`
>`Pasta, 6`
>`Yogurt, 4`
>`Zucchine, 6`
>`Carote, 4]`
>
>Se l'algoritmo √® stabile, ordinando in base ai costi avremo
>`[Pane, 4`
>`Yogurt, 4`
>`Carote, 4`
>`Pasta, 6`
>`Zucchine, 6]`


>[!question]- Il BucketSort √® stabile?
>Si, se si appendono gli elementi di `X` in coda alla opportuna lista `Y[i]` (nello pseudocodice questo avviene nelle righe `4-5`).



## RadixSort
**UTILIT√Ä**: ordina `n` interi con valori in `[1, k]`

**IDEA**: rappresentiamo gli elementi in `base b`, ed eseguiamo una serie di ***BucketSort***

**FUNZIONAMENTO**: Partiamo dalla cifra meno significativa (quella a destra) verso quella pi√π significativa (quella a sinistra)
- ordiniamo per l'`i-esima` cifra con una passata di BucketSort
	- l'`i-esima cifra` √® la <font color="#ff0000">chiave</font>, le altre sono le <font color="#00b0f0">informazioni satellite</font>
		- ogni `i-esima cifra` diventa la chiave quando viene chiamata in esame
	- l'i-esima cifra √® un intero in `[0, b-1]`, dove `b` √® la **base scelta**
###### ESEMPIO VISIVO
![[content/Zphoto/PHOTO ALGORITMI/Pasted image 20241106142406.png]]
>[!question]- ü™£ Come funziona **BucketSort** per una cifra
Supponiamo di voler ordinare in base all‚Äôultima cifra:
Hai i numeri: `2397, 4368, 5924`
>- Cifre meno significative:
>	- `2397 ‚Üí 7`    
>	- `4368 ‚Üí 8`  
>	- `5924 ‚Üí 4`
>
>- Passi di BucketSort:
>	1. Crea **10 bucket** (da 0 a 9).  
>	2. Inserisci ogni numero nel bucket corrispondente alla cifra selezionata:
>		- bucket[4] = [5924]
>		- bucket[7] = [2397] 
>		- bucket[8] = [4368] 
>	3. **Ricompone l‚Äôelenco** leggendo i bucket in ordine crescente:  
>		‚Üí `5924, 2397, 4368`


>[!question] Cosa ci garantisce che l'algoritmo restituisca l'ordine corretto?
>Abbiamo detto che il RadixSort utilizza il BucketSort per ogni passata, e sappiamo che il BucketSort √® stabile perch√© ordina in base all'ordine "precedente".
>
>Guardiamo l'esempio:
>- quando confronto le ultime due cifre meno significative non ho problemi, perch√© sono tutte diverse
>
>- al passaggio successivo (confrontiamo le ultime tre cifre meno significative), abbiamo due cifre uguali (il `3` in `4368` e `2397`); qui entra in gioco il BucketSort, che "si ricorda" che nella passata precedente `4368` si trovava sopra `2397`, quindi lascia l'ordine invariato.

#### Tempo di esecuzione
1. **NUMERO DI PASSATE DEL BUCKET SORT**
	- supponiamo di voler ordinare `n` numeri, e il massimo tra questi √® `k`
	- se rappresentiamo in numeri in una base `b`, allora il numero massimo `k` richiede circa $log_{b}(k)$ cifre per quella base
		- In **base 10**, il numero **999** richiede 3 cifre (perch√© il massimo numero a 3 cifre in base 10 √® 999), infatti $$log_{10}(999) \approx 3$$
		- In **base 2** (binario), il numero **7** richiede 3 cifre binarie (111 in binario √® 7), infatti $$log_{2}(7) \approx 3$$
	- quindi questo vuol dire che per ordinare completamente i numeri devo eseguire $log_{b}(k)$ passate di BucketSort

2. **TEMPO PER OGNI PASSATA**
	- Ogni passata di BucketSort (per ogni cifra) ha un costo di:
		- $O(n)$ per distribuire `n` elementi nei bucket (ovvero scorro l'array originale e li inserisco in quello d'appoggio)
		
		- $O(b)$ per gestire i bucket
			- dopo aver distribuito i numeri nei bucket, devo raccoglierli di nuovo da ciascun bucket
			- se la base √® `b`, io avr√≤ `b-1` possibili bucket (nel senso se la base √® `10`, io so che le cifre vanno da `0` a `9`, quindi io posso avere `9` bucket)
		
		QUINDI IL COSTO DI OGNI PASSATA √à $$O(n + b)$$

3. **TEMPO TOTALE DI ESECUZIONE**
	- Dato che abbiamo bisogno di $log_{b}(k)$ passate e ciascuna passata costa $O(n + b)$, il tempo totale √® $$O((n + b) \times log_{b}(k))$$

4. **SEMPLIFICAZIONE DELLA COMPLESSIT√Ä ASINTOTICA**
	Usiamo questa propriet√† $$log_{b}(k) = \frac {log(k)} {log(b)}$$
	La complessit√† diventa $$O\left((n + b) \times \frac {log(k)} {log(b)}\right)$$

5. **CASO PARTICOLARE: quando $b = \Theta(n)$** 
	- In pratica, quando `b` √® proporzionale a `n` avremo
		- $b = \Theta(n)$, quindi $$n + b = \Theta(n)$$
	Ora il tempo totale diventa $$O(n \times log_{n}(k)) = O\left(n \times \frac {log(k)} {log(n)}\right)$$

6. **RISULTATO FINALE: tempo lineare**
	- Se $$k = O(n^c)$$per una costante `c` (in pratica, se `k` non cresce troppo pi√π velocemente di `n`), allora $$\frac {log(k)} {log(n)} = COSTANTE$$e allora il tempo totale sar√† $$O(n)$$


---

## ORACOLO
>[!question] PROBLEMA 4.10
>Dato un vettore X di n interi in `[1,k]`, costruire in tempo $O(n+k)$ una struttura dati (**oracolo**) che sappia rispondere a domande (**query**) in tempo $O(1)$ del tipo: ‚Äúquanti interi in X cadono nell‚Äôintervallo `[a,b]`?‚Äù, per ogni a e b.
>
>![[content/Zphoto/PHOTO LINGUAGGI/Pasted image 20241106184431.png|center|400]]


**IDEA**: costruire un ***IntegerSort*** (che ha tempo $O(n+k)$) dove in ogni posizione `Y[i]` inserisco gli elementi `i` + tutti gli elementi precedenti.
In pratica, immagina di aver trovato sei volte 1 e quattro volte 2, quindi
- `conteggio_1 = 6` quindi `Y[1] = 6`
- `conteggio_2 = 4`

Io in `Y[2]` vado ad inserire la somma tra `Y[1]` e `conteggio_2`, quindi
- `Y[2] = 10`

Se poi trovo una volta 3, avr√≤
- `conteggio_3 = 1`
- `Y[3] = Y[2] + conteggio_3` quindi `Y[3] = 11`

###### PSEUDOCODICE COSTRUZIONE ORACOLO
```java
CostruisciOracolo (X, k)
1. Sia Y un array di dimensione k
2. for i=1 to k do      // costruisco il mio array d'appoggio
3.     Y[i]=0

4. for i=1 to n do      // IntegerSort
5.     incrementa Y[X[i]]

6. for i=2 to k do 
7.      Y[i]=Y[i]+Y[i-1]  
	 // dalla posizione 2 in poi sommo la posizione corrente (in cui salver√≤ il 
     // risultato) con la precedente
8. return Y
```


###### PSEUDOCODICE PER INTERROGARE L'ORACOLO
```java
InterrogaOracolo (Y, k, a, b)
1. if b > k then 
2.     b=k

3. if a < = 1 then 
4.     return Y[b]
5. else return (Y[b]-Y[a-1])
```

SPIEGAZIONE:
```java
1. if b > k then 
2.     b=k
```
Se come estremo superiore dell'intervallo (`b`) mi √® stato dato un valore che supera la grandezza del mio array (`k`) , allora dovr√≤ ridimensionarlo (`b = k`) .


```java
3. if a < = 1 then 
4.     return Y[b]
```
Se l'estremo inferiore (`a`) √® pi√π piccolo di 1, allora dovr√≤ restituire tutti gli elementi dalla posizione `0` fino a `b`, che √® proprio `Y[b]`


```java
5. else return (Y[b]-Y[a-1])
```
Per calcolare i valori nell'intervallo dovr√≤ sottrarre ai valori totali calcolati fino a quel momento        (`Y[b]`) i valori che si trovano nella posizione precedente ad `a` (`Y[a-1]`).
Questo lo faccio perch√© io in posizione `Y[a]` ho sia `a` che tutti gli elementi precedenti `Y[a-1]`



