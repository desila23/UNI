# Scheduler
In un computer multiprogrammato, molteplici processi/thread possono competere per la CPU contemporaneamente. Per evitare che questo accada si utilizza lo **scheduler**, che decide quale processo/thread eseguire in base a degli algoritmi di scheduling.

Lo scheduling al livello del kernel avviene per thread, indipendentemente dal processo di appartenenza. Ogni thread viene gestito in modo indipendente e non legato al processo.


>[!tip]- Un po' di storia
>Nei sistemi (storici) **batch**, lo **scheduling** era **lineare**, si eseguiva il lavoro successivo sul nastro.
>
>Con la **multiprogrammazione**, lo **scheduling** è diventato più **complesso** a causa della concorrenza di utenti, e sono diventati fondamentali gli algoritmi di scheduling.
>
>Nei PC:
>- spesso un solo processo è attivo
>- la CPU raramente è una risorsa scarsa: la maggior parte dei programmi è limitata dalla velocità dell'input dell'utente.


## Vari contesti di utilizzo
- Nei **server**, la CPU è spesso contesa tra molti processi, quindi lo scheduling è fondamentale.
- Nei **dispositivi** IoT (Internet of Things), smartphone e sensori, l'obiettivo dello scheduling è diverso: si cerca di ottimizzare il consumo energetico per prolungare la durata della batteria


## Contest Switch
È il processo di **passaggio tra due processi** in esecuzione sulla CPU e si verifica quando il SO decide di interrompere un processo per far partire un altro.

È molto oneroso, perché il sistema deve:
- **Cambiare modalità**, passando da modalità **utente** a **kernel**
	
- **Salvare lo stato del processo** interrotto, così da riprenderlo
	
- **Eseguire l'algoritmo di scheduling** per decidere quale processo eseguire dopo
	
- **Aggiornare la mappa della memoria** per il nuovo processo, in pratica cambio l'area di memoria su cui lavora il processo
	
- **Invalidare la cache di memoria**, se necessario, per garantire che i dati siano aggiornati e coerenti (vedremo poi…)

Ovviamente troppe commutazioni possono consumare tempo di CPU


## Problema di scheduling tra processi
I processi alternano due tipi di fasi
- Intenso uso della CPU
- attese di I/O

Abbiamo due tipologie di utilizzo di queste fasi
1. **Compute-Bound (CPU-bound)** <font color="#00b0f0">(a)</font>
	- In cui si ha un "burst" di calcoli lunghi, ossia un'unità di tempo in cui la CPU è utilizzata al 100% e le attese di I/O sono poco frequenti

2. **I/O-bound** <font color="#00b0f0">(b)</font>
	- La CPU viene utilizzata per poco tempo al 100% e le attese di I/O sono più frequenti
		- Con CPU più veloci, i processi tendono ad essere più **I/O-bound**
	Gli SDD hanno sostituito gli Hard Disk (HDD) nei pc

![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241115125515.png|center]]

Ovviamente lo scheduling **varia in base al contesto**, ciò che funziona per un dispositivo potrebbe non essere efficace per un altro.


## Stati dei processi
![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241115130029.png|center]]
Quando ci sono più processi pronti che CPU disponibili, lo scheduler decide quale processo eseguire successivamente tramite degli algoritmi.


## Situazioni in cui è necessario lo scheduler
Lo **scheduler serve SEMPRE**.
Quando interviene?
- **Quando si crea un nuovo processo**
	- bisogna decidere chi eseguire tra genitore o figlio

- **Quando un processo ha finito (esce)**
	- decide il prossimo processo tra quelli pronti
	- se nessuno è pronto, occorre eseguire un processo inattivo del sistema

- **Quando un processo si blocca**
	- anche in questo caso bisogna selezionarne un altro
	- a volte la causa del blocco influisce sulla decisione

- **Quando c'è un Interrupt**
	- alla conclusione di in I/O, un processo potrebbe diventare pronto


## Prelazione
La **prelazione** è la possibilità, da parte del processore, di poter **interrompere un processo attivo**.
È necessaria per prevenire che un driver o una chiamata di sistema lenti blocchino la CPU.

#### Scheduling 
**NON PREEMPTIVE (senza prelazione)**
- Lo scheduler seleziona un processo e lo lascia eseguire fino al blocco o all'uscita volontaria.

**PREEMPTIVE (con prelazione)**
- Lo scheduler sceglie un processo e lo lascia eseguire per un quanto di tempo predefinito
- È necessario un interrupt del clock per restituire il controllo allo scheduler.


## Diversità negli ambienti di scheduling
I sistemi operativi possono essere classificati in base alla gestione dei processi e alla modalità con cui allocano le risorse della CPU. Tra le principali categorie troviamo i sistemi **Batch**, **Interattivi** e **Real-Time**, ognuno con caratteristiche specifiche che li rendono adatti a diversi contesti operativi.

##### Sistemi Batch
In questo ambiente, i processi vengono eseguiti senza prelazione e questo approccio garantisce un'ottimizzazione delle risorse, massimizzando il **throughput** (numero di job completati in un determinato intervallo di tempo) e minimizzando il **turnaround** (tempo complessivo tra l’avvio e la conclusione di un processo). 
L'obiettivo principale di questi sistemi è mantenere la CPU sempre attiva ed evitare inutili tempi di inattività.

##### Sistemi Interattivi
Nei sistemi **Interattivi**, la **prelazione** è un elemento essenziale, permettendo la condivisione equa delle risorse tra più utenti o processi. Questo tipo di sistema è progettato per fornire **risposte rapide e adeguate** alle richieste dell’utente, senza che un singolo processo monopolizzi la CPU. 
È particolarmente indicato per ambienti multiutente, come i server, dove più utenti possono interagire simultaneamente con il sistema senza rallentamenti significativi.

##### Sistemi Real-Time
I sistemi **Real-Time** sono progettati per garantire il rispetto di precise **scadenze temporali**, assicurando che i dati vengano elaborati entro tempi definiti. A differenza dei sistemi interattivi, la **prelazione non è sempre necessaria**, in quanto i processi real-time spesso si auto-terminano rapidamente sapendo di non poter essere eseguiti a lungo. 
Un elemento chiave di questi sistemi è la **prevedibilità** dove il corretto funzionamento dipende dalla costanza delle prestazioni nel tempo.

## Obiettivi generali
Tutti i sistemi devono garantire
- un'**equa condivisione della CPU** 
- l'attuazione delle policy dichiarate
- **bilanciamento**, quindi mantenere tutti i componenti del sistema attivi


---

# Scheduling sistemi BATCH
### FIRST-COME, FIRST-SERVED (FCFS)
Chi prima arriva, prima sborra.

**Descrizione:**
- **Senza prelazione**
- Processi assegnati alla CPU nell'ordine di arrivo
- Una singola coda di processi, l'ultimo arrivato si mette in fondo

**VANTAGGI**:
- Facile da capire e programmare
- Equo in base all'ordine di arrivo

**SVANTAGGI**:
- Prestazioni non ottimali in scenari misti (es. processi CPU-bound e I/O-bound) perché gli I/O-bound potrebbero dover aspettare troppo


### SHORTEST JOB FIRST (SJB)
Quello che impiega di meno, viene fatto sborrare per primo

**DESCRIZIONE**:
- **Senza prelazione**
- Richiede che i tempi siano noti a priori
- Il più breve viene eseguito prima

**ESEMPIO**:![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241115165144.png]]

**PROBLEMATICA**:
Questo algoritmo si ricorda solo il tempo TOTALE di un processo, non eventualmente quanto gli rimane.
Nel senso, se un processo che utilizza I/O-bound dura 10 ore e viene interrotto quando mancano 0.4 secondi, l'algoritmo lo conta come se ancora gli mancassero 10 ore.


## SHORTEST REMAINING TIME NEXT (SRTN)
Quello che sta "on the edge", viene portato a sborrare.

**DESCRIZIONE**:
- Versione con **prelazione** di SJF (nonostante sia un sistema BATCH)
- Seleziona sempre il processo con il tempo rimanente più breve da completare
- Il tempo di esecuzione deve essere noto in anticipo

**FUNZIONAMENTO**:
- Confronta il tempo totale del nuovo job con il tempo rimanente dei processi in esecuzione, se il nuovo job ci impiega di meno viene eseguito
- Assicura che i nuovi job brevi ricevano un servizio più rapido


---

# SISTEMI INTERATTIVI
Il tempo di risposta è fondamentale perché si deve rispondere rapidamente alle richieste
**Vari algoritmi**
- Round-Robin
- Priority scheduling
- Shortest Process Next con aging
- Guaranteed Scheduling
- Lottery Scheduling
- Fair-Share Scheduling


### ROUND-ROBIN
È uno degli algoritmi più vecchi, semplici, equi e ampiamente utilizzati

**CONCETTO**:
- Ogni processo riceve un **quanto di tempo** per l'esecuzione
	- se non ha terminato una volta finito il quanto, la CPU viene data a prescindere ad un altro processo
	- se si blocca o termina prima del quanto, il passaggio avviene automaticamente

**IMPLEMENTAZIONE**:
- Mantiene una lista dei processi eseguibili (ready)
- Se un processo non termina alla fine del quanto, viene posto in fondo alla lista

![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241115171341.png|center]]

##### Cosa fondamentale: durata del quanto
- **Quanto lungo**: diminuzione dell'overhead ma peggioramento della reattività
	
- **Quanto breve**: maggiore overhead e riduzione dell'efficienza della CPU

IL TOP È UN QUANTO TRA 20 E 50ms


### SCHEDULING A PRIORITÀ
Il round robin vede tutti i processi con le stesse priorità, ma in alcuni contesti c'è bisogno di una gerarchia.

Con questo nuovo metodo viene assegnata una priorità ad ogni processo e vengono eseguiti prima quelli con la priorità maggiore e poi a scendere.

##### Gestione delle priorità
Mentre un processo è in esecuzione, la sua priorità può diminuire con il tempo.
- se scende sotto quella del processo successivo, avviene uno scambio

##### Priorità statica vs dinamica
**STATICA**
- la priorità di ogni processo è sempre la stessa
**DINAMICA**
- le priorità possono cambiare

##### Strategie combinate e classi di priorità
I processi sono divisi in **classi di priorità**, ossia in gruppi in base alle priorità, e sul singolo gruppo utilizzo, ad esempio, il round robin

![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241115173029.png|center]]
TUTTI i processi con **priorità 4** vengono eseguiti prima degli altri, e per decidere chi tra loro debba essere eseguito prima utilizziamo il round Robin. 

### SHORTEST PROCESS NEXT CON AGING
L'idea è di replicare lo Shortest Job First nei sistemi interattivi, dove però non sappiamo a priori quanto duri un job.

**SOLUZIONE**:
Creiamo delle stime dando un "peso" `a` ogni volta che un processo viene eseguito.
![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241115174019.png|center]]
In pratica io posso calcolare il peso dei processi, così quando ne arriva uno nuovo calcolo il suo peso effettivo, lo confronto con gli altri, vedo chi ha il peso minore e mi regolo di conseguenza.


### GUARANTEED SCHEDULING
Cerca di garantire equamente l'utilizzo della CPU, dando quindi a `n` processi $\approx \frac l n$ della potenza della CPU.

Quindi in pratica 
- tiene traccia di quanta CPU **HA RICEVUTO** ogni processo dalla sua creazione
- calcola quanto tempo CPU ogni processo **DOVREBBE AVERE** $$\frac {TEMPO \space \space DI \space \space CREAZIONE} n$$
- VALUTA IL RAPPORTO
	- $0,5$ = hai ottenuto **la metà di quanto dovuto**
	- $2,0$ = hai ottenuto **il doppio di quanto dovuto**
	I rapporti possono sfalzarsi ogni volta perché entrano nuovi processi, completamente a caso, e sballano tutto

- ESEGUO IL PROCESSO CON IL RAPPORTO PIÙ BASSO finché non supera il suo concorrente più vicino


### SCHEDULING A LOTTERIA
- Creo dei biglietti della lotteria e li do randomicamente ad ogni processo
- chi vince si esegue e faccio $n$ estrazioni al secondo

Se voglio creare una sorta di priorità posso dare più biglietti a chi ha una priorità maggiore (così ha più possibilità di essere estratto).
Se ho processi figli posso fare in modo che il processo padre dia un po' dei suoi biglietti ai figli


## SCHEDULING FAIR-SHARE
L'idea è dare una frazione predefinita di CPU ad ogni utente e si assicura che ognuno riceva la sua frazione, indipendentemente dal numero di processi posseduti.
LETTERALMENTE DEFINIZIONE DI "**EQUITÀ**"

##### ESEMPIO
![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241115180259.png|center]]


---


# Scheduling nei sistemi Real-Time
Vengono utilizzati nei Sistemi Operativi in applicazioni in cui il tempo di risposta è fondamentale.

**CATEGORIE**:
- ***Hard Real-Time***: le scadenze devono essere rispettate per forza
- ***Soft Real-Time***: qualche scadenza mancata è tollerabile

**TIPI DI EVENTI**:
- **Periodici**: avvengono ad intervalli regolari
- ***Non Periodici***: avvengono in modo imprevedibile

**CONDIZIONE DI SCHEDULABILITÀ**:
La CPU deve essere in grado di gestire la somma totale del tempo richiesto dai processi.
	Per esempio, se ci sono
		- `m` eventi periodici
		- ogni evento `i` avviene con un periodo di $P_i$
		- ogni evento richiede $C_i$ secondi di tempo della CPU per essere gestito
		Il carico totale può essere gestito SOLO SE $$\sum_{i=1}^{m} \frac{C_i}{P_i} \leq 1$$

###### ESEMPIO
- EVENTI PERIODICI: 100ms, 200ms, 500ms
- TEMPI RICHIESTI: 50ms, 30ms, 100ms
Abbiamo quindi

| <center>Processi</center> | <center>Periodo ($P_i$)</center> | <center>Tempi richiesti ($C_i$)</center> | <center>Condizione</center>                                    |
| ------------------------- | -------------------------------- | ---------------------------------------- | -------------------------------------------------------------- |
| <center>P1</center>       | <center>100ms</center>           | <center>50ms</center>                    | <center>$\frac {C_i} {P_i} = \frac {50} {100} = 0,5$</center>  |
| <center>P2</center>       | <center>200ms</center>           | <center>30ms</center>                    | <center>$\frac {C_i} {P_i} = \frac {30} {200} = 0,15$</center> |
| <center>P3</center>       | <center>500ms</center>           | <center>100ms</center>                   | <center>$\frac {C_i} {P_i} = \frac {100} {500} = 0,2$</center> |
E QUINDI $$\sum_{i=1}^{m} \frac{C_i}{P_{i}} \space \space= \space \space 0,5 + 0,15 + 0,2 \space \space \le \space \space 1$$Il numero che ottengo, se moltiplicato per 100, mi da la percentuale di utilizzo della CPU (che non deve superare 1, ossia 100%)

Gli algoritmi di scheduling possono essere
- **Statici**, in cui i requisiti di calcolo e periodicità sono ben definiti in anticipo (l'esempio appena visto era statico) 
	Il risultato deve essere sempre lo stesso (es. $<1$)
- **Dinamici**, in cui i requisiti cambiano durante l'esecuzione
	Il risultato può cambiare (es $< 2$)


---


# Processi e Scheduling
Abbiamo sempre considerato i processi come entità indipendenti, ognuno appartenente ad un utente diverso. Tuttavia, nella realtà, esistono situazioni molto più complesse: un processo può avere uno o più figli sotto il suo controllo.

Se prendiamo come esempio un sistema di gestione di una base di dati, questo può avere vari processi figli per gestire compiti specifici, creando una struttura gerarchica.

Il problema sorge perché gli scheduler tradizionali trattano ogni processo come se fosse isolato, ignorando eventuali relazioni gerarchiche o informazioni utili e questo porta molto spesso a decisioni sub-ottimali, perché manca la possibilità di adattarsi alle necessità dei processi.

### Separazione tra meccanismo e politica di scheduling
Questa separazione è fondamentale, perché
- Il **meccanismo di scheduling** riguarda l'implementazione tecnica dell'algoritmo nel kernel del sistema operativo
- La **politica di scheduling** riguarda le regole e i parametri che determinano come e quando i processi devono essere eseguiti

L'idea di base è far sì che il sistema operativo implementi un algoritmo generico da far eseguire al kernel, mentre la politica è gestita dai processi utente.

Per esempio, il kernel utilizza un algoritmo di scheduling a priorità, ma tramite una chiamata di sistema, un processo genitore può impostare le priorità per i suoi processi figli.
In questo modo, il kernel esegue i processi in base al meccanismo stabilito, ma le politiche di gestione possono essere modificate dinamicamente grazie agli input forniti dai processi utente.


### Parallelismo
Il **parallelismo** è la capacità di un sistema di eseguire più operazioni contemporaneamente.
Si divide in due livelli principali
- **livello di processo**, dove processi indipendenti vengono eseguiti in parallelo
- **livello di thread**, in cui più thread all'interno dello stesso processo lavorano simultaneamente, condividendo memoria e risorse
Lo scheduling varia a seconda che si tratti di **thread a livello utente** o **thread a livello kernel**

##### Thread a livello utente
Il kernel non è consapevole dell'esistenza dei thread, ma vede un processo come un unico blocco, a cui assegna un quantum di tempo per l'esecuzione. All'interno del processo, è il thread stesso a decidere quale parte eseguire, senza interruzioni del clock.

Il vantaggio sta nel fatto che i thread vengono effettivamente eseguiti "gerarchicamente" e non possono essere eseguiti consecutivamente thread di processi diversi.
![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241120120745.png|center|300]]
In questo esempio un possibile ordine è `A1` -> `A2` -> `A3` -> (ricomincia il giro) `A1` -> `A2` -> `A3` 
Ciò che non può MAI succedere è questo `A1` -> `B1`, perché prima eseguo SOLO `A` 

Il problema però si verifica se un thread consuma tutto il quanto di tempo, in quel caso gli altri thread dello stesso processo non verranno eseguiti fino al prossimo turno.

##### Thread a livello kernel
Qui il kernel è consapevole dell'esistenza dei thread e può scegliere quale eseguire.
Se un thread eccede il quanto, viene sospeso.
![[content/Zphoto/PHOTO SO E RETI (VECCHIE)/Pasted image 20241120121309.png|center|300]]
Qui i casi possibili sono 
- `A1` -> `A2` -> `A3` -> (ricomincia il giro) `A1` -> `A2` -> `A3` 
- `A1` -> `B1` -> `A2` -> `B2` -> `A3` -> `B3` -> (ricomincia il giro)

Questo approccio aumenta la flessibilità e sfrutta meglio il parallelismo, ma può introdurre un overhead maggiore rispetto al livello utente, a causa dell'intervento più frequente del kernel.

#### Differenze prestazionali tra livello utente e kernel
***LIVELLO UTENTE***
Qui il passaggio da un thread all'altro è molto veloce, perché richiede solo poche istruzioni senza coinvolgere il kernel. Però in caso di blocco su operazioni di I/O, l'intero processo viene sospeso.

***LIVELLO KERNEL***
Qui lo scambio di thread è più lento, ma se un thread si blocca su un'operazione di I/O solo lui viene sospeso, mentre gli altri continuano ad essere eseguiti.
Inoltre, il kernel può
- considerare i costi per passare da un thread all'altro
- dare priorità ai thread dello stesso processo


Alcune applicazioni hanno la possibilità di gestire direttamente lo scheduling dei propri thread, senza affidarsi completamente al kernel. In pratica, l'applicazione implementa un sistema di scheduling personalizzato (chiamato **scheduling specifico**) per decidere come e quando i propri thread devono essere eseguiti.



>[!lemma] Definizione: OVERHEAD
>Per **overhead** si intende il "costo aggiuntivo" in termini di risorse e tempo necessario per gestire un determinato processo o operazione.